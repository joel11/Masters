\select@language {english}
\contentsline {figure}{\numberline {1}{\ignorespaces The Autoencoder training steps \cite {Hinton2}\relax }}{17}{figure.caption.6}
\contentsline {figure}{\numberline {2}{\ignorespaces An example diagram of a Feedforwrd Neural Network with 1 hidden layer\relax }}{29}{figure.caption.8}
\contentsline {figure}{\numberline {3}{\ignorespaces Learning rates calculated over 1000 epochs with $\eta _{\qopname \relax m{min}} = 0.1$ to $\eta _{\qopname \relax m{max}} = 1.0$ and $i=100$\relax }}{34}{figure.caption.9}
\contentsline {figure}{\numberline {4}{\ignorespaces An example diagram of a Restricted Boltzmann Machine network\relax }}{35}{figure.caption.10}
\contentsline {figure}{\numberline {5}{\ignorespaces Overall Process Flow\relax }}{48}{figure.caption.14}
\contentsline {figure}{\numberline {6}{\ignorespaces Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 1316 Samples \newline \newline Each configuration group is labelled in the following format: 'Hidden Activation-Encoding Activation-Output Activation-Scaling Technique. The MSE scores show significantly poorer performance when Standardizing is used instead of Normalizing or when there is a ReLU output activation. These configurations are excluded from figure \ref {figure-results-linear-act}.\relax }}{53}{figure.caption.20}
\contentsline {figure}{\numberline {7}{\ignorespaces Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 504 Samples \newline \newline Excluding the Standardizing scaling combinations, we can now see that Sigmoid has largely poor performance unless there is a Linear encoding layer (a commonly experienced behaviour \cite {Hinton2}]), and seems mostly unable to outperform a fully linear network. The best configuration is Relu Hidden layers, with Linear encoding and Output - this makes sense with the non-linear benefit at hidden layers but with less loss of error signal and information at the output and encoding layers.\relax }}{54}{figure.caption.21}
\contentsline {figure}{\numberline {8}{\ignorespaces Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 168 Samples \newline \newline These plots show the performance for all configurations with an encoding layer size of 25 (input 30). The fully linear networks show good performance here, where there is greater scope for linear representation in the encoding.\relax }}{54}{figure.caption.22}
\contentsline {figure}{\numberline {9}{\ignorespaces Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 168 Samples \newline \newline These plots show the performance for all configurations with an encoding layer size of 5 (input 30). Here we now see the benefit of non-linear activations in the ReLU based networks, which the fully linear is not able to outperform.\relax }}{55}{figure.caption.23}
\contentsline {figure}{\numberline {10}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration2 (\ref {config2}) - 720 Configurations \newline \newline The above groupings show the P\&L generated for different combinations of Hidden Activation - Output Activation - Scaling Method (which is for both SAE and FFN). For an input of 18, and network sizes of 40 and 80 with hidden layers of 1 an 3, we see the linear activations showing notably better performance than the ReLU activations. \relax }}{56}{figure.caption.24}
\contentsline {figure}{\numberline {11}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration2 (\ref {config2}) - 720 Configurations \newline \newline The boxplots here show the P\&L generated for networks with smaller layer sizes, as indicated by the group naming (each number represents a hidden layer and its node size). The networks were trained on the Synthetic6 dataset with 18 inputs, different sized autoencoders and learning rates. We once again see the outperformance of ReLU by the linear activations.\relax }}{56}{figure.caption.26}
\contentsline {figure}{\numberline {12}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration - 120 configurations \newline \newline The plot above shows the SAE MSE, grouped by encoding size and activations, showing a marginal increase in performance for Leaky ReLU activations. \relax }}{57}{figure.caption.27}
\contentsline {figure}{\numberline {13}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration - 80 configurations \newline \newline The plot shows the FFN P\&L, grouped by activation, showing some improvements from the Leaky ReLU activation.\relax }}{57}{figure.caption.28}
\contentsline {figure}{\numberline {14}{\ignorespaces Dataset AGL\&ACL (\ref {dataset_aglacl}); Configuration7 (\ref {config7}) \newline \newline The boxplots here show the summary of configuration performances, by minimum MSE achieved, grouped according to the number of pre-training epochs which the network had. There is a clear favour to having no pre-training in this scenario.\relax }}{58}{figure.caption.30}
\contentsline {figure}{\numberline {15}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configuration 5 (\ref {config5}) \newline The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the similar performance of He-Adj and Xavier on synthetic datasets.\relax }}{60}{figure.caption.31}
\contentsline {figure}{\numberline {16}{\ignorespaces Dataset Actual10 \newline The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the outperformance of He-Adj to Xavier on actual datasets.\relax }}{60}{figure.caption.32}
\contentsline {figure}{\numberline {17}{\ignorespaces Dataset: AGL (\ref {dataset_agl}); Configuration 3 (\ref {config3}) \newline The configurations run for AGL show notably better performance using the He-Adj based initialization in comparison to both He and Xavier. The emphasis seen here can be attributed to increased pressure on suitable weight initizliations as 1 and 2 node encoding layers were used.\relax }}{61}{figure.caption.33}
\contentsline {figure}{\numberline {18}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configurations 5 and 6 (\ref {config5}, \ref {config6}) \newline The figure here shows the P\&L performance for He-Adj and He being higher than Xavier, as to be expected with more balanced network layers and ReLU activations. The better performance in the lower end of the He-Adj and configurations when compared to He can be attributed to the inclusion of inconsistently sized layers in the configuration set, where He-Adj and would have performed better.\relax }}{61}{figure.caption.34}
\contentsline {figure}{\numberline {19}{\ignorespaces Dataset: AGL (\ref {dataset_agl}); Configuration 4 (\ref {config4}) \newline The configurations run for AGL show notably better performance using the He-Adj and and He based initialization in comparison Xavier when using actual data, as per the differences explained above.\relax }}{62}{figure.caption.35}
\contentsline {figure}{\numberline {20}{\ignorespaces Dataset: ; Configuration \newline Actual P\&L grouped according to feature selection size (where 0 is no encoding), showing increased performance the closer feature selection is to the number of assets (in this case, 10).\relax }}{63}{figure.caption.36}
\contentsline {figure}{\numberline {21}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations 5 \& 6 (\ref {config5}, \ref {config6}) \newline Synthetic P\&L grouped according to feature selection size.\relax }}{64}{figure.caption.37}
\contentsline {figure}{\numberline {22}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual SAE MSE by network size.\relax }}{66}{figure.caption.38}
\contentsline {figure}{\numberline {23}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by network size.\relax }}{67}{figure.caption.39}
\contentsline {figure}{\numberline {24}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by SGD Learning rates.\relax }}{67}{figure.caption.40}
\contentsline {figure}{\numberline {25}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by OGD Learning rates.\relax }}{68}{figure.caption.41}
\contentsline {figure}{\numberline {26}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by L1 Regularization rates.\relax }}{68}{figure.caption.42}
\contentsline {figure}{\numberline {27}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by L1 Regularization rates.\relax }}{69}{figure.caption.43}
\contentsline {figure}{\numberline {28}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by dropout rates.\relax }}{69}{figure.caption.44}
\contentsline {figure}{\numberline {29}{\ignorespaces \textbf {Synthtic SAE Data Window Aggregations} \newline \relax }}{70}{figure.caption.45}
\contentsline {figure}{\numberline {30}{\ignorespaces \textbf {Actual SAE Data Window Aggregations} \newline \relax }}{70}{figure.caption.46}
\contentsline {figure}{\numberline {31}{\ignorespaces \textbf {Synthetic Predictive P\&L Data Window Aggregations} \newline \relax }}{71}{figure.caption.47}
\contentsline {figure}{\numberline {32}{\ignorespaces \textbf {Actual Predictive P\&L Data Window Aggregations} \newline \relax }}{71}{figure.caption.48}
\contentsline {figure}{\numberline {33}{\ignorespaces \textbf {Actual Predictive P\&L by SGD Max Epochs} \newline \relax }}{72}{figure.caption.49}
\contentsline {figure}{\numberline {34}{\ignorespaces \textbf {SGD Training Dataset Size - 6 Synthetic Assets} \newline The plot above shows the P\&L for FFN predictive networks, grouped by the percentage of data excluded from the SGD training. While there is a decrease in P\&L, it is much less than the decrease in data, and may only be a result of the reduced samples used to train (due to a constant number of SGD epochs)\relax }}{72}{figure.caption.50}
\contentsline {figure}{\numberline {35}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The PDF of all OOS P\&L values, with the benchmark P\&L indicated in orange.\relax }}{73}{figure.caption.51}
\contentsline {figure}{\numberline {36}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The confusion matrix for the network with the highest OOS P\&L.\relax }}{73}{figure.caption.52}
\contentsline {figure}{\numberline {37}{\ignorespaces Dataset: AGL (\ref {dataset_agl}) ; Configurations 3 \& 4 (\ref {config3}, \ref {config4}) \newline This figure shows the effects of the difference encoding layer sizes on the P\&L when the relevant SAE is used for the predictive FFN, as for the AGL dataset (with an input of 3 features). Performance for both sizes are similar, though increasing the number of assets may result in a different effect being seen.\relax }}{76}{figure.caption.53}
\contentsline {figure}{\numberline {38}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configuration 5 (\ref {config5})\relax }}{76}{figure.caption.54}
\contentsline {figure}{\numberline {39}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configuration 5 (\ref {config5})\relax }}{77}{figure.caption.55}
\contentsline {figure}{\numberline {40}{\ignorespaces Network Sizes for SAE on the Synthetic10 dataset \newline The box plots show the MSE for the series of SAE networks trained. The results show worse performance for the typical descending network sizes used for SAE, and show no improvement form increasing network size and complexity.\relax }}{77}{figure.caption.56}
\contentsline {figure}{\numberline {41}{\ignorespaces Network Sizes for Predictive FFN on the Synthetic10 dataset \newline The box plots show the profits for the series of FFN networks trained, showing small improvments for network size increases.\relax }}{78}{figure.caption.57}
