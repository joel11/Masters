\select@language {english}
\contentsline {figure}{\numberline {1}{\ignorespaces The Autoencoder training steps \cite {Hinton2}\relax }}{21}{figure.caption.6}
\contentsline {figure}{\numberline {2}{\ignorespaces Learning rates calculated over 1000 epochs with $\eta _{\qopname \relax m{min}} = 0.1$ to $\eta _{\qopname \relax m{max}} = 1.0$ and $i=100$\relax }}{38}{figure.caption.8}
\contentsline {figure}{\numberline {3}{\ignorespaces Overall Process Flow\relax }}{52}{figure.caption.12}
\contentsline {figure}{\numberline {4}{\ignorespaces Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 1316 Samples \newline \newline Each configuration group is labelled in the following format: 'Hidden Activation-Encoding Activation-Output Activation-Scaling Technique. The MSE scores show significantly poorer performance when Standardizing is used instead of Normalizing or when there is a ReLU output activation. These configurations are excluded from the next figure.\relax }}{56}{figure.caption.18}
\contentsline {figure}{\numberline {5}{\ignorespaces Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 504 Samples \newline \newline Excluding the Standardizing scaling combinations, we can now see that Sigmoid has largely poor performance unless there is a Linear encoding layer (a commonly experienced behaviour \cite {Hinton2}]), and seems mostly unable to outperform a fully linear network. The best configuration is Relu Hidden layers, with Linear encoding and Output - this makes sense with the non-linear benefit at hidden layers but with less loss of error signal and information at the output and encoding layers.\relax }}{56}{figure.caption.19}
\contentsline {figure}{\numberline {6}{\ignorespaces Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 168 Samples \newline \newline These plots show the performance for all configurations with an encoding layer size of 25 (input 30). The fully linear networks show good performance here, where there is greater scope for linear representation in the encoding.\relax }}{57}{figure.caption.20}
\contentsline {figure}{\numberline {7}{\ignorespaces Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 168 Samples \newline \newline These plots show the performance for all configurations with an encoding layer size of 5 (input 30). Here we now see the benefit of non-linear activations in the ReLU based networks, which the fully linear is not able to outperform.\relax }}{57}{figure.caption.21}
\contentsline {figure}{\numberline {8}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration2 (\ref {config2}) - 720 Configurations \newline \newline The above groupings show the P\&L generated for different combinations of Hidden Activation - Output Activation - Scaling Method (which is for both SAE and FFN). For an input of 18, and network sizes of 40 and 80 with hidden layers of 1 an 3, we see the linear activations showing notably better performance than the ReLU activations. \relax }}{58}{figure.caption.22}
\contentsline {figure}{\numberline {9}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration2 (\ref {config2}) - 720 Configurations \newline \newline The boxplots here show the P\&L generated for networks with smaller layer sizes, as indicated by the group naming (each number represents a hidden layer and its node size). The networks were trained on the Synthetic6 dataset with 18 inputs, different sized autoencoders and learning rates. We once again see the outperformance of ReLU by the linear activations.\relax }}{59}{figure.caption.23}
\contentsline {figure}{\numberline {10}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration - 120 configurations \newline \newline The plot above shows the SAE MSE, grouped by encoding size and activations, showing a marginal increase in performance for Leaky ReLU activations. \relax }}{59}{figure.caption.24}
\contentsline {figure}{\numberline {11}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration - 80 configurations \newline \newline The plot shows the FFN P\&, grouped by activation, showing some improvements from the Leaky ReLU activation.\relax }}{60}{figure.caption.25}
\contentsline {figure}{\numberline {12}{\ignorespaces Dataset AGL\&ACL (\ref {dataset_aglacl}); Configuration7 (\ref {config7}) \newline \newline The boxplots here show the summary of configuration performances, by minimum MSE achieved, grouped according to the number of pre-training epochs which the network had. There is a clear favour to having no pre-training in this scenario.\relax }}{61}{figure.caption.27}
\contentsline {figure}{\numberline {13}{\ignorespaces \newline The box plots show the MSE for a series of SAE networks trained. As suggested in \ref {imp_weights}, the He initialization results in poorer fitting for networks with varying network layer sizes. The Xavier and DC initializations were largely on par, thought with DC resulting in better performance for smaller encoding layers.\relax }}{62}{figure.caption.28}
\contentsline {figure}{\numberline {14}{\ignorespaces \newline The configurations run for AGL show notably better performance using the DC based initialization in comparison to both He and Xavier. The emphasis seen here can be attributed to increased pressure on suitable weight initizliations with 1 and 2 node encoding layers used.\relax }}{62}{figure.caption.29}
\contentsline {figure}{\numberline {15}{\ignorespaces \newline The figure here shows the best network P\&L performance for DC and He being higher than Xavier, as to be expected with more balanced network layers and ReLU activations. The better performance in the lower end of the DC configurations when compared to He can be attributed to the inclusion of inconsistently sized layers in the configuration set, where DC would have performed better.\relax }}{63}{figure.caption.30}
\contentsline {figure}{\numberline {16}{\ignorespaces \newline The configurations run for AGL show notably better performance using the DC and He based initialization in comparison Xavier, thought with less of a difference between the 2 in comparison to the Synthetic set.\relax }}{63}{figure.caption.31}
\contentsline {figure}{\numberline {17}{\ignorespaces \textbf {Performance of AGL SAE networks by size} \newline This figure shows the MSE performance of the SAE networks for AGL. The scores show improved results for larger networks (i.e. the '9,9,9' and '12,12' sized networks have the lowest scores). The results also show much worse results for the networks in the networks with gradually decreasing layer sizes.\relax }}{64}{figure.caption.32}
\contentsline {figure}{\numberline {18}{\ignorespaces \textbf {Performance of AGL predictive FFN networks by size} \newline The plot above shows the P\&L for AGL predictive FFN networks. Similarly, there is increased preformance for networks of larger sizes, and no notable benefits of a decreasing layer size structure.\relax }}{64}{figure.caption.32}
\contentsline {figure}{\numberline {19}{\ignorespaces \textbf {Encoding Size Effects on P\&L for Synthetic Data} \newline This figure shows the effecst of the difference encoding layer sizes on the P\&L when the relevant SAE is used for the predictive FFN, as for the Synthetic10 dataset (with an input of 30 features - 3 per asset). There is a clear effect of the SAE being able to perform effective feature reduction, with the best performance being at encoding layer size 10.\relax }}{65}{figure.caption.33}
\contentsline {figure}{\numberline {20}{\ignorespaces \textbf {Encoding Size Effects on P\&L for AGL Data} \newline This figure shows the effecst of the difference encoding layer sizes on the P\&L when the relevant SAE is used for the predictive FFN, as for the AGL dataset (with an input of 3 features). Performance for both sizes are similar, though increasing the number of assets may result in a different effect being seen.\relax }}{65}{figure.caption.33}
\contentsline {figure}{\numberline {21}{\ignorespaces \textbf {SAE Data Window Aggregations - Synthetic Data} \newline The box plots show the MSE for a series of SAE networks trained, grouped by different data window aggregations. The minimum and median MSE is lower for the higher aggregations, suggesting that the reduced noise in the stationary feature set may support longer windows in further training.\relax }}{66}{figure.caption.34}
\contentsline {figure}{\numberline {22}{\ignorespaces \textbf {SAE Data Window Aggregations - AGL Data} \newline For the AGL dataset, the minimum MSE score was found in the '10,20,60' aggregated dataset, though this configuration also results in the worst MSE scores and highest median, suggesting that there is more difficultly in a non-stationary dataset.\relax }}{66}{figure.caption.34}
\contentsline {figure}{\numberline {23}{\ignorespaces \textbf {FFN Data Window Aggregations - Synthetic Data} \newline The figure here shows the P\&L for the series of FFN networks trained, grouped by different data window aggregations. The profit is higher for the longer aggregations, which is in line with expectations, where the network is able to learn a mean for the synthetic GBM data.\relax }}{66}{figure.caption.35}
\contentsline {figure}{\numberline {24}{\ignorespaces \textbf {FFN Data Window Aggregations - AGL Data} \newline The figure here shows a notably higher P\&L for datasets with a 1 day aggregation, highlighting again the non-stationarity of the real data, and thus the benefit of more accurate recent data.\relax }}{66}{figure.caption.35}
\contentsline {figure}{\numberline {25}{\ignorespaces \textbf {SGD Training Dataset Size - 6 Synthetic Assets} \newline The plot above shows the P\&L for FFN predictive networks, grouped by the percentage of data excluded from the SGD training. While there is a decrease in P\&L, it is much less than the decrease in data, and may only be a result of the reduced samples used to train (due to a constant number of SGD epochs)\relax }}{67}{figure.caption.36}
\contentsline {figure}{\numberline {26}{\ignorespaces \textbf {Effects of Learning Rate Schedule on SAE MSE} \newline The figure here shows the improvements in MSE scores gained from increasing the upper bound on the learning rate schedule for SAE configurations, with a notably better performance from the largest upper bound.\relax }}{68}{figure.caption.38}
\contentsline {figure}{\numberline {27}{\ignorespaces \textbf {Effects of Learning Rate Schedule on Predictive FFN P\&L} \newline Unlike the SAE configurations, the predictive FFN performance deteriorates as the upper bound is increased, with the largest upper bound having significantly worse P\&L results.\relax }}{68}{figure.caption.38}
\contentsline {figure}{\numberline {28}{\ignorespaces \textbf {SAE L1 Regularization - 10 Real Asset} \newline The plot above shows the MSE for 360 different SAE networks, grouped by L1 Lambda, showing notable degradation as regularization is increased. Synthetic data tests showed no performance changes below 0.01.\relax }}{69}{figure.caption.39}
\contentsline {figure}{\numberline {29}{\ignorespaces \textbf {L1 Regularization - AGL Predictive FFN} \newline Similarly, performance decreases for the AGL predictive FFN as regularization increases.\relax }}{69}{figure.caption.40}
\contentsline {figure}{\numberline {30}{\ignorespaces Network Sizes for SAE on the Synthetic10 dataset \newline The box plots show the MSE for the series of SAE networks trained. The results show worse performance for the typical descending network sizes used for SAE, and show no improvement form increasing network size and complexity.\relax }}{70}{figure.caption.41}
\contentsline {figure}{\numberline {31}{\ignorespaces Network Sizes for Predictive FFN on the Synthetic10 dataset \newline The box plots show the profits for the series of FFN networks trained, showing small improvments for network size increases.\relax }}{70}{figure.caption.42}
\contentsline {figure}{\numberline {32}{\ignorespaces Weight Initialization for SAE \newline The box plots show the MSE for a series of SAE networks trained. Surprisingly, the generally used He initialization has the worst performance, whereas the Xavier and DC initializations largely better and on par.\relax }}{83}{figure.caption.43}
\contentsline {figure}{\numberline {33}{\ignorespaces Weight Initialization for Predictive FFN \newline The box plots show the profits for a series of FFN networks trained, with the custom DC init showing better performance to Xavier.\relax }}{84}{figure.caption.44}
\contentsline {figure}{\numberline {34}{\ignorespaces Network Sizes for SAE \newline The box plots show the MSE for the series of SAE networks trained. The results show worse performance for the typical descending network sizes used for SAE, and show no improvement form increasing network size and complexity.\relax }}{84}{figure.caption.45}
\contentsline {figure}{\numberline {35}{\ignorespaces Network Sizes for Predictive FFN \newline The box plots show the profits for the series of FFN networks trained, showing small improvments for network size increases.\relax }}{85}{figure.caption.46}
\contentsline {figure}{\numberline {36}{\ignorespaces SAE Data Window Aggregations \newline The box plots show the MSE for a series of SAE networks trained, grouped by different data window aggregations. The MSE is lower for the higher aggregations, presumably due to reduced noise in the dataset, which may support longer windows in further training.\relax }}{85}{figure.caption.47}
\contentsline {figure}{\numberline {37}{\ignorespaces FFN Data Window Aggregations \newline The box plots show the profits for the series of FFN networks trained, grouped by different data window aggregations. The profit is higher for the longer aggregations once again supporting longer windows in further training.\relax }}{86}{figure.caption.48}
\contentsline {figure}{\numberline {38}{\ignorespaces Best Network Cumulative Profits \newline The graph here shows cumulative profits for the best network and benchmark, both producing profits with costs attached.\relax }}{86}{figure.caption.49}
\contentsline {figure}{\numberline {39}{\ignorespaces Best Network Confusion Matrix \newline A confusion matrix showing the match up of trades and no trades on all assets for the best model and benchmark.\relax }}{87}{figure.caption.50}
\contentsline {figure}{\numberline {40}{\ignorespaces Predictive Network Regularization \newline Effects of L1 regularization on predictive network profits.\relax }}{87}{figure.caption.51}
\contentsline {figure}{\numberline {41}{\ignorespaces Linear vs ReLU activation on smaller networks \newline The boxplots here show the summary of P\&L at networks with smaller layer sizes than previously. The networks were trained on 6 synthetic assets with a total of 18 inputs, and box groupings reflect different sized autoencoders and learning rates (720 configurations in total). We once again see the outperformance of ReLU by the linear activations.\relax }}{88}{figure.caption.52}
\contentsline {figure}{\numberline {42}{\ignorespaces SAE: Leaky ReLU vs ReLU \newline The plot above shows the MSE for 120 different SAEs, grouped by encoding size and activations.\relax }}{89}{figure.caption.53}
\contentsline {figure}{\numberline {43}{\ignorespaces FFN: Leaky ReLU vs ReLU \newline The plot above shows the P\&L for 80 different predictive networks, grouped by activation, showing some improvements from the Leaky ReLU.\relax }}{89}{figure.caption.54}
\contentsline {figure}{\numberline {44}{\ignorespaces MSE vs MAPE \newline The plot above shows the P\&L for 1920 different predictive networks, grouped by SAE selection method, showing little difference, but some improvement from MSE.\relax }}{90}{figure.caption.55}
\contentsline {figure}{\numberline {45}{\ignorespaces SAE L1 Regularization - 10 Real Assets \newline The plot above shows the MSE for 360 different SAE networks, grouped by L1 Lambda, showing notable degradation as regularization is increased. Synthetic data tests showed no performance changes below 0.01.\relax }}{90}{figure.caption.56}
\contentsline {figure}{\numberline {46}{\ignorespaces SAE Learning Rate Schedule - 10 Real Assets \newline The plot above shows the MSE for 120 different SAE networks, grouped by the maximum learning rate, where all minimum learning rates were 0.00001. The scheduling with high rates shows a clearly better performance and exploration of solution space.\relax }}{91}{figure.caption.57}
\contentsline {figure}{\numberline {47}{\ignorespaces SAE Denoising - 10 Real Assets \newline The plot above shows the MSE for 72 different SAE networks, grouped by the percentage of features switch off at random.\relax }}{91}{figure.caption.58}
\contentsline {figure}{\numberline {48}{\ignorespaces SAE Denoising - 6 Synthetic Assets \newline The plot above shows the P\&L for 160 different FFN predictive networks, grouped by the percentage of data excluded from the SGD training.\relax }}{92}{figure.caption.59}
\contentsline {figure}{\numberline {49}{\ignorespaces SAE Encoding Size P\&L \newline The plot above shows the P\&L for 1 asset according to the different encoding size in the P\&L. Maximum P\$L is the same for both 1 and 2 encoding.\relax }}{93}{figure.caption.60}
\contentsline {figure}{\numberline {50}{\ignorespaces Cumulative P\&L for best 1 Asset network \newline The plot above shows the P\&L for the best 1 asset network.\relax }}{93}{figure.caption.61}
\contentsline {figure}{\numberline {51}{\ignorespaces Price Recreation for best 1 Asset network \newline The plot above shows the predicted prices for the best 1 asset network.\relax }}{93}{figure.caption.62}
\contentsline {figure}{\numberline {52}{\ignorespaces Variations P\&L \newline The plot above shows the P\&L for 2 asset according to the different mean \& variance combinations. \relax }}{94}{figure.caption.63}
\contentsline {figure}{\numberline {53}{\ignorespaces SAE Encoding Size P\&L \newline The plot above shows the P\&L for 2 asset according to the different encoding size in the P\&L. This goes against the idea of the number of assets being the best encoding.\relax }}{94}{figure.caption.64}
\contentsline {figure}{\numberline {54}{\ignorespaces Cumulative P\&L for best 2 Asset network \newline The plot above shows the P\&L for the best 2 asset network.\relax }}{94}{figure.caption.65}
\contentsline {figure}{\numberline {55}{\ignorespaces Price Recreation for best 2 Asset network \newline The plot above shows one of the predicted prices for the best 2 asset network.\relax }}{95}{figure.caption.66}
\contentsline {figure}{\numberline {56}{\ignorespaces Variations P\&L \newline The plot above shows the P\&L for 3 asset according to the different mean \& variance combinations. \relax }}{95}{figure.caption.67}
\contentsline {figure}{\numberline {57}{\ignorespaces SAE Encoding Size P\&L \newline The plot above shows the P\&L for 3 asset according to the different encoding size in the P\&L. This goes against the idea of the number of assets being the best encoding.\relax }}{96}{figure.caption.68}
\contentsline {figure}{\numberline {58}{\ignorespaces Cumulative P\&L for best 3 Asset network \newline The plot above shows the P\&L for the best 3 asset network.\relax }}{96}{figure.caption.69}
\contentsline {figure}{\numberline {59}{\ignorespaces Variations P\&L \newline The plot above shows the P\&L for 4 asset according to the different mean \& variance combinations. \relax }}{97}{figure.caption.70}
\contentsline {figure}{\numberline {60}{\ignorespaces SAE Encoding Size P\&L \newline The plot above shows the P\&L for 4 asset according to the different encoding size in the P\&L. This supports the idea of the number of assets being the best encoding.\relax }}{97}{figure.caption.71}
\contentsline {figure}{\numberline {61}{\ignorespaces Cumulative P\&L for best 4 Asset network \newline The plot above shows the P\&L for the best 4 asset network.\relax }}{97}{figure.caption.72}
\contentsline {figure}{\numberline {62}{\ignorespaces Effect of Rolling Window Sizes on P\$L for 1, 2, 3 and 4 Assets\relax }}{98}{figure.caption.73}
\contentsline {figure}{\numberline {63}{\ignorespaces SAE MSE By Scaling \newline The three series above show the classification accuracy scores (percentage) by epoch on an AutoEncoder which was used to classify MNIST images. The series were trained with 0, 1 and 5 pre-training epochs, and show a clear improvment in having an epoch of pre-training in the SAE formation (though not much for more than 1).\relax }}{99}{figure.caption.74}
\contentsline {figure}{\numberline {64}{\ignorespaces Pre-training Effects on financial SAE \newline The boxplots here show the summary of configuration performances, by minimum MSE achieved, grouped according to the number of pre-training epochs which the network had. There is a clear favour to having no pre-training in this scenario.\relax }}{100}{figure.caption.75}
\contentsline {figure}{\numberline {65}{\ignorespaces Pre-training Effects on financial SAE, by learning rate \newline These boxplots show the same as above, but further grouped by learning rate. We can see the few samples that appears to be benefiting from having 1 epoch of pre-training are simply a result of the learning rate being small enough so as not to have much effect.\relax }}{100}{figure.caption.76}
\contentsline {figure}{\numberline {66}{\ignorespaces Effects of Standardizing \& ReLU Output \newline These boxplots show the MSE scores for the combinations run grouped by 'Hidden Activation-Encoding Activation-Output Activation-Scaling Technique. There is significantly poorer performance when Standardizing is used instead of Normalizing or when there is a ReLU output activation. These configurations are excluded from the graphs below.\relax }}{101}{figure.caption.77}
\contentsline {figure}{\numberline {67}{\ignorespaces Effects of Linear Activation \newline This is the same data as in Figure 4, focusing on the more effective configurations. We can see Sigmoid has largely poor performance unless there is a Linear encoding layer (a widely experienced behaviour), and seems mostly unable to outperform a fully linear network. The best configuration is Relu Hidden layers, with Linear encoding and Output - this makes sense with the non-linear benefit at hidden layers but with less loss of error signal and information at the output and encoding layers.\relax }}{101}{figure.caption.78}
\contentsline {figure}{\numberline {68}{\ignorespaces Encoding Size 25 \newline These plots show the performance for all configurations with an encoding layer size of 25 (input 30). There is once again a surprisingly high performance in the fully linear network.\relax }}{102}{figure.caption.79}
\contentsline {figure}{\numberline {69}{\ignorespaces Encoding Size 5 \newline These plots show the performance for all configurations with an encoding layer size of 5 (input 30). Here we finally see the benefit of non-linear activations in the ReLU based newtorks, which the fully linear is not able to outperform.\relax }}{102}{figure.caption.80}
\contentsline {figure}{\numberline {70}{\ignorespaces Performance by Network Size \newline We can further break these down by network size, and see performance is as one would hope, with the best ReLU configurations corresponding with more layers and of larger sizes.\relax }}{103}{figure.caption.81}
\contentsline {figure}{\numberline {71}{\ignorespaces Performance by Denoising Variation \newline The above groupings show the decreasing SAE performance as the level of denoising is increased. The grouping with variance 1.0e-11 essentially represents no denoising.\relax }}{103}{figure.caption.82}
\contentsline {figure}{\numberline {72}{\ignorespaces Linear Activations in FFN \newline The above groupings show the profits generated for different combinations of Hidden Activation - Output Activation - Scaling Method (which is for both SAE and FFN). There are several takeaways: \newline 1. Higher performance of Linear networks (may be down to network size and amount of input data) \newline 2. The limited scaling technique is having a noticeable impact on profits \newline 3. The decrease in performance with ReLU output persists \relax }}{104}{figure.caption.83}
\contentsline {figure}{\numberline {73}{\ignorespaces SAE Effects on P\&L \newline The above groupings show the profits generated for different SAE encoding layers. The results are encouraging, showing that no SAE (encoding 0) has the worst results, and that the best results are found in one of the lower encoding layers (6).\relax }}{104}{figure.caption.84}
\contentsline {figure}{\numberline {74}{\ignorespaces Network Size Effects on P\&L \newline We can see the effects are largely as expected here, with networks of more layers and larger layer sizes having a better effect on profits. \relax }}{105}{figure.caption.85}
\contentsline {figure}{\numberline {75}{\ignorespaces Validation Set Effects on P\&L \newline This tested the witholding of 10\% of the data from the SGD training versus witholding no data. Curiously, there doesn't seem to be a notable effect here, possibly speaking to the much larger general effect of the OGD training in comparison. \relax }}{105}{figure.caption.86}
\contentsline {figure}{\numberline {76}{\ignorespaces Histogram of P\&L from Predictive MMS \newline The P\&L values seem to be rather low, with few losses. Could warrant a different trading strategy being used. \relax }}{106}{figure.caption.87}
\contentsline {figure}{\numberline {77}{\ignorespaces Cumulative P\&L \newline We can see even the best configuration seems to be delivering very poor results relative to the benchmark, with negative P\$L with costs considered. \relax }}{106}{figure.caption.88}
\contentsline {figure}{\numberline {78}{\ignorespaces Cumulative Rates \newline We can see even the best configuration seems to be delivering very poor results relative to the benchmark, with negative P\$L with costs considered. \relax }}{106}{figure.caption.89}
\contentsline {figure}{\numberline {79}{\ignorespaces Price Predictions \newline This graphs displays the actual prices for the one stock, as well as the predictions made by the two best networks (chosen by MSE and Profit) \relax }}{107}{figure.caption.90}
\contentsline {figure}{\numberline {80}{\ignorespaces Price Predictions (Zoomed) \newline A closer zoom reveals that it looks like the predictions are very much a lagged time series \relax }}{107}{figure.caption.91}
\contentsline {figure}{\numberline {81}{\ignorespaces SAE MSE By Scaling\relax }}{108}{figure.caption.92}
\contentsline {figure}{\numberline {82}{\ignorespaces OGD MSE By Scaling\relax }}{109}{figure.caption.93}
\contentsline {figure}{\numberline {83}{\ignorespaces OGD Profits by Scaling\relax }}{109}{figure.caption.94}
\contentsline {figure}{\numberline {84}{\ignorespaces Price Plot\relax }}{110}{figure.caption.95}
\contentsline {figure}{\numberline {85}{\ignorespaces PDF of all Profits Generated\relax }}{111}{figure.caption.96}
\contentsline {figure}{\numberline {86}{\ignorespaces SAE Profits\relax }}{111}{figure.caption.97}
\contentsline {figure}{\numberline {87}{\ignorespaces Network Structure Profits\relax }}{112}{figure.caption.98}
\contentsline {figure}{\numberline {88}{\ignorespaces Cumulative Profits\relax }}{112}{figure.caption.99}
\contentsline {figure}{\numberline {89}{\ignorespaces Cumulative Return Rates\relax }}{113}{figure.caption.100}
\contentsline {figure}{\numberline {90}{\ignorespaces Daily Rates\relax }}{113}{figure.caption.101}
\contentsline {figure}{\numberline {91}{\ignorespaces Stock 1\relax }}{114}{figure.caption.102}
\contentsline {figure}{\numberline {92}{\ignorespaces Stock 2\relax }}{114}{figure.caption.103}
\contentsline {figure}{\numberline {93}{\ignorespaces Stock 3\relax }}{115}{figure.caption.104}
\contentsline {figure}{\numberline {94}{\ignorespaces Stock 4\relax }}{115}{figure.caption.105}
\contentsline {figure}{\numberline {95}{\ignorespaces Stock 5\relax }}{116}{figure.caption.106}
\contentsline {figure}{\numberline {96}{\ignorespaces Stock 6\relax }}{116}{figure.caption.107}
\contentsline {figure}{\numberline {97}{\ignorespaces Stock 6\relax }}{117}{figure.caption.108}
