\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ this will need to be rewritten once everything is finalised}{2}{section*.1}}
\pgfsyspdfmark {pgfid1}{7555917}{40875316}
\pgfsyspdfmark {pgfid4}{36009438}{40908876}
\pgfsyspdfmark {pgfid5}{37631454}{40663690}
\citation{Hinton2}
\citation{Hinton2}
\@writefile{toc}{\contentsline {section}{List of Figures}{5}{section*.1}}
\@writefile{toc}{\contentsline {section}{List of Tables}{8}{section*.1}}
\citation{BailyPBO}
\@writefile{toc}{\contentsline {part}{{\fontencoding  {OT1}\selectfont  I}\hspace  {1em}\relax \fontsize  {12}{14}\selectfont  {Online non-linear prediction of {\@@par }financial time-series patterns}}{9}{part.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{9}{section.1}}
\newlabel{Introduction}{{1}{9}{Introduction}{section.1}{}}
\newlabel{Introduction@cref}{{[section][1][]1}{9}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ may need to add some appropriate in this section}{9}{section*.2}}
\pgfsyspdfmark {pgfid6}{7555917}{37483163}
\pgfsyspdfmark {pgfid9}{36009438}{37516723}
\pgfsyspdfmark {pgfid10}{37631454}{37271537}
\citation{Hinton2}
\citation{BailyPBO}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{red}{}{red!25}{\leavevmode {\color  {red!25}o}}\ check this}{10}{section*.3}}
\pgfsyspdfmark {pgfid11}{13166537}{43201844}
\pgfsyspdfmark {pgfid14}{36009438}{43235404}
\pgfsyspdfmark {pgfid15}{37631454}{42990218}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add ref}{10}{section*.4}}
\pgfsyspdfmark {pgfid16}{16936821}{36910388}
\pgfsyspdfmark {pgfid19}{36009438}{36943948}
\pgfsyspdfmark {pgfid20}{37631454}{36698762}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ this will need to be finished once results are known}{10}{section*.5}}
\pgfsyspdfmark {pgfid21}{30717467}{32191796}
\pgfsyspdfmark {pgfid24}{36009438}{32225356}
\pgfsyspdfmark {pgfid25}{37631454}{31980170}
\citation{Murphy}
\citation{Murphy}
\citation{Griffioen}
\citation{Kahn}
\citation{Schwager}
\citation{Johnson}
\citation{Arthur}
\citation{Crutchfield}
\citation{Packard}
\citation{Takens}
\citation{Skabar}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{11}{section.2}}
\newlabel{lr_LiteratureReview}{{2}{11}{Literature Review}{section.2}{}}
\newlabel{lr_LiteratureReview@cref}{{[section][2][]2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Technical Analysis}{11}{subsection.2.1}}
\newlabel{lr_TechnicalAnalysis}{{2.1}{11}{Technical Analysis}{subsection.2.1}{}}
\newlabel{lr_TechnicalAnalysis@cref}{{[subsection][1][2]2.1}{11}}
\citation{Schmidhuber}
\citation{Ivakhnenko}
\citation{Werbos}
\citation{Siegelmann}
\citation{Hochreiter}
\citation{Schmidhuber}
\citation{Minksy}
\citation{LeCun2}
\citation{Werbos2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural Networks}{12}{subsection.2.2}}
\newlabel{lr_nn}{{2.2}{12}{Neural Networks}{subsection.2.2}{}}
\newlabel{lr_nn@cref}{{[subsection][2][2]2.2}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Training and Backpropagation}{12}{subsubsection.2.2.1}}
\newlabel{lr_trainingbackprop}{{2.2.1}{12}{Training and Backpropagation}{subsubsection.2.2.1}{}}
\newlabel{lr_trainingbackprop@cref}{{[subsubsection][1][2,2]2.2.1}{12}}
\citation{Rumelhart}
\citation{LeCun3}
\citation{Pascanu}
\citation{Schmidhuber}
\citation{LeCun4}
\citation{Dauphin}
\citation{Ge}
\citation{Hornik}
\citation{Wu}
\citation{Glorot}
\citation{Glorot2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Activation Functions}{13}{subsubsection.2.2.2}}
\newlabel{lr_activationfunctions}{{2.2.2}{13}{Activation Functions}{subsubsection.2.2.2}{}}
\newlabel{lr_activationfunctions@cref}{{[subsubsection][2][2,2]2.2.2}{13}}
\citation{Bengio1}
\citation{Hinton1}
\citation{Hinton1}
\citation{Ranzato1}
\citation{Hinton2}
\citation{Hinton1}
\citation{LeRoux}
\citation{Bengio2}
\citation{Sermanet}
\citation{ImageNet}
\citation{WaveNet}
\citation{ImageNet}
\citation{Glorot2}
\citation{Ciresan}
\citation{Bengio3}
\citation{Glorot}
\citation{Glorot}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Deep Learning}{14}{subsubsection.2.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Weight Initialization Improvements}{14}{subsubsection.2.2.4}}
\newlabel{lr_weight_init}{{2.2.4}{14}{Weight Initialization Improvements}{subsubsection.2.2.4}{}}
\newlabel{lr_weight_init@cref}{{[subsubsection][4][2,2]2.2.4}{14}}
\citation{He}
\citation{Hornik}
\citation{Schaefer}
\citation{Donoho}
\citation{Fan1}
\citation{Fan2}
\citation{Fama}
\citation{Langkvist}
\citation{Langkvist}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Stacked Autoencoders}{15}{subsection.2.3}}
\newlabel{lr_SAE}{{2.3}{15}{Stacked Autoencoders}{subsection.2.3}{}}
\newlabel{lr_SAE@cref}{{[subsection][3][2]2.3}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}High Dimensional Data Reduction}{15}{subsubsection.2.3.1}}
\newlabel{HDDR}{{2.3.1}{15}{High Dimensional Data Reduction}{subsubsection.2.3.1}{}}
\newlabel{HDDR@cref}{{[subsubsection][1][2,3]2.3.1}{15}}
\citation{Hinton1}
\citation{Ranzato1}
\citation{Bengio1}
\citation{Hinton2}
\citation{Hinton3}
\citation{Hinton2}
\citation{Hinton2}
\citation{Vincent}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Deep Belief Networks}{16}{subsubsection.2.3.2}}
\newlabel{DBN}{{2.3.2}{16}{Deep Belief Networks}{subsubsection.2.3.2}{}}
\newlabel{DBN@cref}{{[subsubsection][2][2,3]2.3.2}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Stacked Denoising Autoencoders}{16}{subsubsection.2.3.3}}
\newlabel{lr_SDAE}{{2.3.3}{16}{Stacked Denoising Autoencoders}{subsubsection.2.3.3}{}}
\newlabel{lr_SDAE@cref}{{[subsubsection][3][2,3]2.3.3}{16}}
\citation{Vincent}
\citation{Vincent}
\citation{Erhan}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Autoencoder training steps \cite  {Hinton2}\relax }}{17}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figure-DBN-RBM}{{1}{17}{The Autoencoder training steps \cite {Hinton2}\relax }{figure.caption.6}{}}
\newlabel{figure-DBN-RBM@cref}{{[figure][1][]1}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Pre-training}{17}{subsubsection.2.3.4}}
\citation{Lv}
\citation{Langkvist}
\citation{Takeuchi}
\citation{Zhao}
\citation{Troiano}
\citation{Bao}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}Time Series Applications}{18}{subsubsection.2.3.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.6}Financial Applications}{18}{subsubsection.2.3.6}}
\citation{Hsu}
\citation{Albers}
\citation{Bottou}
\citation{LeCun}
\citation{Bottou2}
\citation{Bottou}
\citation{Shalev}
\citation{Zhang}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Online Learning Algorithms and Gradient Descent}{19}{subsection.2.4}}
\newlabel{lr_OGD}{{2.4}{19}{Online Learning Algorithms and Gradient Descent}{subsection.2.4}{}}
\newlabel{lr_OGD@cref}{{[subsection][4][2]2.4}{19}}
\citation{Tseng}
\citation{Bartlett}
\citation{Langford}
\citation{Duchi}
\citation{Zeiler}
\citation{Hinton4}
\citation{Goodfellow}
\citation{Wang2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Gradient Learning Improvements}{20}{subsection.2.5}}
\newlabel{lr_grad_improv}{{2.5}{20}{Gradient Learning Improvements}{subsection.2.5}{}}
\newlabel{lr_grad_improv@cref}{{[subsection][5][2]2.5}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Gradient Adjustments and Regularization}{20}{subsubsection.2.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Dropout}{20}{subsubsection.2.5.2}}
\citation{Smith}
\citation{Smith}
\citation{Loshchilov}
\citation{Loshchilov}
\citation{Ioannidis}
\citation{BailyPBO}
\citation{McLean}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Learning Rate Schedules}{21}{subsubsection.2.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Backtesting and Model Validation}{21}{subsection.2.6}}
\newlabel{lr_backtesting}{{2.6}{21}{Backtesting and Model Validation}{subsection.2.6}{}}
\newlabel{lr_backtesting@cref}{{[subsection][6][2]2.6}{21}}
\citation{Schorfheide}
\citation{Prado}
\citation{Schorfheide}
\citation{Weiss}
\citation{Hawkins}
\citation{BailyPBO}
\citation{Hansen}
\citation{Aparicio}
\citation{BailyPBO}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Testing Methodologies}{22}{subsubsection.2.6.1}}
\newlabel{lr_cscv}{{2.6.1}{22}{Testing Methodologies}{subsubsection.2.6.1}{}}
\newlabel{lr_cscv@cref}{{[subsubsection][1][2,6]2.6.1}{22}}
\citation{Lo}
\citation{BaileyBTL}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Test Data Length}{23}{subsubsection.2.6.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ shorten section?}{23}{section*.7}}
\pgfsyspdfmark {pgfid26}{7555917}{36130890}
\pgfsyspdfmark {pgfid29}{36009438}{36164450}
\pgfsyspdfmark {pgfid30}{37631454}{35919264}
\newlabel{SRAnnual}{{1}{23}{Test Data Length}{equation.2.1}{}}
\newlabel{SRAnnual@cref}{{[equation][1][]1}{23}}
\newlabel{SRConvergence}{{2}{23}{Test Data Length}{equation.2.2}{}}
\newlabel{SRConvergence@cref}{{[equation][2][]2}{23}}
\newlabel{MinBTL}{{3}{23}{Test Data Length}{equation.2.3}{}}
\newlabel{MinBTL@cref}{{[equation][3][]3}{23}}
\citation{BaileyBTL}
\citation{BaileySharpe}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Sharpe Ratio}{24}{subsubsection.2.6.3}}
\newlabel{SR}{{4}{24}{Sharpe Ratio}{equation.2.4}{}}
\newlabel{SR@cref}{{[equation][4][]4}{24}}
\newlabel{tratio}{{5}{24}{Sharpe Ratio}{equation.2.5}{}}
\newlabel{tratio@cref}{{[equation][5][]5}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation: Data Processing and Generation }{25}{section.3}}
\newlabel{Data}{{3}{25}{Implementation: Data Processing and Generation }{section.3}{}}
\newlabel{Data@cref}{{[section][3][]3}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data Processing}{25}{subsection.3.1}}
\newlabel{data_processing}{{3.1}{25}{Data Processing}{subsection.3.1}{}}
\newlabel{data_processing@cref}{{[subsection][1][3]3.1}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Log Difference Transformation and Aggregation}{25}{subsubsection.3.1.1}}
\newlabel{ldata_og_difference}{{3.1.1}{25}{Log Difference Transformation and Aggregation}{subsubsection.3.1.1}{}}
\newlabel{ldata_og_difference@cref}{{[subsubsection][1][3,1]3.1.1}{25}}
\newlabel{eq_logdiff}{{6}{25}{Log Difference Transformation and Aggregation}{equation.3.6}{}}
\newlabel{eq_logdiff@cref}{{[equation][6][]6}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Data Scaling}{25}{subsubsection.3.1.2}}
\newlabel{data_scaling}{{3.1.2}{25}{Data Scaling}{subsubsection.3.1.2}{}}
\newlabel{data_scaling@cref}{{[subsubsection][2][3,1]3.1.2}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Reverse Data Scaling}{26}{subsubsection.3.1.3}}
\newlabel{data_reverse_scaling}{{3.1.3}{26}{Reverse Data Scaling}{subsubsection.3.1.3}{}}
\newlabel{data_reverse_scaling@cref}{{[subsubsection][3][3,1]3.1.3}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Price Reconstruction}{26}{subsubsection.3.1.4}}
\newlabel{data_price_recon}{{3.1.4}{26}{Price Reconstruction}{subsubsection.3.1.4}{}}
\newlabel{data_price_recon@cref}{{[subsubsection][4][3,1]3.1.4}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Synthetic Data Generation}{27}{subsection.3.2}}
\newlabel{data_synthetic}{{3.2}{27}{Synthetic Data Generation}{subsection.3.2}{}}
\newlabel{data_synthetic@cref}{{[subsection][2][3]3.2}{27}}
\newlabel{algo_brownianmotion}{{1}{27}{Synthetic Data Generation}{algocfline.1}{}}
\newlabel{algo_brownianmotion@cref}{{[line][1][]1}{27}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Geometric Brownian Motion Simulation\relax }}{27}{algocf.1}}
\citation{Schmidhuber}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation: Models and Algorithms}{28}{section.4}}
\newlabel{Implementation}{{4}{28}{Implementation: Models and Algorithms}{section.4}{}}
\newlabel{Implementation@cref}{{[section][4][]4}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Process Overview}{28}{subsection.4.1}}
\newlabel{ProcessOverview}{{4.1}{28}{Process Overview}{subsection.4.1}{}}
\newlabel{ProcessOverview@cref}{{[subsection][1][4]4.1}{28}}
\newlabel{imp_overview}{{4.1}{28}{Process Overview}{subsection.4.1}{}}
\newlabel{imp_overview@cref}{{[subsection][1][4]4.1}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Feedforward Neural Networks}{28}{subsection.4.2}}
\newlabel{imp_ffn}{{4.2}{28}{Feedforward Neural Networks}{subsection.4.2}{}}
\newlabel{imp_ffn@cref}{{[subsection][2][4]4.2}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example diagram of a Feedforwrd Neural Network with 1 hidden layer\relax }}{29}{figure.caption.8}}
\newlabel{figure-neural_network_diagram}{{2}{29}{An example diagram of a Feedforwrd Neural Network with 1 hidden layer\relax }{figure.caption.8}{}}
\newlabel{figure-neural_network_diagram@cref}{{[figure][2][]2}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Notation and Network Representation}{29}{subsubsection.4.2.1}}
\newlabel{imp_ffn_functions}{{4.2.1}{29}{Notation and Network Representation}{subsubsection.4.2.1}{}}
\newlabel{imp_ffn_functions@cref}{{[subsubsection][1][4,2]4.2.1}{29}}
\newlabel{eq_weighted_input}{{16}{29}{Notation and Network Representation}{equation.4.16}{}}
\newlabel{eq_weighted_input@cref}{{[equation][16][]16}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Activation Functions}{29}{subsubsection.4.2.2}}
\newlabel{imp_activation_functions}{{4.2.2}{29}{Activation Functions}{subsubsection.4.2.2}{}}
\newlabel{imp_activation_functions@cref}{{[subsubsection][2][4,2]4.2.2}{29}}
\@writefile{toc}{\contentsline {subparagraph}{Sigmoid}{29}{subsubsection.4.2.2}}
\newlabel{func_sigmoid}{{17}{29}{Sigmoid}{equation.4.17}{}}
\newlabel{func_sigmoid@cref}{{[equation][17][]17}{29}}
\newlabel{func_sigmoidprime}{{18}{29}{Sigmoid}{equation.4.18}{}}
\newlabel{func_sigmoidprime@cref}{{[equation][18][]18}{29}}
\@writefile{toc}{\contentsline {subparagraph}{ReLU}{30}{equation.4.18}}
\newlabel{func_relu}{{19}{30}{ReLU}{equation.4.19}{}}
\newlabel{func_relu@cref}{{[equation][19][]19}{30}}
\newlabel{func_relu_prime}{{4.2.2}{30}{ReLU}{equation.4.20}{}}
\newlabel{func_relu_prime@cref}{{[subsubsection][2][4,2]4.2.2}{30}}
\@writefile{toc}{\contentsline {subparagraph}{Leaky ReLU}{30}{equation.4.20}}
\newlabel{func_leaky_relu}{{4.2.2}{30}{Leaky ReLU}{equation.4.21}{}}
\newlabel{func_leaky_relu@cref}{{[subsubsection][2][4,2]4.2.2}{30}}
\newlabel{func_leaky_relu_prime}{{4.2.2}{30}{Leaky ReLU}{equation.4.22}{}}
\newlabel{func_leaky_relu_prime@cref}{{[subsubsection][2][4,2]4.2.2}{30}}
\@writefile{toc}{\contentsline {subparagraph}{Linear Activation}{30}{equation.4.22}}
\newlabel{func_linear}{{23}{30}{Linear Activation}{equation.4.23}{}}
\newlabel{func_linear@cref}{{[equation][23][]23}{30}}
\newlabel{func_linear_prime}{{24}{30}{Linear Activation}{equation.4.24}{}}
\newlabel{func_linear_prime@cref}{{[equation][24][]24}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Backpropagation}{31}{subsubsection.4.2.3}}
\newlabel{imp_backprop}{{4.2.3}{31}{Backpropagation}{subsubsection.4.2.3}{}}
\newlabel{imp_backprop@cref}{{[subsubsection][3][4,2]4.2.3}{31}}
\newlabel{func_MSE}{{25}{31}{Backpropagation}{equation.4.25}{}}
\newlabel{func_MSE@cref}{{[equation][25][]25}{31}}
\newlabel{eq_lambda1}{{26}{31}{Backpropagation}{equation.4.26}{}}
\newlabel{eq_lambda1@cref}{{[equation][26][]26}{31}}
\newlabel{eq_lambda2}{{27}{31}{Backpropagation}{equation.4.27}{}}
\newlabel{eq_lambda2@cref}{{[equation][27][]27}{31}}
\newlabel{eq_lambda3}{{28}{31}{Backpropagation}{equation.4.28}{}}
\newlabel{eq_lambda3@cref}{{[equation][28][]28}{31}}
\newlabel{eq_lambda4}{{29}{31}{Backpropagation}{equation.4.29}{}}
\newlabel{eq_lambda4@cref}{{[equation][29][]29}{31}}
\newlabel{eq_bp_weightupdate}{{30}{32}{Backpropagation}{equation.4.30}{}}
\newlabel{eq_bp_weightupdate@cref}{{[equation][30][]30}{32}}
\newlabel{algo_backprop}{{2}{32}{Backpropagation}{algocfline.2}{}}
\newlabel{algo_backprop@cref}{{[line][2][]2}{32}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Backpropagation\relax }}{32}{algocf.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Gradient Descent Algorithms}{32}{subsubsection.4.2.4}}
\newlabel{imp_sgd}{{4.2.4}{32}{Gradient Descent Algorithms}{subsubsection.4.2.4}{}}
\newlabel{imp_sgd@cref}{{[subsubsection][4][4,2]4.2.4}{32}}
\newlabel{eq_backprop_weightupdate_sgd}{{31}{33}{Gradient Descent Algorithms}{equation.4.31}{}}
\newlabel{eq_backprop_weightupdate_sgd@cref}{{[equation][31][]31}{33}}
\@writefile{toc}{\contentsline {subparagraph}{Online Gradient Descent}{33}{equation.4.31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Regularization}{33}{subsubsection.4.2.5}}
\newlabel{imp_regularization}{{4.2.5}{33}{Regularization}{subsubsection.4.2.5}{}}
\newlabel{imp_regularization@cref}{{[subsubsection][5][4,2]4.2.5}{33}}
\newlabel{func_l2reg}{{32}{33}{Regularization}{equation.4.32}{}}
\newlabel{func_l2reg@cref}{{[equation][32][]32}{33}}
\newlabel{func_l2_weight_update}{{33}{33}{Regularization}{equation.4.33}{}}
\newlabel{func_l2_weight_update@cref}{{[equation][33][]33}{33}}
\newlabel{func_sgd_l2}{{34}{33}{Regularization}{equation.4.34}{}}
\newlabel{func_sgd_l2@cref}{{[equation][34][]34}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.6}Learning Rate Schedule}{33}{subsubsection.4.2.6}}
\newlabel{imp_learning_rate_schedule}{{4.2.6}{33}{Learning Rate Schedule}{subsubsection.4.2.6}{}}
\newlabel{imp_learning_rate_schedule@cref}{{[subsubsection][6][4,2]4.2.6}{33}}
\citation{Smith}
\newlabel{func_learning_rate_sched}{{35}{34}{Learning Rate Schedule}{equation.4.35}{}}
\newlabel{func_learning_rate_sched@cref}{{[equation][35][]35}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning rates calculated over 1000 epochs with $\eta _{\qopname  \relax m{min}} = 0.1$ to $\eta _{\qopname  \relax m{max}} = 1.0$ and $i=100$\relax }}{34}{figure.caption.9}}
\newlabel{figure-SGDRLearningRates}{{3}{34}{Learning rates calculated over 1000 epochs with $\eta _{\min } = 0.1$ to $\eta _{\max } = 1.0$ and $i=100$\relax }{figure.caption.9}{}}
\newlabel{figure-SGDRLearningRates@cref}{{[figure][3][]3}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.7}Dropout}{34}{subsubsection.4.2.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Restricted Boltzmann Machines}{34}{subsection.4.3}}
\newlabel{imp_rbm}{{4.3}{34}{Restricted Boltzmann Machines}{subsection.4.3}{}}
\newlabel{imp_rbm@cref}{{[subsection][3][4]4.3}{34}}
\citation{Hinton5}
\citation{Hinton5}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example diagram of a Restricted Boltzmann Machine network\relax }}{35}{figure.caption.10}}
\newlabel{figure-rbm_network_diagram}{{4}{35}{An example diagram of a Restricted Boltzmann Machine network\relax }{figure.caption.10}{}}
\newlabel{figure-rbm_network_diagram@cref}{{[figure][4][]4}{35}}
\newlabel{func_rbmenergy}{{36}{35}{Restricted Boltzmann Machines}{equation.4.36}{}}
\newlabel{func_rbmenergy@cref}{{[equation][36][]36}{35}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Contrastive Divergence}{36}{subsubsection.4.3.1}}
\newlabel{imp_CD}{{4.3.1}{36}{Contrastive Divergence}{subsubsection.4.3.1}{}}
\newlabel{imp_CD@cref}{{[subsubsection][1][4,3]4.3.1}{36}}
\newlabel{algo_cd1}{{3}{36}{Contrastive Divergence}{equation.4.41}{}}
\newlabel{algo_cd1@cref}{{[line][3][]3}{36}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces CD-1\relax }}{36}{algocf.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}CD-1 and SGD}{36}{subsubsection.4.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Stacked Autoencoders}{36}{subsection.4.4}}
\newlabel{imp_SAE}{{4.4}{36}{Stacked Autoencoders}{subsection.4.4}{}}
\newlabel{imp_SAE@cref}{{[subsection][4][4]4.4}{36}}
\citation{Hinton2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Sigmoid based Greedy Layerwise SAE Training}{37}{subsubsection.4.4.1}}
\newlabel{imp_sigmoidsae}{{4.4.1}{37}{Sigmoid based Greedy Layerwise SAE Training}{subsubsection.4.4.1}{}}
\newlabel{imp_sigmoidsae@cref}{{[subsubsection][1][4,4]4.4.1}{37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}ReLU based SAE Training}{37}{subsubsection.4.4.2}}
\newlabel{imp_relusae}{{4.4.2}{37}{ReLU based SAE Training}{subsubsection.4.4.2}{}}
\newlabel{imp_relusae@cref}{{[subsubsection][2][4,4]4.4.2}{37}}
\citation{He}
\citation{Glorot}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Denoising Autoencoders}{38}{subsubsection.4.4.3}}
\@writefile{toc}{\contentsline {subparagraph}{Additive Gaussian Noise}{38}{subsubsection.4.4.3}}
\@writefile{toc}{\contentsline {subparagraph}{Masking Noise}{38}{equation.4.42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Variance Based Weight Initializations}{38}{subsection.4.5}}
\newlabel{imp_weights}{{4.5}{38}{Variance Based Weight Initializations}{subsection.4.5}{}}
\newlabel{imp_weights@cref}{{[subsection][5][4]4.5}{38}}
\newlabel{func_uniform_init}{{43}{38}{Variance Based Weight Initializations}{equation.4.43}{}}
\newlabel{func_uniform_init@cref}{{[equation][43][]43}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Initialization Rationale}{38}{subsubsection.4.5.1}}
\citation{Glorot}
\citation{He}
\newlabel{eq_init_var}{{47}{39}{Initialization Rationale}{equation.4.47}{}}
\newlabel{eq_init_var@cref}{{[equation][47][]47}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Initializations}{39}{subsubsection.4.5.2}}
\@writefile{toc}{\contentsline {subparagraph}{Xavier}{39}{subsubsection.4.5.2}}
\@writefile{toc}{\contentsline {subparagraph}{He}{39}{equation.4.48}}
\@writefile{toc}{\contentsline {subparagraph}{He-Adjusted}{39}{equation.4.49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}CSCV \& PBO}{39}{subsection.4.6}}
\newlabel{imp_cscv}{{4.6}{39}{CSCV \& PBO}{subsection.4.6}{}}
\newlabel{imp_cscv@cref}{{[subsection][6][4]4.6}{39}}
\citation{BailyPBO}
\newlabel{eq:PBO1}{{51}{40}{CSCV \& PBO}{equation.4.51}{}}
\newlabel{eq:PBO1@cref}{{[equation][51][]51}{40}}
\newlabel{eq:PBO2}{{52}{40}{CSCV \& PBO}{equation.4.52}{}}
\newlabel{eq:PBO2@cref}{{[equation][52][]52}{40}}
\newlabel{algo_cscv}{{4}{41}{CSCV \& PBO}{equation.4.53}{}}
\newlabel{algo_cscv@cref}{{[line][4][]4}{41}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces CSCV\relax }}{41}{algocf.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Money Management Strategy and Returns}{42}{subsection.4.7}}
\newlabel{imp_mms}{{4.7}{42}{Money Management Strategy and Returns}{subsection.4.7}{}}
\newlabel{imp_mms@cref}{{[subsection][7][4]4.7}{42}}
\@writefile{toc}{\contentsline {subparagraph}{Input Variables}{42}{subsection.4.7}}
\@writefile{toc}{\contentsline {subparagraph}{Calculated Stock Variables}{42}{subsection.4.7}}
\@writefile{toc}{\contentsline {subparagraph}{Calculated Strategy Variables}{43}{equation.4.64}}
\citation{Wilcox}
\@writefile{toc}{\contentsline {section}{\numberline {5}Implementation: Process}{44}{section.5}}
\newlabel{imp_proc}{{5}{44}{Implementation: Process}{section.5}{}}
\newlabel{imp_proc@cref}{{[section][5][]5}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data Preparation}{44}{subsection.5.1}}
\newlabel{proc_dataprep}{{5.1}{44}{Data Preparation}{subsection.5.1}{}}
\newlabel{proc_dataprep@cref}{{[subsection][1][5]5.1}{44}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Data Window Aggregations}{44}{subsubsection.5.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Point Predictions}{44}{subsubsection.5.1.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add result reference}{44}{section*.11}}
\pgfsyspdfmark {pgfid31}{8349364}{8610957}
\pgfsyspdfmark {pgfid34}{36009438}{8644517}
\pgfsyspdfmark {pgfid35}{37631454}{8399331}
\citation{BailyPBO}
\citation{BaileyBTL}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add result references}{45}{section*.12}}
\pgfsyspdfmark {pgfid36}{26819891}{37729588}
\pgfsyspdfmark {pgfid39}{36009438}{37763148}
\pgfsyspdfmark {pgfid40}{37631454}{37517962}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Scaling}{45}{subsubsection.5.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Data Segregation}{45}{subsection.5.2}}
\newlabel{proc_dataseg}{{5.2}{45}{Data Segregation}{subsection.5.2}{}}
\newlabel{proc_dataseg@cref}{{[subsection][2][5]5.2}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}SAE Training}{45}{subsection.5.3}}
\newlabel{proc_sae}{{5.3}{45}{SAE Training}{subsection.5.3}{}}
\newlabel{proc_sae@cref}{{[subsection][3][5]5.3}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Prediction Network Training}{46}{subsection.5.4}}
\newlabel{proc_predictionnetwork}{{5.4}{46}{Prediction Network Training}{subsection.5.4}{}}
\newlabel{proc_predictionnetwork@cref}{{[subsection][4][5]5.4}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Price Reconstruction}{46}{subsection.5.5}}
\newlabel{proc_precerecon}{{5.5}{46}{Price Reconstruction}{subsection.5.5}{}}
\newlabel{proc_precerecon@cref}{{[subsection][5][5]5.5}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Money Management Strategy}{46}{subsection.5.6}}
\newlabel{proc_mms}{{5.6}{46}{Money Management Strategy}{subsection.5.6}{}}
\newlabel{proc_mms@cref}{{[subsection][6][5]5.6}{46}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ Can we learn to beat best stock paper ? Paper}{47}{section*.13}}
\pgfsyspdfmark {pgfid41}{7555917}{36943156}
\pgfsyspdfmark {pgfid44}{36009438}{36976716}
\pgfsyspdfmark {pgfid45}{37631454}{36731530}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}CSCV \& PBO}{47}{subsection.5.7}}
\newlabel{proc_cscv}{{5.7}{47}{CSCV \& PBO}{subsection.5.7}{}}
\newlabel{proc_cscv@cref}{{[subsection][7][5]5.7}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Process Diagram}{47}{subsection.5.8}}
\newlabel{proc_diagram}{{5.8}{47}{Process Diagram}{subsection.5.8}{}}
\newlabel{proc_diagram@cref}{{[subsection][8][5]5.8}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Overall Process Flow\relax }}{48}{figure.caption.14}}
\newlabel{figure-proc_diagram}{{5}{48}{Overall Process Flow\relax }{figure.caption.14}{}}
\newlabel{figure-proc_diagram@cref}{{[figure][5][]5}{48}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Datasets Used}{49}{section.6}}
\newlabel{Datasets}{{6}{49}{Datasets Used}{section.6}{}}
\newlabel{Datasets@cref}{{[section][6][]6}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Synthetic Datasets}{49}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Synthetic6}{49}{subsubsection.6.1.1}}
\newlabel{dataset_synthetic6}{{6.1.1}{49}{Synthetic6}{subsubsection.6.1.1}{}}
\newlabel{dataset_synthetic6@cref}{{[subsubsection][1][6,1]6.1.1}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Synthetic 6 Dataset Configuration\relax }}{49}{table.caption.15}}
\newlabel{tab_synth6}{{1}{49}{Synthetic 6 Dataset Configuration\relax }{table.caption.15}{}}
\newlabel{tab_synth6@cref}{{[table][1][]1}{49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Synthetic10}{49}{subsubsection.6.1.2}}
\newlabel{dataset_synthetic10}{{6.1.2}{49}{Synthetic10}{subsubsection.6.1.2}{}}
\newlabel{dataset_synthetic10@cref}{{[subsubsection][2][6,1]6.1.2}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Synthetic 10 Dataset Configuration\relax }}{50}{table.caption.16}}
\newlabel{tab_synth10}{{2}{50}{Synthetic 10 Dataset Configuration\relax }{table.caption.16}{}}
\newlabel{tab_synth10@cref}{{[table][2][]2}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Actual Datasets}{50}{subsection.6.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add ref}{50}{section*.17}}
\pgfsyspdfmark {pgfid46}{31755965}{23132196}
\pgfsyspdfmark {pgfid49}{36009438}{23165756}
\pgfsyspdfmark {pgfid50}{37631454}{22920570}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Actual10}{50}{subsubsection.6.2.1}}
\newlabel{dataset_actual10}{{6.2.1}{50}{Actual10}{subsubsection.6.2.1}{}}
\newlabel{dataset_actual10@cref}{{[subsubsection][1][6,2]6.2.1}{50}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Sytnetic 10 Dataset Configuration\relax }}{51}{table.caption.18}}
\newlabel{tab_actual10}{{3}{51}{Sytnetic 10 Dataset Configuration\relax }{table.caption.18}{}}
\newlabel{tab_actual10@cref}{{[table][3][]3}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}AGL}{51}{subsubsection.6.2.2}}
\newlabel{dataset_agl}{{6.2.2}{51}{AGL}{subsubsection.6.2.2}{}}
\newlabel{dataset_agl@cref}{{[subsubsection][2][6,2]6.2.2}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}AGL\&ACL}{51}{subsubsection.6.2.3}}
\newlabel{dataset_aglacl}{{6.2.3}{51}{AGL\&ACL}{subsubsection.6.2.3}{}}
\newlabel{dataset_aglacl@cref}{{[subsubsection][3][6,2]6.2.3}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4}Scaling10}{51}{subsubsection.6.2.4}}
\newlabel{dataset_scaling10}{{6.2.4}{51}{Scaling10}{subsubsection.6.2.4}{}}
\newlabel{dataset_scaling10@cref}{{[subsubsection][4][6,2]6.2.4}{51}}
\citation{Peters}
\citation{Hinton2}
\citation{Glorot2}
\@writefile{toc}{\contentsline {section}{\numberline {7}Results}{52}{section.7}}
\newlabel{Results}{{7}{52}{Results}{section.7}{}}
\newlabel{Results@cref}{{[section][7][]7}{52}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ }{52}{section*.19}}
\pgfsyspdfmark {pgfid51}{30844910}{45214588}
\pgfsyspdfmark {pgfid54}{36009438}{45248148}
\pgfsyspdfmark {pgfid55}{37631454}{45002962}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Linearity, Complexity and Structure of Data}{52}{subsection.7.1}}
\newlabel{results_linearity}{{7.1}{52}{Linearity, Complexity and Structure of Data}{subsection.7.1}{}}
\newlabel{results_linearity@cref}{{[subsection][1][7]7.1}{52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}GBM Generated Data}{52}{subsubsection.7.1.1}}
\newlabel{results_gbm_data}{{7.1.1}{52}{GBM Generated Data}{subsubsection.7.1.1}{}}
\newlabel{results_gbm_data@cref}{{[subsubsection][1][7,1]7.1.1}{52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Activations: Linear, Sigmoid, ReLU and Leaky ReLU}{52}{subsubsection.7.1.2}}
\@writefile{toc}{\contentsline {subparagraph}{SAE Activations and Scaling}{52}{subsubsection.7.1.2}}
\newlabel{results_sae_activations_scaling}{{7.1.2}{52}{SAE Activations and Scaling}{subsubsection.7.1.2}{}}
\newlabel{results_sae_activations_scaling@cref}{{[subsubsection][2][7,1]7.1.2}{52}}
\citation{Hinton2}
\citation{Hinton2}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Scaling10 dataset (\ref  {dataset_scaling10}) ; Configuration1 (\ref  {config1}) - 1316 Samples \newline  \newline  Each configuration group is labelled in the following format: 'Hidden Activation-Encoding Activation-Output Activation-Scaling Technique. The MSE scores show significantly poorer performance when Standardizing is used instead of Normalizing or when there is a ReLU output activation. These configurations are excluded from figure \ref  {figure-results-linear-act}.\relax }}{53}{figure.caption.20}}
\newlabel{figure-results-scaling-and-relu}{{6}{53}{Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 1316 Samples \newline \newline Each configuration group is labelled in the following format: 'Hidden Activation-Encoding Activation-Output Activation-Scaling Technique. The MSE scores show significantly poorer performance when Standardizing is used instead of Normalizing or when there is a ReLU output activation. These configurations are excluded from figure \ref {figure-results-linear-act}.\relax }{figure.caption.20}{}}
\newlabel{figure-results-scaling-and-relu@cref}{{[figure][6][]6}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Scaling10 dataset (\ref  {dataset_scaling10}) ; Configuration1 (\ref  {config1}) - 504 Samples \newline  \newline  Excluding the Standardizing scaling combinations, we can now see that Sigmoid has largely poor performance unless there is a Linear encoding layer (a commonly experienced behaviour \cite  {Hinton2}]), and seems mostly unable to outperform a fully linear network. The best configuration is Relu Hidden layers, with Linear encoding and Output - this makes sense with the non-linear benefit at hidden layers but with less loss of error signal and information at the output and encoding layers.\relax }}{54}{figure.caption.21}}
\newlabel{figure-results-linear-act}{{7}{54}{Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 504 Samples \newline \newline Excluding the Standardizing scaling combinations, we can now see that Sigmoid has largely poor performance unless there is a Linear encoding layer (a commonly experienced behaviour \cite {Hinton2}]), and seems mostly unable to outperform a fully linear network. The best configuration is Relu Hidden layers, with Linear encoding and Output - this makes sense with the non-linear benefit at hidden layers but with less loss of error signal and information at the output and encoding layers.\relax }{figure.caption.21}{}}
\newlabel{figure-results-linear-act@cref}{{[figure][7][]7}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Scaling10 dataset (\ref  {dataset_scaling10}) ; Configuration1 (\ref  {config1}) - 168 Samples \newline  \newline  These plots show the performance for all configurations with an encoding layer size of 25 (input 30). The fully linear networks show good performance here, where there is greater scope for linear representation in the encoding.\relax }}{54}{figure.caption.22}}
\newlabel{figure-results-encoding25}{{8}{54}{Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 168 Samples \newline \newline These plots show the performance for all configurations with an encoding layer size of 25 (input 30). The fully linear networks show good performance here, where there is greater scope for linear representation in the encoding.\relax }{figure.caption.22}{}}
\newlabel{figure-results-encoding25@cref}{{[figure][8][]8}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Scaling10 dataset (\ref  {dataset_scaling10}) ; Configuration1 (\ref  {config1}) - 168 Samples \newline  \newline  These plots show the performance for all configurations with an encoding layer size of 5 (input 30). Here we now see the benefit of non-linear activations in the ReLU based networks, which the fully linear is not able to outperform.\relax }}{55}{figure.caption.23}}
\newlabel{figure-results-encoding5}{{9}{55}{Scaling10 dataset (\ref {dataset_scaling10}) ; Configuration1 (\ref {config1}) - 168 Samples \newline \newline These plots show the performance for all configurations with an encoding layer size of 5 (input 30). Here we now see the benefit of non-linear activations in the ReLU based networks, which the fully linear is not able to outperform.\relax }{figure.caption.23}{}}
\newlabel{figure-results-encoding5@cref}{{[figure][9][]9}{55}}
\@writefile{toc}{\contentsline {subparagraph}{Predictive FFN Activations and Scaling}{55}{figure.caption.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Dataset Synthetic6 (\ref  {dataset_synthetic6}) ; Configuration2 (\ref  {config2}) - 720 Configurations \newline  \newline  The above groupings show the P\&L generated for different combinations of Hidden Activation - Output Activation - Scaling Method (which is for both SAE and FFN). For an input of 18, and network sizes of 40 and 80 with hidden layers of 1 an 3, we see the linear activations showing notably better performance than the ReLU activations. \relax }}{56}{figure.caption.24}}
\newlabel{figure-results-linear-ffn_activations}{{10}{56}{Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration2 (\ref {config2}) - 720 Configurations \newline \newline The above groupings show the P\&L generated for different combinations of Hidden Activation - Output Activation - Scaling Method (which is for both SAE and FFN). For an input of 18, and network sizes of 40 and 80 with hidden layers of 1 an 3, we see the linear activations showing notably better performance than the ReLU activations. \relax }{figure.caption.24}{}}
\newlabel{figure-results-linear-ffn_activations@cref}{{[figure][10][]10}{56}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ format properly}{56}{section*.25}}
\pgfsyspdfmark {pgfid56}{8538958}{27395594}
\pgfsyspdfmark {pgfid59}{36009438}{27429154}
\pgfsyspdfmark {pgfid60}{37631454}{27183968}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Dataset Synthetic6 (\ref  {dataset_synthetic6}) ; Configuration2 (\ref  {config2}) - 720 Configurations \newline  \newline  The boxplots here show the P\&L generated for networks with smaller layer sizes, as indicated by the group naming (each number represents a hidden layer and its node size). The networks were trained on the Synthetic6 dataset with 18 inputs, different sized autoencoders and learning rates. We once again see the outperformance of ReLU by the linear activations.\relax }}{56}{figure.caption.26}}
\newlabel{figure-results_linear_vs_relu}{{11}{56}{Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration2 (\ref {config2}) - 720 Configurations \newline \newline The boxplots here show the P\&L generated for networks with smaller layer sizes, as indicated by the group naming (each number represents a hidden layer and its node size). The networks were trained on the Synthetic6 dataset with 18 inputs, different sized autoencoders and learning rates. We once again see the outperformance of ReLU by the linear activations.\relax }{figure.caption.26}{}}
\newlabel{figure-results_linear_vs_relu@cref}{{[figure][11][]11}{56}}
\@writefile{toc}{\contentsline {subparagraph}{Leaky ReLU vs ReLU}{57}{figure.caption.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Dataset Synthetic6 (\ref  {dataset_synthetic6}) ; Configuration - 120 configurations \newline  \newline  The plot above shows the SAE MSE, grouped by encoding size and activations, showing a marginal increase in performance for Leaky ReLU activations. \relax }}{57}{figure.caption.27}}
\newlabel{figure-results_leaky_relu_1}{{12}{57}{Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration - 120 configurations \newline \newline The plot above shows the SAE MSE, grouped by encoding size and activations, showing a marginal increase in performance for Leaky ReLU activations. \relax }{figure.caption.27}{}}
\newlabel{figure-results_leaky_relu_1@cref}{{[figure][12][]12}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Dataset Synthetic6 (\ref  {dataset_synthetic6}) ; Configuration - 80 configurations \newline  \newline  The plot shows the FFN P\&L, grouped by activation, showing some improvements from the Leaky ReLU activation.\relax }}{57}{figure.caption.28}}
\newlabel{figure-results_leaky_relu_3}{{13}{57}{Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration - 80 configurations \newline \newline The plot shows the FFN P\&L, grouped by activation, showing some improvements from the Leaky ReLU activation.\relax }{figure.caption.28}{}}
\newlabel{figure-results_leaky_relu_3@cref}{{[figure][13][]13}{57}}
\citation{Hinton2}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Weight Initialization Techniques}{58}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}RBM Pretraining for Sigmoid Networks}{58}{subsubsection.7.2.1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add result ref}{58}{section*.29}}
\pgfsyspdfmark {pgfid61}{11453497}{33276832}
\pgfsyspdfmark {pgfid64}{36009438}{33310392}
\pgfsyspdfmark {pgfid65}{37631454}{33065206}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Dataset AGL\&ACL (\ref  {dataset_aglacl}); Configuration7 (\ref  {config7}) \newline  \newline  The boxplots here show the summary of configuration performances, by minimum MSE achieved, grouped according to the number of pre-training epochs which the network had. There is a clear favour to having no pre-training in this scenario.\relax }}{58}{figure.caption.30}}
\newlabel{figure-results-pretraining-effect}{{14}{58}{Dataset AGL\&ACL (\ref {dataset_aglacl}); Configuration7 (\ref {config7}) \newline \newline The boxplots here show the summary of configuration performances, by minimum MSE achieved, grouped according to the number of pre-training epochs which the network had. There is a clear favour to having no pre-training in this scenario.\relax }{figure.caption.30}{}}
\newlabel{figure-results-pretraining-effect@cref}{{[figure][14][]14}{58}}
\@writefile{toc}{\contentsline {subparagraph}{Sigmoid Activation Functions}{58}{figure.caption.30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Variance Based Weight Initialization Techniques}{59}{subsubsection.7.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}); Configuration 5 (\ref  {config5}) \newline  The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the similar performance of He-Adj and Xavier on synthetic datasets.\relax }}{60}{figure.caption.31}}
\newlabel{figure-results_it4_sae_init}{{15}{60}{Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configuration 5 (\ref {config5}) \newline The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the similar performance of He-Adj and Xavier on synthetic datasets.\relax }{figure.caption.31}{}}
\newlabel{figure-results_it4_sae_init@cref}{{[figure][15][]15}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Dataset Actual10 \newline  The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the outperformance of He-Adj to Xavier on actual datasets.\relax }}{60}{figure.caption.32}}
\newlabel{figure-results_init_actual10_all}{{16}{60}{Dataset Actual10 \newline The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the outperformance of He-Adj to Xavier on actual datasets.\relax }{figure.caption.32}{}}
\newlabel{figure-results_init_actual10_all@cref}{{[figure][16][]16}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Dataset: AGL (\ref  {dataset_agl}); Configuration 3 (\ref  {config3}) \newline  The configurations run for AGL show notably better performance using the He-Adj based initialization in comparison to both He and Xavier. The emphasis seen here can be attributed to increased pressure on suitable weight initizliations as 1 and 2 node encoding layers were used.\relax }}{61}{figure.caption.33}}
\newlabel{figure-results_init_agl_all}{{17}{61}{Dataset: AGL (\ref {dataset_agl}); Configuration 3 (\ref {config3}) \newline The configurations run for AGL show notably better performance using the He-Adj based initialization in comparison to both He and Xavier. The emphasis seen here can be attributed to increased pressure on suitable weight initizliations as 1 and 2 node encoding layers were used.\relax }{figure.caption.33}{}}
\newlabel{figure-results_init_agl_all@cref}{{[figure][17][]17}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}); Configurations 5 and 6 (\ref  {config5}, \ref  {config6}) \newline  The figure here shows the P\&L performance for He-Adj and He being higher than Xavier, as to be expected with more balanced network layers and ReLU activations. The better performance in the lower end of the He-Adj and configurations when compared to He can be attributed to the inclusion of inconsistently sized layers in the configuration set, where He-Adj and would have performed better.\relax }}{61}{figure.caption.34}}
\newlabel{figure-init4_ffn_init}{{18}{61}{Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configurations 5 and 6 (\ref {config5}, \ref {config6}) \newline The figure here shows the P\&L performance for He-Adj and He being higher than Xavier, as to be expected with more balanced network layers and ReLU activations. The better performance in the lower end of the He-Adj and configurations when compared to He can be attributed to the inclusion of inconsistently sized layers in the configuration set, where He-Adj and would have performed better.\relax }{figure.caption.34}{}}
\newlabel{figure-init4_ffn_init@cref}{{[figure][18][]18}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Dataset: AGL (\ref  {dataset_agl}); Configuration 4 (\ref  {config4}) \newline  The configurations run for AGL show notably better performance using the He-Adj and and He based initialization in comparison Xavier when using actual data, as per the differences explained above.\relax }}{62}{figure.caption.35}}
\newlabel{figure-init5_ffn_init}{{19}{62}{Dataset: AGL (\ref {dataset_agl}); Configuration 4 (\ref {config4}) \newline The configurations run for AGL show notably better performance using the He-Adj and and He based initialization in comparison Xavier when using actual data, as per the differences explained above.\relax }{figure.caption.35}{}}
\newlabel{figure-init5_ffn_init@cref}{{[figure][19][]19}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Feature Selection}{63}{subsection.7.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Dataset: ; Configuration \newline  Actual P\&L grouped according to feature selection size (where 0 is no encoding), showing increased performance the closer feature selection is to the number of assets (in this case, 10).\relax }}{63}{figure.caption.36}}
\newlabel{figure-results_encoding_actual}{{20}{63}{Dataset: ; Configuration \newline Actual P\&L grouped according to feature selection size (where 0 is no encoding), showing increased performance the closer feature selection is to the number of assets (in this case, 10).\relax }{figure.caption.36}{}}
\newlabel{figure-results_encoding_actual@cref}{{[figure][20][]20}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}) ; Configurations 5 \& 6 (\ref  {config5}, \ref  {config6}) \newline  Synthetic P\&L grouped according to feature selection size.\relax }}{64}{figure.caption.37}}
\newlabel{figure-results_encoding_synthetic}{{21}{64}{Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations 5 \& 6 (\ref {config5}, \ref {config6}) \newline Synthetic P\&L grouped according to feature selection size.\relax }{figure.caption.37}{}}
\newlabel{figure-results_encoding_synthetic@cref}{{[figure][21][]21}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Network Structure and Training}{66}{subsection.7.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Effects of Network Size}{66}{subsubsection.7.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  Actual SAE MSE by network size.\relax }}{66}{figure.caption.38}}
\newlabel{figure-results_sae_size_agl}{{22}{66}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual SAE MSE by network size.\relax }{figure.caption.38}{}}
\newlabel{figure-results_sae_size_agl@cref}{{[figure][22][]22}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  Actual P\&L by network size.\relax }}{67}{figure.caption.39}}
\newlabel{figure-results_ffn_size_agl}{{23}{67}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by network size.\relax }{figure.caption.39}{}}
\newlabel{figure-results_ffn_size_agl@cref}{{[figure][23][]23}{67}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Learning Rate Schedules}{67}{subsubsection.7.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  Actual P\&L by SGD Learning rates.\relax }}{67}{figure.caption.40}}
\newlabel{figure-sgd_lr}{{24}{67}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by SGD Learning rates.\relax }{figure.caption.40}{}}
\newlabel{figure-sgd_lr@cref}{{[figure][24][]24}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  Actual P\&L by OGD Learning rates.\relax }}{68}{figure.caption.41}}
\newlabel{figure-ogd_lr}{{25}{68}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by OGD Learning rates.\relax }{figure.caption.41}{}}
\newlabel{figure-ogd_lr@cref}{{[figure][25][]25}{68}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Regularization}{68}{subsubsection.7.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  Actual P\&L by L1 Regularization rates.\relax }}{68}{figure.caption.42}}
\newlabel{figure-sgd_reg}{{26}{68}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by L1 Regularization rates.\relax }{figure.caption.42}{}}
\newlabel{figure-sgd_reg@cref}{{[figure][26][]26}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  Actual P\&L by L1 Regularization rates.\relax }}{69}{figure.caption.43}}
\newlabel{figure-ogd_reg}{{27}{69}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by L1 Regularization rates.\relax }{figure.caption.43}{}}
\newlabel{figure-ogd_reg@cref}{{[figure][27][]27}{69}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.4}Dropout}{69}{subsubsection.7.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  Actual P\&L by dropout rates.\relax }}{69}{figure.caption.44}}
\newlabel{figure-ogd_dropout}{{28}{69}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline Actual P\&L by dropout rates.\relax }{figure.caption.44}{}}
\newlabel{figure-ogd_dropout@cref}{{[figure][28][]28}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Data Aggregation and Selection}{70}{subsection.7.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}Data Aggregation}{70}{subsubsection.7.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces \textbf  {Synthtic SAE Data Window Aggregations} \newline  \relax }}{70}{figure.caption.45}}
\newlabel{figure-results_sae_deltas_synth}{{29}{70}{\textbf {Synthtic SAE Data Window Aggregations} \newline \relax }{figure.caption.45}{}}
\newlabel{figure-results_sae_deltas_synth@cref}{{[figure][29][]29}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces \textbf  {Actual SAE Data Window Aggregations} \newline  \relax }}{70}{figure.caption.46}}
\newlabel{figure-results_sae_deltas_actual}{{30}{70}{\textbf {Actual SAE Data Window Aggregations} \newline \relax }{figure.caption.46}{}}
\newlabel{figure-results_sae_deltas_actual@cref}{{[figure][30][]30}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces \textbf  {Synthetic Predictive P\&L Data Window Aggregations} \newline  \relax }}{71}{figure.caption.47}}
\newlabel{figure-it5_ffn_deltas}{{31}{71}{\textbf {Synthetic Predictive P\&L Data Window Aggregations} \newline \relax }{figure.caption.47}{}}
\newlabel{figure-it5_ffn_deltas@cref}{{[figure][31][]31}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces \textbf  {Actual Predictive P\&L Data Window Aggregations} \newline  \relax }}{71}{figure.caption.48}}
\newlabel{figure-results_pl_deltas_actual}{{32}{71}{\textbf {Actual Predictive P\&L Data Window Aggregations} \newline \relax }{figure.caption.48}{}}
\newlabel{figure-results_pl_deltas_actual@cref}{{[figure][32][]32}{71}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}Historical Data and SGD training}{72}{subsubsection.7.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces \textbf  {Actual Predictive P\&L by SGD Max Epochs} \newline  \relax }}{72}{figure.caption.49}}
\newlabel{figure-results_pl_max_epochs}{{33}{72}{\textbf {Actual Predictive P\&L by SGD Max Epochs} \newline \relax }{figure.caption.49}{}}
\newlabel{figure-results_pl_max_epochs@cref}{{[figure][33][]33}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces \textbf  {SGD Training Dataset Size - 6 Synthetic Assets} \newline  The plot above shows the P\&L for FFN predictive networks, grouped by the percentage of data excluded from the SGD training. While there is a decrease in P\&L, it is much less than the decrease in data, and may only be a result of the reduced samples used to train (due to a constant number of SGD epochs)\relax }}{72}{figure.caption.50}}
\newlabel{figure-results_it3_validationset}{{34}{72}{\textbf {SGD Training Dataset Size - 6 Synthetic Assets} \newline The plot above shows the P\&L for FFN predictive networks, grouped by the percentage of data excluded from the SGD training. While there is a decrease in P\&L, it is much less than the decrease in data, and may only be a result of the reduced samples used to train (due to a constant number of SGD epochs)\relax }{figure.caption.50}{}}
\newlabel{figure-results_it3_validationset@cref}{{[figure][34][]34}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}PBO Results}{72}{subsection.7.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}MMS Results}{73}{subsection.7.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  The PDF of all OOS P\&L values, with the benchmark P\&L indicated in orange.\relax }}{73}{figure.caption.51}}
\newlabel{figure-results_pl_pdf}{{35}{73}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The PDF of all OOS P\&L values, with the benchmark P\&L indicated in orange.\relax }{figure.caption.51}{}}
\newlabel{figure-results_pl_pdf@cref}{{[figure][35][]35}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  The confusion matrix for the network with the highest OOS P\&L.\relax }}{73}{figure.caption.52}}
\newlabel{figure-results_confusion}{{36}{73}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The confusion matrix for the network with the highest OOS P\&L.\relax }{figure.caption.52}{}}
\newlabel{figure-results_confusion@cref}{{[figure][36][]36}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8}Results Summary}{74}{subsection.7.8}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusions}{75}{section.8}}
\newlabel{Conclusion}{{8}{75}{Conclusions}{section.8}{}}
\newlabel{Conclusion@cref}{{[section][8][]8}{75}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Appendix}{76}{section.9}}
\newlabel{Appendix}{{9}{76}{Appendix}{section.9}{}}
\newlabel{Appendix@cref}{{[section][9][]9}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Additional Results}{76}{subsection.9.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}AGLEncoding Layer Results}{76}{subsubsection.9.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Dataset: AGL (\ref  {dataset_agl}) ; Configurations 3 \& 4 (\ref  {config3}, \ref  {config4}) \newline  This figure shows the effects of the difference encoding layer sizes on the P\&L when the relevant SAE is used for the predictive FFN, as for the AGL dataset (with an input of 3 features). Performance for both sizes are similar, though increasing the number of assets may result in a different effect being seen.\relax }}{76}{figure.caption.53}}
\newlabel{figure-results_encoding_agl}{{37}{76}{Dataset: AGL (\ref {dataset_agl}) ; Configurations 3 \& 4 (\ref {config3}, \ref {config4}) \newline This figure shows the effects of the difference encoding layer sizes on the P\&L when the relevant SAE is used for the predictive FFN, as for the AGL dataset (with an input of 3 features). Performance for both sizes are similar, though increasing the number of assets may result in a different effect being seen.\relax }{figure.caption.53}{}}
\newlabel{figure-results_encoding_agl@cref}{{[figure][37][]37}{76}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2}Effects of Network Size on SAE and FFN for Synthetic Data}{76}{subsubsection.9.1.2}}
\newlabel{appendix_sae_init}{{9.1.2}{76}{Effects of Network Size on SAE and FFN for Synthetic Data}{subsubsection.9.1.2}{}}
\newlabel{appendix_sae_init@cref}{{[subsubsection][2][9,1]9.1.2}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}); Configuration 5 (\ref  {config5})\relax }}{76}{figure.caption.54}}
\newlabel{figure-results_init_synthetic10_25}{{38}{76}{Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configuration 5 (\ref {config5})\relax }{figure.caption.54}{}}
\newlabel{figure-results_init_synthetic10_25@cref}{{[figure][38][]38}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}); Configuration 5 (\ref  {config5})\relax }}{77}{figure.caption.55}}
\newlabel{figure-results_init_synthetic10_5}{{39}{77}{Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configuration 5 (\ref {config5})\relax }{figure.caption.55}{}}
\newlabel{figure-results_init_synthetic10_5@cref}{{[figure][39][]39}{77}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.3}Effects of Network Size on SAE and FFN for Synthetic Data}{77}{subsubsection.9.1.3}}
\newlabel{appendix_sae_ffn_network_size}{{9.1.3}{77}{Effects of Network Size on SAE and FFN for Synthetic Data}{subsubsection.9.1.3}{}}
\newlabel{appendix_sae_ffn_network_size@cref}{{[subsubsection][3][9,1]9.1.3}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Network Sizes for SAE on the Synthetic10 dataset \newline  The box plots show the MSE for the series of SAE networks trained. The results show worse performance for the typical descending network sizes used for SAE, and show no improvement form increasing network size and complexity.\relax }}{77}{figure.caption.56}}
\newlabel{figure-appendix-it4_sae_layers}{{40}{77}{Network Sizes for SAE on the Synthetic10 dataset \newline The box plots show the MSE for the series of SAE networks trained. The results show worse performance for the typical descending network sizes used for SAE, and show no improvement form increasing network size and complexity.\relax }{figure.caption.56}{}}
\newlabel{figure-appendix-it4_sae_layers@cref}{{[figure][40][]40}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Network Sizes for Predictive FFN on the Synthetic10 dataset \newline  The box plots show the profits for the series of FFN networks trained, showing small improvments for network size increases.\relax }}{78}{figure.caption.57}}
\newlabel{figure-appendix-it4_ffn_network_size}{{41}{78}{Network Sizes for Predictive FFN on the Synthetic10 dataset \newline The box plots show the profits for the series of FFN networks trained, showing small improvments for network size increases.\relax }{figure.caption.57}{}}
\newlabel{figure-appendix-it4_ffn_network_size@cref}{{[figure][41][]41}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Configuration Sets Used}{78}{subsection.9.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Configuration1 - SAE}{78}{subsubsection.9.2.1}}
\newlabel{config1}{{9.2.1}{78}{Configuration1 - SAE}{subsubsection.9.2.1}{}}
\newlabel{config1@cref}{{[subsubsection][1][9,2]9.2.1}{78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Config2 - Predictive FFN}{78}{subsubsection.9.2.2}}
\newlabel{config2}{{9.2.2}{78}{Config2 - Predictive FFN}{subsubsection.9.2.2}{}}
\newlabel{config2@cref}{{[subsubsection][2][9,2]9.2.2}{78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.3}Configuration3 - SAE}{79}{subsubsection.9.2.3}}
\newlabel{config3}{{9.2.3}{79}{Configuration3 - SAE}{subsubsection.9.2.3}{}}
\newlabel{config3@cref}{{[subsubsection][3][9,2]9.2.3}{79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.4}Configuration4 - Predictive FFN}{79}{subsubsection.9.2.4}}
\newlabel{config4}{{9.2.4}{79}{Configuration4 - Predictive FFN}{subsubsection.9.2.4}{}}
\newlabel{config4@cref}{{[subsubsection][4][9,2]9.2.4}{79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.5}Configuration5 - SAE}{79}{subsubsection.9.2.5}}
\newlabel{config5}{{9.2.5}{79}{Configuration5 - SAE}{subsubsection.9.2.5}{}}
\newlabel{config5@cref}{{[subsubsection][5][9,2]9.2.5}{79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.6}Configuration6 - Predictive FFN}{80}{subsubsection.9.2.6}}
\newlabel{config6}{{9.2.6}{80}{Configuration6 - Predictive FFN}{subsubsection.9.2.6}{}}
\newlabel{config6@cref}{{[subsubsection][6][9,2]9.2.6}{80}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.7}Configuration7 - SAE}{80}{subsubsection.9.2.7}}
\newlabel{config7}{{9.2.7}{80}{Configuration7 - SAE}{subsubsection.9.2.7}{}}
\newlabel{config7@cref}{{[subsubsection][7][9,2]9.2.7}{80}}
\bibcite{Albers}{1}
\bibcite{Aparicio}{2}
\bibcite{Arthur}{3}
\bibcite{BailyPBO}{4}
\bibcite{BaileyBTL}{5}
\bibcite{BaileySharpe}{6}
\bibcite{Bakiri}{7}
\bibcite{Bao}{8}
\bibcite{Bartlett}{9}
\bibcite{Bengio1}{10}
\bibcite{Bengio2}{11}
\@writefile{toc}{\contentsline {section}{\numberline {10}References}{81}{section.10}}
\bibcite{Bengio3}{12}
\bibcite{Bottou}{13}
\bibcite{Bottou2}{14}
\bibcite{Ciresan}{15}
\bibcite{Chu}{16}
\bibcite{Chung}{17}
\bibcite{Crutchfield}{18}
\bibcite{Dauphin}{19}
\bibcite{Devarakonda}{20}
\bibcite{Donoho}{21}
\bibcite{Duchi}{22}
\bibcite{Erhan}{23}
\bibcite{Fama}{24}
\bibcite{Fan1}{25}
\bibcite{Fan2}{26}
\bibcite{Ge}{27}
\bibcite{Goodfellow}{28}
\bibcite{Glorot}{29}
\bibcite{Glorot2}{30}
\bibcite{Griffioen}{31}
\bibcite{Hansen}{32}
\bibcite{Harvey}{33}
\bibcite{Hawkins}{34}
\bibcite{He}{35}
\bibcite{Hinton1}{36}
\bibcite{Hinton2}{37}
\bibcite{Hinton3}{38}
\bibcite{Hinton4}{39}
\bibcite{Hinton5}{40}
\bibcite{HLZ}{41}
\bibcite{Hochreiter}{42}
\bibcite{Hornik}{43}
\bibcite{Hsu}{44}
\bibcite{Ivakhnenko}{45}
\bibcite{ImageNet}{46}
\bibcite{Ioannidis}{47}
\bibcite{Johnson}{48}
\bibcite{Kahn}{49}
\bibcite{Knerr}{50}
\bibcite{Langford}{51}
\bibcite{Langkvist}{52}
\bibcite{LeCun}{53}
\bibcite{LeCun2}{54}
\bibcite{LeCun3}{55}
\bibcite{LeCun4}{56}
\bibcite{LeRoux}{57}
\bibcite{Liu}{58}
\bibcite{Lo}{59}
\bibcite{Loshchilov}{60}
\bibcite{Lv}{61}
\bibcite{Mahajan}{62}
\bibcite{McLean}{63}
\bibcite{Minksy}{64}
\bibcite{Murphy}{65}
\bibcite{Packard}{66}
\bibcite{Pascanu}{67}
\bibcite{Peters}{68}
\bibcite{Prado}{69}
\bibcite{Povey}{70}
\bibcite{Ranzato1}{71}
\bibcite{Rumelhart}{72}
\bibcite{Schaefer}{73}
\bibcite{Schmidhuber}{74}
\bibcite{Schorfheide}{75}
\bibcite{Schwager}{76}
\bibcite{Sermanet}{77}
\bibcite{Shalev}{78}
\bibcite{Siegelmann}{79}
\bibcite{Smith}{80}
\bibcite{Skabar}{81}
\bibcite{Takeuchi}{82}
\bibcite{Takens}{83}
\bibcite{Troiano}{84}
\bibcite{Tseng}{85}
\bibcite{Vincent}{86}
\bibcite{Wan}{87}
\bibcite{Wang}{88}
\bibcite{Wang2}{89}
\bibcite{WaveNet}{90}
\bibcite{Wilcox}{91}
\bibcite{Weiss}{92}
\bibcite{Werbos}{93}
\bibcite{Werbos2}{94}
\bibcite{Wu}{95}
\bibcite{Yin}{96}
\bibcite{Zeiler}{97}
\bibcite{Zinkevich}{98}
\bibcite{Zhao}{99}
\bibcite{Zhang}{100}
\bibcite{Zhou}{101}
