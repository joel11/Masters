\BOOKMARK [1][-]{section*.1}{List of Figures}{}% 1
\BOOKMARK [1][-]{section*.1}{List of Tables}{}% 2
\BOOKMARK [1][-]{section.1}{Introduction}{}% 3
\BOOKMARK [1][-]{section.2}{Literature Review}{}% 4
\BOOKMARK [2][-]{subsection.2.1}{Technical Analysis}{section.2}% 5
\BOOKMARK [2][-]{subsection.2.2}{Neural Networks}{section.2}% 6
\BOOKMARK [3][-]{subsubsection.2.2.1}{Training and Backpropagation}{subsection.2.2}% 7
\BOOKMARK [3][-]{subsubsection.2.2.2}{Activation Functions}{subsection.2.2}% 8
\BOOKMARK [3][-]{subsubsection.2.2.3}{Deep Learning}{subsection.2.2}% 9
\BOOKMARK [3][-]{subsubsection.2.2.4}{Weight Initialization Improvements}{subsection.2.2}% 10
\BOOKMARK [2][-]{subsection.2.3}{Stacked Autoencoders}{section.2}% 11
\BOOKMARK [3][-]{subsubsection.2.3.1}{High Dimensional Data Reduction}{subsection.2.3}% 12
\BOOKMARK [3][-]{subsubsection.2.3.2}{Deep Belief Networks}{subsection.2.3}% 13
\BOOKMARK [3][-]{subsubsection.2.3.3}{Stacked Denoising Autoencoders}{subsection.2.3}% 14
\BOOKMARK [3][-]{subsubsection.2.3.4}{Pre-training}{subsection.2.3}% 15
\BOOKMARK [3][-]{subsubsection.2.3.5}{Time Series Applications}{subsection.2.3}% 16
\BOOKMARK [3][-]{subsubsection.2.3.6}{Financial Applications}{subsection.2.3}% 17
\BOOKMARK [2][-]{subsection.2.4}{Online Learning Algorithms and Gradient Descent}{section.2}% 18
\BOOKMARK [2][-]{subsection.2.5}{Gradient Learning Improvements}{section.2}% 19
\BOOKMARK [3][-]{subsubsection.2.5.1}{Gradient Adjustments and Regularization}{subsection.2.5}% 20
\BOOKMARK [3][-]{subsubsection.2.5.2}{Dropout}{subsection.2.5}% 21
\BOOKMARK [3][-]{subsubsection.2.5.3}{Learning Rate Schedules}{subsection.2.5}% 22
\BOOKMARK [2][-]{subsection.2.6}{Backtesting and Model Validation}{section.2}% 23
\BOOKMARK [3][-]{subsubsection.2.6.1}{Testing Methodologies}{subsection.2.6}% 24
\BOOKMARK [3][-]{subsubsection.2.6.2}{Test Data Length}{subsection.2.6}% 25
\BOOKMARK [3][-]{subsubsection.2.6.3}{Sharpe Ratio}{subsection.2.6}% 26
\BOOKMARK [1][-]{section.3}{Software Libraries and Development}{}% 27
\BOOKMARK [2][-]{subsection.3.1}{Programming Languages}{section.3}% 28
\BOOKMARK [2][-]{subsection.3.2}{Data Generation \046 Processing}{section.3}% 29
\BOOKMARK [2][-]{subsection.3.3}{Network Training}{section.3}% 30
\BOOKMARK [3][-]{subsubsection.3.3.1}{Network Parameters}{subsection.3.3}% 31
\BOOKMARK [3][-]{subsubsection.3.3.2}{SGD \046 CD-1 Training Parameters}{subsection.3.3}% 32
\BOOKMARK [3][-]{subsubsection.3.3.3}{OGD Training Parameters}{subsection.3.3}% 33
\BOOKMARK [3][-]{subsubsection.3.3.4}{Network Trainer Module}{subsection.3.3}% 34
\BOOKMARK [2][-]{subsection.3.4}{Process Implementation}{section.3}% 35
\BOOKMARK [2][-]{subsection.3.5}{Database Implementation}{section.3}% 36
\BOOKMARK [1][-]{section.4}{Data Processing and Generation }{}% 37
\BOOKMARK [2][-]{subsection.4.1}{Data Processing}{section.4}% 38
\BOOKMARK [3][-]{subsubsection.4.1.1}{Log Difference Transformation and Aggregation}{subsection.4.1}% 39
\BOOKMARK [3][-]{subsubsection.4.1.2}{Data Scaling}{subsection.4.1}% 40
\BOOKMARK [3][-]{subsubsection.4.1.3}{Reverse Data Scaling}{subsection.4.1}% 41
\BOOKMARK [3][-]{subsubsection.4.1.4}{Price Reconstruction}{subsection.4.1}% 42
\BOOKMARK [2][-]{subsection.4.2}{Synthetic Data Generation}{section.4}% 43
\BOOKMARK [3][-]{subsubsection.4.2.1}{GBM Data Distributions}{subsection.4.2}% 44
\BOOKMARK [2][-]{subsection.4.3}{Price Considerations}{section.4}% 45
\BOOKMARK [1][-]{section.5}{Models and Algorithms}{}% 46
\BOOKMARK [2][-]{subsection.5.1}{Process Overview}{section.5}% 47
\BOOKMARK [2][-]{subsection.5.2}{Feedforward Neural Networks}{section.5}% 48
\BOOKMARK [3][-]{subsubsection.5.2.1}{Notation and Network Representation}{subsection.5.2}% 49
\BOOKMARK [3][-]{subsubsection.5.2.2}{Activation Functions}{subsection.5.2}% 50
\BOOKMARK [3][-]{subsubsection.5.2.3}{Backpropagation}{subsection.5.2}% 51
\BOOKMARK [3][-]{subsubsection.5.2.4}{Gradient Descent Algorithms}{subsection.5.2}% 52
\BOOKMARK [3][-]{subsubsection.5.2.5}{Regularization}{subsection.5.2}% 53
\BOOKMARK [3][-]{subsubsection.5.2.6}{Learning Rate Schedule}{subsection.5.2}% 54
\BOOKMARK [3][-]{subsubsection.5.2.7}{Dropout}{subsection.5.2}% 55
\BOOKMARK [2][-]{subsection.5.3}{Restricted Boltzmann Machines}{section.5}% 56
\BOOKMARK [3][-]{subsubsection.5.3.1}{Contrastive Divergence}{subsection.5.3}% 57
\BOOKMARK [3][-]{subsubsection.5.3.2}{CD-1 and SGD}{subsection.5.3}% 58
\BOOKMARK [2][-]{subsection.5.4}{Stacked Autoencoders}{section.5}% 59
\BOOKMARK [3][-]{subsubsection.5.4.1}{Sigmoid based Greedy Layerwise SAE Training}{subsection.5.4}% 60
\BOOKMARK [3][-]{subsubsection.5.4.2}{ReLU based SAE Training}{subsection.5.4}% 61
\BOOKMARK [3][-]{subsubsection.5.4.3}{Denoising Autoencoders}{subsection.5.4}% 62
\BOOKMARK [2][-]{subsection.5.5}{Variance Based Weight Initializations}{section.5}% 63
\BOOKMARK [3][-]{subsubsection.5.5.1}{Initialization Rationale}{subsection.5.5}% 64
\BOOKMARK [3][-]{subsubsection.5.5.2}{Initializations}{subsection.5.5}% 65
\BOOKMARK [2][-]{subsection.5.6}{CSCV \046 PBO}{section.5}% 66
\BOOKMARK [2][-]{subsection.5.7}{Money Management Strategy and Returns}{section.5}% 67
\BOOKMARK [1][-]{section.6}{Full Process}{}% 68
\BOOKMARK [2][-]{subsection.6.1}{Parameter Space Exploration}{section.6}% 69
\BOOKMARK [2][-]{subsection.6.2}{Synthetic Data}{section.6}% 70
\BOOKMARK [2][-]{subsection.6.3}{Data Preparation}{section.6}% 71
\BOOKMARK [3][-]{subsubsection.6.3.1}{Data Window Aggregations}{subsection.6.3}% 72
\BOOKMARK [3][-]{subsubsection.6.3.2}{Point Predictions}{subsection.6.3}% 73
\BOOKMARK [3][-]{subsubsection.6.3.3}{Scaling}{subsection.6.3}% 74
\BOOKMARK [2][-]{subsection.6.4}{Data Segregation}{section.6}% 75
\BOOKMARK [2][-]{subsection.6.5}{Unsupervised Learning: SAE Training}{section.6}% 76
\BOOKMARK [2][-]{subsection.6.6}{Supervised Learning: Prediction Network Training}{section.6}% 77
\BOOKMARK [2][-]{subsection.6.7}{Price Reconstruction}{section.6}% 78
\BOOKMARK [2][-]{subsection.6.8}{Money Management Strategy}{section.6}% 79
\BOOKMARK [2][-]{subsection.6.9}{CSCV \046 PBO}{section.6}% 80
\BOOKMARK [2][-]{subsection.6.10}{Process Diagram}{section.6}% 81
\BOOKMARK [1][-]{section.7}{Datasets Used}{}% 82
\BOOKMARK [2][-]{subsection.7.1}{Synthetic Datasets}{section.7}% 83
\BOOKMARK [3][-]{subsubsection.7.1.1}{Synthetic6}{subsection.7.1}% 84
\BOOKMARK [3][-]{subsubsection.7.1.2}{Synthetic10}{subsection.7.1}% 85
\BOOKMARK [2][-]{subsection.7.2}{Actual Datasets}{section.7}% 86
\BOOKMARK [3][-]{subsubsection.7.2.1}{Actual10}{subsection.7.2}% 87
\BOOKMARK [3][-]{subsubsection.7.2.2}{AGL}{subsection.7.2}% 88
\BOOKMARK [3][-]{subsubsection.7.2.3}{AGL\046ACL}{subsection.7.2}% 89
\BOOKMARK [3][-]{subsubsection.7.2.4}{Scaling10}{subsection.7.2}% 90
\BOOKMARK [1][-]{section.8}{Results}{}% 91
\BOOKMARK [2][-]{subsection.8.1}{Introduction}{section.8}% 92
\BOOKMARK [3][-]{subsubsection.8.1.1}{Overview}{subsection.8.1}% 93
\BOOKMARK [2][-]{subsection.8.2}{Linearity, Complexity and Structure of Data}{section.8}% 94
\BOOKMARK [3][-]{subsubsection.8.2.1}{GBM Generated Data}{subsection.8.2}% 95
\BOOKMARK [3][-]{subsubsection.8.2.2}{Effects of Activation Functions and Scaling}{subsection.8.2}% 96
\BOOKMARK [3][-]{subsubsection.8.2.3}{Predictive FFN Activations and Scaling}{subsection.8.2}% 97
\BOOKMARK [3][-]{subsubsection.8.2.4}{Leaky ReLU vs ReLU}{subsection.8.2}% 98
\BOOKMARK [2][-]{subsection.8.3}{Weight Initialization Techniques}{section.8}% 99
\BOOKMARK [3][-]{subsubsection.8.3.1}{RBM Pretraining for Sigmoid Networks}{subsection.8.3}% 100
\BOOKMARK [4][-]{paragraph.8.3.1.1}{Sigmoid Activation Functions}{subsubsection.8.3.1}% 101
\BOOKMARK [3][-]{subsubsection.8.3.2}{Variance Based Weight Initialization Techniques}{subsection.8.3}% 102
\BOOKMARK [2][-]{subsection.8.4}{Feature Selection}{section.8}% 103
\BOOKMARK [2][-]{subsection.8.5}{Network Structure and Training}{section.8}% 104
\BOOKMARK [3][-]{subsubsection.8.5.1}{Effects of Network Size}{subsection.8.5}% 105
\BOOKMARK [4][-]{paragraph.8.5.1.1}{SAE Network Structures}{subsubsection.8.5.1}% 106
\BOOKMARK [3][-]{subsubsection.8.5.2}{Effects of Learning Rates and Schedules}{subsection.8.5}% 107
\BOOKMARK [3][-]{subsubsection.8.5.3}{Effects of Regularization}{subsection.8.5}% 108
\BOOKMARK [3][-]{subsubsection.8.5.4}{Effects of Denoising}{subsection.8.5}% 109
\BOOKMARK [2][-]{subsection.8.6}{The Effects of Data Aggregation and Value of Historical Data}{section.8}% 110
\BOOKMARK [3][-]{subsubsection.8.6.1}{Data Aggregation and SAE MSE Scores}{subsection.8.6}% 111
\BOOKMARK [3][-]{subsubsection.8.6.2}{Data Aggregation and Predictive P\046L Scores}{subsection.8.6}% 112
\BOOKMARK [3][-]{subsubsection.8.6.3}{Effects of IS Training and Historical Data}{subsection.8.6}% 113
\BOOKMARK [2][-]{subsection.8.7}{Probability of Backtest Overfitting}{section.8}% 114
\BOOKMARK [3][-]{subsubsection.8.7.1}{Concerns Regarding the PBO Calculation}{subsection.8.7}% 115
\BOOKMARK [3][-]{subsubsection.8.7.2}{PBO Results}{subsection.8.7}% 116
\BOOKMARK [3][-]{subsubsection.8.7.3}{Framework Success}{subsection.8.7}% 117
\BOOKMARK [2][-]{subsection.8.8}{DSR Results}{section.8}% 118
\BOOKMARK [2][-]{subsection.8.9}{MMS Results}{section.8}% 119
\BOOKMARK [3][-]{subsubsection.8.9.1}{Summary of Experiment Results}{subsection.8.9}% 120
\BOOKMARK [3][-]{subsubsection.8.9.2}{Best Network Results}{subsection.8.9}% 121
\BOOKMARK [2][-]{subsection.8.10}{Results Summary}{section.8}% 122
\BOOKMARK [3][-]{subsubsection.8.10.1}{Framework Success and PBO}{subsection.8.10}% 123
\BOOKMARK [3][-]{subsubsection.8.10.2}{Historical Data}{subsection.8.10}% 124
\BOOKMARK [3][-]{subsubsection.8.10.3}{Data Processing Considerations}{subsection.8.10}% 125
\BOOKMARK [3][-]{subsubsection.8.10.4}{Weight Initializations}{subsection.8.10}% 126
\BOOKMARK [3][-]{subsubsection.8.10.5}{Feature Selection}{subsection.8.10}% 127
\BOOKMARK [3][-]{subsubsection.8.10.6}{Output Activations and Network Structure}{subsection.8.10}% 128
\BOOKMARK [3][-]{subsubsection.8.10.7}{Learning Optimizations}{subsection.8.10}% 129
\BOOKMARK [1][-]{section.9}{Conclusion}{}% 130
\BOOKMARK [2][-]{subsection.9.1}{Future Work}{section.9}% 131
\BOOKMARK [1][-]{section.10}{Appendix}{}% 132
\BOOKMARK [2][-]{subsection.10.1}{Additional Results}{section.10}% 133
\BOOKMARK [3][-]{subsubsection.10.1.1}{Additional Results for Section 8.2.2 - Activation Functions and Scaling }{subsection.10.1}% 134
\BOOKMARK [3][-]{subsubsection.10.1.2}{Additional Results for Section 8.3 - Weight Initialization Techniques }{subsection.10.1}% 135
\BOOKMARK [3][-]{subsubsection.10.1.3}{Additional Results for Section 8.4 - Feature Selection }{subsection.10.1}% 136
\BOOKMARK [3][-]{subsubsection.10.1.4}{Additional Results for Section 8.5 - Network Structure and Training }{subsection.10.1}% 137
\BOOKMARK [2][-]{subsection.10.2}{Configuration Sets Used}{section.10}% 138
\BOOKMARK [3][-]{subsubsection.10.2.1}{Configuration 1}{subsection.10.2}% 139
\BOOKMARK [3][-]{subsubsection.10.2.2}{Configuration 2}{subsection.10.2}% 140
\BOOKMARK [3][-]{subsubsection.10.2.3}{Configuration 3}{subsection.10.2}% 141
\BOOKMARK [3][-]{subsubsection.10.2.4}{Configuration 4}{subsection.10.2}% 142
\BOOKMARK [3][-]{subsubsection.10.2.5}{Configuration 5}{subsection.10.2}% 143
\BOOKMARK [3][-]{subsubsection.10.2.6}{Configuration 6}{subsection.10.2}% 144
\BOOKMARK [3][-]{subsubsection.10.2.7}{Configuration 7}{subsection.10.2}% 145
\BOOKMARK [3][-]{subsubsection.10.2.8}{Configuration 8}{subsection.10.2}% 146
\BOOKMARK [3][-]{subsubsection.10.2.9}{Configuration 9}{subsection.10.2}% 147
\BOOKMARK [3][-]{subsubsection.10.2.10}{Configuration 10}{subsection.10.2}% 148
\BOOKMARK [3][-]{subsubsection.10.2.11}{Configuration 11}{subsection.10.2}% 149
\BOOKMARK [3][-]{subsubsection.10.2.12}{Configuration 12}{subsection.10.2}% 150
\BOOKMARK [3][-]{subsubsection.10.2.13}{Configuration 13}{subsection.10.2}% 151
\BOOKMARK [3][-]{subsubsection.10.2.14}{Configuration 14}{subsection.10.2}% 152
\BOOKMARK [3][-]{subsubsection.10.2.15}{Configuration 15}{subsection.10.2}% 153
\BOOKMARK [3][-]{subsubsection.10.2.16}{Configuration 16}{subsection.10.2}% 154
\BOOKMARK [3][-]{subsubsection.10.2.17}{Configuration 17}{subsection.10.2}% 155
\BOOKMARK [1][-]{section.11}{References}{}% 156
