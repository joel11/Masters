\BOOKMARK [1][-]{Doc-Start}{List of Figures}{}% 1
\BOOKMARK [1][-]{Doc-Start}{List of Tables}{}% 2
\BOOKMARK [1][-]{section.1}{Introduction}{}% 3
\BOOKMARK [1][-]{section.2}{Literature Review}{}% 4
\BOOKMARK [2][-]{subsection.2.1}{Technical Analysis}{section.2}% 5
\BOOKMARK [2][-]{subsection.2.2}{Neural Networks}{section.2}% 6
\BOOKMARK [3][-]{subsubsection.2.2.1}{Training and Backpropagation}{subsection.2.2}% 7
\BOOKMARK [3][-]{subsubsection.2.2.2}{Activation Functions}{subsection.2.2}% 8
\BOOKMARK [3][-]{subsubsection.2.2.3}{Deep Learning}{subsection.2.2}% 9
\BOOKMARK [3][-]{subsubsection.2.2.4}{Weight Initialization Improvements}{subsection.2.2}% 10
\BOOKMARK [2][-]{subsection.2.3}{Stacked Autoencoders}{section.2}% 11
\BOOKMARK [3][-]{subsubsection.2.3.1}{High Dimensional Data Reduction}{subsection.2.3}% 12
\BOOKMARK [3][-]{subsubsection.2.3.2}{Deep Belief Networks}{subsection.2.3}% 13
\BOOKMARK [3][-]{subsubsection.2.3.3}{Stacked Denoising Autoencoders}{subsection.2.3}% 14
\BOOKMARK [3][-]{subsubsection.2.3.4}{Pre-training}{subsection.2.3}% 15
\BOOKMARK [3][-]{subsubsection.2.3.5}{Time Series Applications}{subsection.2.3}% 16
\BOOKMARK [3][-]{subsubsection.2.3.6}{Financial Applications}{subsection.2.3}% 17
\BOOKMARK [2][-]{subsection.2.4}{Online Learning Algorithms and Gradient Descent}{section.2}% 18
\BOOKMARK [2][-]{subsection.2.5}{Gradient Learning Improvements}{section.2}% 19
\BOOKMARK [3][-]{subsubsection.2.5.1}{Gradient Adjustments and Regularization}{subsection.2.5}% 20
\BOOKMARK [3][-]{subsubsection.2.5.2}{Dropout}{subsection.2.5}% 21
\BOOKMARK [3][-]{subsubsection.2.5.3}{Learning Rate Schedules}{subsection.2.5}% 22
\BOOKMARK [2][-]{subsection.2.6}{Backtesting and Model Validation}{section.2}% 23
\BOOKMARK [3][-]{subsubsection.2.6.1}{Sharpe Ratio Assessment Methodologies}{subsection.2.6}% 24
\BOOKMARK [3][-]{subsubsection.2.6.2}{Generalised Assessment Methodologies}{subsection.2.6}% 25
\BOOKMARK [3][-]{subsubsection.2.6.3}{Test Data Length}{subsection.2.6}% 26
\BOOKMARK [1][-]{section.3}{Data Processing and Generation }{}% 27
\BOOKMARK [2][-]{subsection.3.1}{Data Processing}{section.3}% 28
\BOOKMARK [3][-]{subsubsection.3.1.1}{Log Difference Transformation and Aggregation}{subsection.3.1}% 29
\BOOKMARK [3][-]{subsubsection.3.1.2}{Data Scaling}{subsection.3.1}% 30
\BOOKMARK [4][-]{paragraph.3.1.2.1}{Standardization}{subsubsection.3.1.2}% 31
\BOOKMARK [4][-]{paragraph.3.1.2.2}{Normalization}{subsubsection.3.1.2}% 32
\BOOKMARK [4][-]{paragraph.3.1.2.3}{Limited Standardization}{subsubsection.3.1.2}% 33
\BOOKMARK [4][-]{paragraph.3.1.2.4}{Limited Normalization}{subsubsection.3.1.2}% 34
\BOOKMARK [3][-]{subsubsection.3.1.3}{Reverse Data Scaling}{subsection.3.1}% 35
\BOOKMARK [4][-]{paragraph.3.1.3.1}{Reverse Standardization}{subsubsection.3.1.3}% 36
\BOOKMARK [4][-]{paragraph.3.1.3.2}{Reverse Normalization}{subsubsection.3.1.3}% 37
\BOOKMARK [3][-]{subsubsection.3.1.4}{Price Reconstruction}{subsection.3.1}% 38
\BOOKMARK [2][-]{subsection.3.2}{Synthetic Data Generation}{section.3}% 39
\BOOKMARK [3][-]{subsubsection.3.2.1}{GBM Data Distributions}{subsection.3.2}% 40
\BOOKMARK [2][-]{subsection.3.3}{Price Considerations}{section.3}% 41
\BOOKMARK [1][-]{section.4}{Models and Algorithms}{}% 42
\BOOKMARK [2][-]{subsection.4.1}{Process Overview}{section.4}% 43
\BOOKMARK [2][-]{subsection.4.2}{Feedforward Neural Networks}{section.4}% 44
\BOOKMARK [3][-]{subsubsection.4.2.1}{Notation and Network Representation}{subsection.4.2}% 45
\BOOKMARK [3][-]{subsubsection.4.2.2}{Activation Functions}{subsection.4.2}% 46
\BOOKMARK [4][-]{paragraph.4.2.2.1}{Sigmoid}{subsubsection.4.2.2}% 47
\BOOKMARK [4][-]{paragraph.4.2.2.2}{ReLU}{subsubsection.4.2.2}% 48
\BOOKMARK [4][-]{paragraph.4.2.2.3}{Leaky ReLU}{subsubsection.4.2.2}% 49
\BOOKMARK [4][-]{paragraph.4.2.2.4}{Linear Activation}{subsubsection.4.2.2}% 50
\BOOKMARK [3][-]{subsubsection.4.2.3}{Backpropagation}{subsection.4.2}% 51
\BOOKMARK [3][-]{subsubsection.4.2.4}{Gradient Descent Algorithms}{subsection.4.2}% 52
\BOOKMARK [3][-]{subsubsection.4.2.5}{Regularization}{subsection.4.2}% 53
\BOOKMARK [3][-]{subsubsection.4.2.6}{Learning Rate Schedule}{subsection.4.2}% 54
\BOOKMARK [3][-]{subsubsection.4.2.7}{Dropout}{subsection.4.2}% 55
\BOOKMARK [2][-]{subsection.4.3}{Restricted Boltzmann Machines}{section.4}% 56
\BOOKMARK [3][-]{subsubsection.4.3.1}{Contrastive Divergence}{subsection.4.3}% 57
\BOOKMARK [3][-]{subsubsection.4.3.2}{CD-1 and SGD}{subsection.4.3}% 58
\BOOKMARK [2][-]{subsection.4.4}{Stacked Autoencoders}{section.4}% 59
\BOOKMARK [3][-]{subsubsection.4.4.1}{Sigmoid based Greedy Layerwise SAE Training}{subsection.4.4}% 60
\BOOKMARK [3][-]{subsubsection.4.4.2}{ReLU based SAE Training}{subsection.4.4}% 61
\BOOKMARK [3][-]{subsubsection.4.4.3}{Denoising Autoencoders}{subsection.4.4}% 62
\BOOKMARK [4][-]{paragraph.4.4.3.1}{Additive Gaussian Noise}{subsubsection.4.4.3}% 63
\BOOKMARK [4][-]{paragraph.4.4.3.2}{Masking Noise}{subsubsection.4.4.3}% 64
\BOOKMARK [2][-]{subsection.4.5}{Variance Based Weight Initializations}{section.4}% 65
\BOOKMARK [3][-]{subsubsection.4.5.1}{Initialization Rationale}{subsection.4.5}% 66
\BOOKMARK [3][-]{subsubsection.4.5.2}{Initializations}{subsection.4.5}% 67
\BOOKMARK [4][-]{paragraph.4.5.2.1}{Xavier}{subsubsection.4.5.2}% 68
\BOOKMARK [4][-]{paragraph.4.5.2.2}{He}{subsubsection.4.5.2}% 69
\BOOKMARK [4][-]{paragraph.4.5.2.3}{He-Adjusted}{subsubsection.4.5.2}% 70
\BOOKMARK [2][-]{subsection.4.6}{Money Management Strategy and Returns}{section.4}% 71
\BOOKMARK [3][-]{subsubsection.4.6.1}{Input Variables}{subsection.4.6}% 72
\BOOKMARK [3][-]{subsubsection.4.6.2}{Calculated Asset Variables}{subsection.4.6}% 73
\protect \let \reserved@d =[\def \def \let  
\protect \let \reserved@d =[\def \def \unskip \reserved@e {\reserved@f \relax }\penalty \@M \hfil \penalty -\@M {[1][-]}\futurelet \@let@token \let \unskip \reserved@e {\reserved@f \relax }\penalty \@M \hfil \penalty -\@M \unskip \reserved@e {\reserved@f \relax }\penalty \@M \hfil \penalty -\@M \unskip \reserved@e {\reserved@f \relax }\penalty \@M \hfil \penalty -\@M [2][-]{subsection.4.8}{DSR and ONC}{section.4}% 75
\protect \let \reserved@d =[\def \def \unskip \reserved@e {\reserved@f \relax }\penalty \@M \hfil \penalty -\@M {[1][-]}\futurelet \@let@token \let \unskip \reserved@e {\reserved@f \relax }\penalty \@M \hfil \penalty -\@M \unskip \reserved@e {\reserved@f \relax }\penalty \@M \hfil \penalty -\@M \unskip \reserved@e {\reserved@f \relax }\penalty \@M \hfil \penalty -\@M [3][-]{subsubsection.4.8.1}{Optimal Number of Clusters Algorithm}{subsection.4.8}% 76
