\select@language {english}
\contentsline {figure}{\numberline {1}{\ignorespaces The Autoencoder training steps \cite {Hinton2}\relax }}{21}{figure.caption.6}
\contentsline {figure}{\numberline {2}{\ignorespaces An example diagram of a Feedforwrd Neural Network with 1 hidden layer\relax }}{33}{figure.caption.8}
\contentsline {figure}{\numberline {3}{\ignorespaces Learning rates calculated over 1000 epochs with $\eta _{\qopname \relax m{min}} = 0.1$ to $\eta _{\qopname \relax m{max}} = 1.0$ and $i=100$\relax }}{38}{figure.caption.9}
\contentsline {figure}{\numberline {4}{\ignorespaces An example diagram of a Restricted Boltzmann Machine network\relax }}{39}{figure.caption.10}
\contentsline {figure}{\numberline {5}{\ignorespaces Overall Process Flow\relax }}{52}{figure.caption.14}
\contentsline {figure}{\numberline {6}{\ignorespaces Dataset: Scaling10 dataset (\ref {dataset_scaling10}), Configuration \newline Figure (a) here shows SAE MSE performance according to the scaling techniques as described in \ref {data_scaling}. As discussed more fully in \ref {proc_dataprep}, the use of standardization for scaling the data doesn't allow for effective outlier treatments, resulting in the significantly worse performance seen here. \newline Figure (b) shows the effects of the output activation for configurations using Normalized scaling. The ReLU activations also seemed to result in very poor performance, most likely due to the loss of error signal in the output layer where it is most critical. \relax }}{57}{figure.caption.20}
\contentsline {figure}{\numberline {7}{\ignorespaces Dataset: Scaling10 dataset (\ref {dataset_scaling10}), Configuration \newline Figure (a) shows the best (minimum) MSE scores for different encoding activations at the different encoding layer sizes (with a network input of 30). There is a consisent out performance of Linear activations in the encoding layer, which usual to expected from literature \cite {Hinton2}, and in line with reducing loss of error signal at critical points. \newline Figure (b) shows the best (minimum) MSE scores for different hidden layer activations at the different encoding layer sizes (with a network input of 30). At larger encoding layers, there is a competitive performance from all linear networks, where they are less pressured to take advantage of non-linear effects. As the size of the encoding layer is reduced, the advantages of non-linear activations become more apparent, with outperformance by ReLU and Simgoid activations. Sigmoid activations are known to suffer from learning slow down as a result of saturation and so have increased sensitivity to vanishing gradients \cite {Glorot2}. SAE networks are deep by nature, and so it is not surprising that the Sigmoid activations are resulting in worse performance over the same training period when compared to ReLU activations. \newline \relax }}{58}{figure.caption.21}
\contentsline {figure}{\numberline {8}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6})), Configuration \newline Figure (a) shows the impact of the limited scaling technique (as described in \ref {data_scaling}) in comparison to the non-limited version. The implementation of this is not a choice when interested in simulating a real world implementation, but it is still of interest to note the impact of this issue. Additionally, the use of a Linear output layer has been shown to assist in reducing the impact of this effect. \newline Figure (b) offers a comparison of Linear and ReLU hidden layer activations on P\&L, with the linear activations resulting in notably better P\&L when compared to ReLU activations. Unlike the SAE networks, this persists even when the network size is decreased - this difference highlights the effects of GBM data and that it can be represented linearly, as per \ref {results_gbm_data}, whereas we would see actual stock data to benefit from a non-linear representation. Linear configurations for actual data were not run, as real stock data is not expected to follow patterns that are linear through time, thus offering little value.\relax }}{59}{figure.caption.22}
\contentsline {figure}{\numberline {9}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration \newline The plot shows the FFN P\&L, grouped by activation, showing some improvements from the Leaky ReLU activation. The effect is most noticeable in reducing the lower bounds of performance (Â±10.8\%), though has a clear benefit in the upper bounds as well (3.4\%).\relax }}{60}{figure.caption.23}
\contentsline {figure}{\numberline {10}{\ignorespaces Dataset AGL\&ACL (\ref {dataset_aglacl}); Configuration7 (\ref {config7}) \newline \newline The boxplots here show the summary of configuration performances, by minimum MSE achieved, grouped according to the number of pre-training epochs which the network had. There is a clear favour to having no pre-training in this scenario as there is a clear decrease in performance as the number of epochs increase (the low value outliers that can be seen for the 1 epoch configuration were with learning rates which were low enough to approximate no epochs).\relax }}{61}{figure.caption.24}
\contentsline {figure}{\numberline {11}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configuration 5 (\ref {config5}) \newline The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the similar performance of He-Adj and Xavier on synthetic datasets.\relax }}{63}{figure.caption.25}
\contentsline {figure}{\numberline {12}{\ignorespaces Dataset Actual10 \newline The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the outperformance of He-Adj to Xavier on actual datasets.\relax }}{63}{figure.caption.26}
\contentsline {figure}{\numberline {13}{\ignorespaces Dataset: AGL (\ref {dataset_agl}); Configuration 3 (\ref {config3}) \newline The configurations run for AGL show notably better performance using the He-Adj based initialization in comparison to both He and Xavier. The emphasis seen here can be attributed to increased pressure on suitable weight initizliations as 1 and 2 node encoding layers were used.\relax }}{64}{figure.caption.27}
\contentsline {figure}{\numberline {14}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configurations 5 and 6 (\ref {config5}, \ref {config6}) \newline The figure here shows the P\&L performance for He-Adj and He being higher than Xavier, as to be expected with more balanced network layers and ReLU activations. The better performance in the lower end of the He-Adj and configurations when compared to He can be attributed to the inclusion of inconsistently sized layers in the configuration set, where He-Adj and would have performed better.\relax }}{64}{figure.caption.28}
\contentsline {figure}{\numberline {15}{\ignorespaces Dataset: AGL (\ref {dataset_agl}); Configuration 4 (\ref {config4}) \newline The configurations run for AGL show notably better performance using the He-Adj and and He based initialization in comparison Xavier when using actual data, as per the differences explained above.\relax }}{65}{figure.caption.29}
\contentsline {figure}{\numberline {16}{\ignorespaces Dataset: ; Configuration \newline Actual P\&L grouped according to feature selection size (where 0 is no encoding), showing increased performance the closer feature selection is to the number of assets (in this case, 10).\relax }}{66}{figure.caption.30}
\contentsline {figure}{\numberline {17}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations 5 \& 6 (\ref {config5}, \ref {config6}) \newline Synthetic P\&L grouped according to feature selection size.\relax }}{67}{figure.caption.31}
\contentsline {figure}{\numberline {18}{\ignorespaces Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration \newline Figure (a) shows the median minimum MSE by SAE networks achieved with the indicated number of layers and layer nodes. It's worth noting that the number of layers here indicate the final SAE structure of $N$ hidden layers, rather than the training structure of $2N + 1$ hidden layers. Interestingly, the networks with 120 node layers seem to struggled with training, performance decreases as layers increased. This is to be expected if the learning parameters (e.g. SGD learning rate) are not ideal for the structure, the effects of which are amplified as network sizes increase. This is resolved in the larger 240 node networks which show significant improvement as more layers are added (as one would generally expect, subject to effective SGD). \newline Figure (b) shows the median OOS P\&L achieved by predictive networks with the indicated number of layers and layer nodes. The behaviour is as expected here, where network performance generally increases with size. The exception is the four layer network of 60 nodes, where the training parameters were no longer effective in passing signal back (larger layer sizes are less likely to suffer from this, particuarly with ReLU activations). \relax }}{69}{figure.caption.32}
\contentsline {figure}{\numberline {19}{\ignorespaces Dataset: Synthetic10 dataset (\ref {dataset_synthetic10}), Actual10 dataset (\ref {dataset_actual10}), Configuration \newline It is interesting to note that the effects here are quite different for Synthetic and Actual data, with networks for Synthetic data favouring learning rate ranges with a higher upper bound, and Actual data networks trending in favour of lower upper bound ranges. The synthetic data is structurally less complex, as discussed further in \ref {results_gbm_data} and \ref {results_data_hist}, and so the larger learning rates will allow for quicker optimisation. Conversely, SAE networks for actual data benefit from a more careful exploration of the solution space.\relax }}{70}{figure.caption.33}
\contentsline {figure}{\numberline {20}{\ignorespaces Dataset: Synthetic10 dataset (\ref {dataset_synthetic10}), Actual10 dataset (\ref {dataset_actual10}), Configuration \newline The learning rates for Actual data showin in Figure (b) here were chosen accordingly to the best rates seen in the SAE training. There's no substantial difference shown by changing the lower bound by a factor. The results for Synthetic data were interesting in that the networks had higher P\&L when trainined with ssmaller learning rates, suggesting that the predictive task still warrants a more careful solution space exploration despite the less complex nature of the data.\relax }}{71}{figure.caption.34}
\contentsline {figure}{\numberline {21}{\ignorespaces Dataset: Actual10 dataset (\ref {dataset_actual10}), Configurations \newline Figure(a) shows the MSE scores for SAE networks, with the shorter epoch cycle offering the best (and the worst) results. Synthetic data for the SAE MSE scores behaved similarly, as seen in the appendix (section \ref {results_network_appendix}). \newline In Figure (b), for the predictive networks, P\&L was highest in the 100 epoch cycle as well, where -1 indicates a constant learning rate rather than the cyclical pattern, and 10 epoch learning rates were used for configurations which only ran for 10 epochs. The configurations without learning epochs (-1) were in the second round of testing and had a more effective set of parameters elsewhere, hence the large difference in the lower bounds relative to the other configurations. Ultimately, the implementation of the learning rate cycle has offered a small but real improvement relative to the constant learning rate for P\&L.\relax }}{71}{figure.caption.35}
\contentsline {figure}{\numberline {22}{\ignorespaces Dataset: Actual10 dataset (\ref {dataset_actual10}), Configurations \newline The P\&L performance for OGD learning rates is expectedly non-linear, with performance steadily increasing up until 0.05 where the network is adapting at the correct rate, and degrading thereafter as the network will overcorrect to noise and short term trends picked up in the online learning period. Networks trained on synthetic data showed similar trends (though were not trained past a turning point), and can be seen in the Appendix in Section \ref {results_network_appendix}.\relax }}{72}{figure.caption.36}
\contentsline {figure}{\numberline {23}{\ignorespaces Dataset: Actual10 dataset (\ref {dataset_actual10}), Configurations \newline \relax }}{72}{figure.caption.37}
\contentsline {figure}{\numberline {24}{\ignorespaces Dataset: , Configurations \newline \relax }}{73}{figure.caption.38}
\contentsline {figure}{\numberline {25}{\ignorespaces Dataset: Actual10 dataset (\ref {dataset_actual10}), Configurations \newline \relax }}{73}{figure.caption.39}
\contentsline {figure}{\numberline {26}{\ignorespaces Dataset: Synthetic10 dataset (\ref {dataset_synthetic10}), Configuration \newline Figure (a) shows the distribution of values to be replicated by the SAE for synthetic data. Geometric Brownian Motion generates discrete changes that follow a log-normal distribution, the log difference and scaling of which (as per the data processing described in \ref {data_processing}) result in normally distributed price changes. We see small differences in the distributions according to the data aggregation used, as one would expect, with slightly lower variances occuring for smaller data windows ($\sigma _{[1,5,20]} = 0.145$, $\sigma _{[5,20,60]} = 0.152$, $\sigma _{[10,20,60]} = 0.154$). There were some differences in SAE performance as indicated in Figure (b), though not to large extents and considering the very similar distribution of values, there isn't any expectation of seeing fundamental differences in the networks ability to compress and replicate them. \relax }}{74}{figure.caption.40}
\contentsline {figure}{\numberline {27}{\ignorespaces Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration \newline Figure (a) shows the distribution of values to be replicated by the SAE for actual data. The distributions are noticeably different from synthetic data, as there is no prior on the price change values, and so variances differ far more across the configurations ($\sigma _{[1,5,20]} = 0.118$, $\sigma _{[5,20,60]} = 0.146$, $\sigma _{[10,20,60]} = 0.150$). Smaller data windows are less likely to capture larger fundamental price changes, resulting in the lower variances. This in turn, makes for easier replication for the SAE networks due to less variety in the sample set, which is seen in Figure (b) with the noticeably better performance in the [1, 5, 20] configuration. \relax }}{75}{figure.caption.41}
\contentsline {figure}{\numberline {28}{\ignorespaces Dataset: Synthetic10 dataset (\ref {dataset_synthetic10}), Configuration \newline The boxplots here show the P\&L from the MMS according group by the data window configurations. There's a clear trend of P\&L increasing as the length of the windows increase. Shorter term GBM data would be more likely to represent fluctuations, whereas the longer term windows will be more representative of the constant mean in the stocks, leading to easier predictive performance and higher P\&L.\relax }}{75}{figure.caption.42}
\contentsline {figure}{\numberline {29}{\ignorespaces Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration \newline The P\&L by data window configurations for actual data show a more complex view. At a general level, predictive performance is increasing as the span of the windows increase. However, the highest P\&L occurs for the shortest window configuration ([1, 5, 20]). The suggestion here is that both long term trends and short term fluctuations are useful for making short term predictions in price changes, but that a middle of the road configuration offers the worst of both dynamics.\relax }}{76}{figure.caption.43}
\contentsline {figure}{\numberline {30}{\ignorespaces Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration \newline The box plots above show P\&L grouped by the number of epochs in the SGD IS training phase (i.e. the number of times the IS data was trained on). In this set of configurations, 100 Epochs offers the best overall performance, and further training to 500 or 1000 epochs degrades performance due to the network overfitting on the IS data. The results here are noteworthy as they suggest the benefit of historical data is limited - having a network become better at predicting returns 10 years ago is not leading to increased P\&L for more current data. The small difference between 10 and 100 Epochs further emphasises this point. \relax }}{77}{figure.caption.44}
\contentsline {figure}{\numberline {31}{\ignorespaces Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration (some samples with 0 P\&L were excluded for more effective visualization) \newline To further explore the effect of IS training on historical data, configurations were run with a percentage of the usual trianing data excluded, with the P\&L results grouped above. It is very interesting to note that the exclusion of up to 80\% of the IS training data resulted in only a 2.2\% drop in median P\&L for those networks. The training in these instances were not adjusted to increase the number of epochs according to the size of IS data, and so the configurations with more data excluded were also in essence trained less. These results, combined with those in Figure \ref {figure-results_pl_max_epochs} show the limited use in training on historical data, and hence the far greater value in current cross sectional information.\relax }}{77}{figure.caption.45}
\contentsline {figure}{\numberline {32}{\ignorespaces \newline The curve here shows the significant impact that the choice in the split parameters has for the PBO score calculated, with a monotonically decreasing score as S increases (these scores were for the same set of results). \relax }}{79}{figure.caption.48}
\contentsline {figure}{\numberline {33}{\ignorespaces \newline The number of combinations that have to be procssed by the CSCV method according to the value of the split parameter ($S$) chosen\relax }}{80}{figure.caption.49}
\contentsline {figure}{\numberline {34}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The CSCV logit distribution for all configurations run, with a calculated PBO of 1.6\%.\relax }}{80}{figure.caption.51}
\contentsline {figure}{\numberline {35}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The CSCV logit distribution for a narrower of configurations run, with a calculated PBO of 6.3\%.\relax }}{81}{figure.caption.52}
\contentsline {figure}{\numberline {36}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The PDF of all OOS P\&L values, with the benchmark P\&L indicated in orange.\relax }}{82}{figure.caption.54}
\contentsline {figure}{\numberline {37}{\ignorespaces Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration \newline The plot above shows the SAE MSE, grouped by encoding size and activations, showing a marginal increase in performance for Leaky ReLU activations. \relax }}{85}{figure.caption.57}
\contentsline {figure}{\numberline {38}{\ignorespaces The plot above shows the prediction accuracy achieved on the MNIST dataset according to the number of pre-training epochs using the RBM greedy layerwise methodology as per section \ref {imp_rbm}. There's a clear indication that pre-training allows the network to achieve much higher accuracy much quicker.\relax }}{85}{figure.caption.58}
\contentsline {figure}{\numberline {39}{\ignorespaces Dataset: AGL (\ref {dataset_agl}) ; Configurations 3 \& 4 (\ref {config3}, \ref {config4}) \newline This figure shows the effects of the difference encoding layer sizes on the P\&L when the relevant SAE is used for the predictive FFN, as for the AGL dataset (with an input of 3 features). Performance for both sizes are similar, though increasing the number of assets may result in a different effect being seen.\relax }}{86}{figure.caption.59}
\contentsline {figure}{\numberline {40}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations \newline This figure shows the MSE achieved by SAE networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. The network sizes here indicate the final $N$ layers for the SAE, rather than the $2N + 1$ that are present in training. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training. As noted in \ref {results_network}, the more typical structure of SAEs with descending layer sizes did not show better performance.\relax }}{86}{figure.caption.60}
\contentsline {figure}{\numberline {41}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline This figure shows the MSE achieved by networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. The network sizes here indicate the final $N$ layers for the SAE, rather than the $2N + 1$ that are present in training. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training. As noted in \ref {results_network}, the more typical structure of SAEs with descending layer sizes did not show better performance.\relax }}{87}{figure.caption.61}
\contentsline {figure}{\numberline {42}{\ignorespaces Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations \newline This figure shows the P\&L achieved by networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. Networks that suffered from exploding ReLU's (and hence have approximately 0 P\&L) have been excluded for the more effective visualization. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training.\relax }}{87}{figure.caption.62}
\contentsline {figure}{\numberline {43}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline This figure shows the P\&L achieved by networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. Networks that suffered from exploding ReLU's (and hence have approximately 0 P\&L) have been excluded for the more effective visualization. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training.\relax }}{88}{figure.caption.63}
\contentsline {figure}{\numberline {44}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline The SAE networks didn't show significant differences according to the epoch cycles. As discussed in sections \ref {results_gbm_data} and \ref {results_data_hist}, the structure of synthetic data for SAE replication is structurally less complex, and so learning optimizations are less likely to produce large differences.\relax }}{88}{figure.caption.64}
\contentsline {figure}{\numberline {45}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline The networks trained on Synthetic data showed a sharp P\&L increase as the OGD learning rate increased. It wasn't tested if they would experience the same turning point and begin to degrade as seen in networks for Actual data in \ref {results_lr}\relax }}{89}{figure.caption.65}
\contentsline {figure}{\numberline {46}{\ignorespaces Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline \relax }}{89}{figure.caption.66}
