\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ this will need to be rewritten once everything is finalised}{2}{section*.1}}
\pgfsyspdfmark {pgfid1}{7555917}{40875316}
\pgfsyspdfmark {pgfid4}{36009438}{40908876}
\pgfsyspdfmark {pgfid5}{37631454}{40663690}
\citation{Hinton2}
\@writefile{toc}{\contentsline {section}{List of Figures}{5}{section*.1}}
\citation{Hinton2}
\citation{Glorot2}
\@writefile{toc}{\contentsline {section}{List of Tables}{12}{section*.1}}
\citation{BailyPBO}
\@writefile{toc}{\contentsline {part}{{\fontencoding  {OT1}\selectfont  I}\hspace  {1em}\relax \fontsize  {12}{14}\selectfont  {Online non-linear prediction of {\@@par }financial time-series patterns}}{13}{part.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{13}{section.1}}
\newlabel{Introduction}{{1}{13}{Introduction}{section.1}{}}
\newlabel{Introduction@cref}{{[section][1][]1}{13}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ may need to add some appropriate in this section}{13}{section*.2}}
\pgfsyspdfmark {pgfid6}{7555917}{37483163}
\pgfsyspdfmark {pgfid9}{36009438}{37516723}
\pgfsyspdfmark {pgfid10}{37631454}{37271537}
\citation{Hinton2}
\citation{BailyPBO}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{red}{}{red!25}{\leavevmode {\color  {red!25}o}}\ check this}{14}{section*.3}}
\pgfsyspdfmark {pgfid11}{13166537}{43201844}
\pgfsyspdfmark {pgfid14}{36009438}{43235404}
\pgfsyspdfmark {pgfid15}{37631454}{42990218}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add ref}{14}{section*.4}}
\pgfsyspdfmark {pgfid16}{16936821}{36910388}
\pgfsyspdfmark {pgfid19}{36009438}{36943948}
\pgfsyspdfmark {pgfid20}{37631454}{36698762}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ this will need to be finished once results are known}{14}{section*.5}}
\pgfsyspdfmark {pgfid21}{30717467}{32191796}
\pgfsyspdfmark {pgfid24}{36009438}{32225356}
\pgfsyspdfmark {pgfid25}{37631454}{31980170}
\citation{Murphy}
\citation{Murphy}
\citation{Griffioen}
\citation{Kahn}
\citation{Schwager}
\citation{Johnson}
\citation{Arthur}
\citation{Crutchfield}
\citation{Packard}
\citation{Takens}
\citation{Skabar}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{15}{section.2}}
\newlabel{lr_LiteratureReview}{{2}{15}{Literature Review}{section.2}{}}
\newlabel{lr_LiteratureReview@cref}{{[section][2][]2}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Technical Analysis}{15}{subsection.2.1}}
\newlabel{lr_TechnicalAnalysis}{{2.1}{15}{Technical Analysis}{subsection.2.1}{}}
\newlabel{lr_TechnicalAnalysis@cref}{{[subsection][1][2]2.1}{15}}
\citation{Schmidhuber}
\citation{Ivakhnenko}
\citation{Werbos}
\citation{Siegelmann}
\citation{Hochreiter}
\citation{Schmidhuber}
\citation{Minksy}
\citation{LeCun2}
\citation{Werbos2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural Networks}{16}{subsection.2.2}}
\newlabel{lr_nn}{{2.2}{16}{Neural Networks}{subsection.2.2}{}}
\newlabel{lr_nn@cref}{{[subsection][2][2]2.2}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Training and Backpropagation}{16}{subsubsection.2.2.1}}
\newlabel{lr_trainingbackprop}{{2.2.1}{16}{Training and Backpropagation}{subsubsection.2.2.1}{}}
\newlabel{lr_trainingbackprop@cref}{{[subsubsection][1][2,2]2.2.1}{16}}
\citation{Rumelhart}
\citation{LeCun3}
\citation{Pascanu}
\citation{Schmidhuber}
\citation{LeCun4}
\citation{Dauphin}
\citation{Ge}
\citation{Hornik}
\citation{Wu}
\citation{Glorot}
\citation{Glorot2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Activation Functions}{17}{subsubsection.2.2.2}}
\newlabel{lr_activationfunctions}{{2.2.2}{17}{Activation Functions}{subsubsection.2.2.2}{}}
\newlabel{lr_activationfunctions@cref}{{[subsubsection][2][2,2]2.2.2}{17}}
\citation{Bengio1}
\citation{Hinton1}
\citation{Hinton1}
\citation{Ranzato1}
\citation{Hinton2}
\citation{Hinton1}
\citation{LeRoux}
\citation{Bengio2}
\citation{Sermanet}
\citation{ImageNet}
\citation{WaveNet}
\citation{ImageNet}
\citation{Glorot2}
\citation{Ciresan}
\citation{Bengio3}
\citation{Glorot}
\citation{Glorot}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Deep Learning}{18}{subsubsection.2.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Weight Initialization Improvements}{18}{subsubsection.2.2.4}}
\newlabel{lr_weight_init}{{2.2.4}{18}{Weight Initialization Improvements}{subsubsection.2.2.4}{}}
\newlabel{lr_weight_init@cref}{{[subsubsection][4][2,2]2.2.4}{18}}
\citation{He}
\citation{Hornik}
\citation{Schaefer}
\citation{Donoho}
\citation{Fan1}
\citation{Fan2}
\citation{Fama}
\citation{Langkvist}
\citation{Langkvist}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Stacked Autoencoders}{19}{subsection.2.3}}
\newlabel{lr_SAE}{{2.3}{19}{Stacked Autoencoders}{subsection.2.3}{}}
\newlabel{lr_SAE@cref}{{[subsection][3][2]2.3}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}High Dimensional Data Reduction}{19}{subsubsection.2.3.1}}
\newlabel{HDDR}{{2.3.1}{19}{High Dimensional Data Reduction}{subsubsection.2.3.1}{}}
\newlabel{HDDR@cref}{{[subsubsection][1][2,3]2.3.1}{19}}
\citation{Hinton1}
\citation{Ranzato1}
\citation{Bengio1}
\citation{Hinton2}
\citation{Hinton3}
\citation{Hinton2}
\citation{Hinton2}
\citation{Vincent}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Deep Belief Networks}{20}{subsubsection.2.3.2}}
\newlabel{DBN}{{2.3.2}{20}{Deep Belief Networks}{subsubsection.2.3.2}{}}
\newlabel{DBN@cref}{{[subsubsection][2][2,3]2.3.2}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Stacked Denoising Autoencoders}{20}{subsubsection.2.3.3}}
\newlabel{lr_SDAE}{{2.3.3}{20}{Stacked Denoising Autoencoders}{subsubsection.2.3.3}{}}
\newlabel{lr_SDAE@cref}{{[subsubsection][3][2,3]2.3.3}{20}}
\citation{Vincent}
\citation{Vincent}
\citation{Erhan}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Autoencoder training steps \cite  {Hinton2}\relax }}{21}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figure-DBN-RBM}{{1}{21}{The Autoencoder training steps \cite {Hinton2}\relax }{figure.caption.6}{}}
\newlabel{figure-DBN-RBM@cref}{{[figure][1][]1}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Pre-training}{21}{subsubsection.2.3.4}}
\citation{Lv}
\citation{Langkvist}
\citation{Takeuchi}
\citation{Zhao}
\citation{Troiano}
\citation{Bao}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}Time Series Applications}{22}{subsubsection.2.3.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.6}Financial Applications}{22}{subsubsection.2.3.6}}
\citation{Hsu}
\citation{Albers}
\citation{Bottou}
\citation{LeCun}
\citation{Bottou2}
\citation{Bottou}
\citation{Shalev}
\citation{Zhang}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Online Learning Algorithms and Gradient Descent}{23}{subsection.2.4}}
\newlabel{lr_OGD}{{2.4}{23}{Online Learning Algorithms and Gradient Descent}{subsection.2.4}{}}
\newlabel{lr_OGD@cref}{{[subsection][4][2]2.4}{23}}
\citation{Tseng}
\citation{Bartlett}
\citation{Langford}
\citation{Duchi}
\citation{Zeiler}
\citation{Hinton4}
\citation{Goodfellow}
\citation{Wang2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Gradient Learning Improvements}{24}{subsection.2.5}}
\newlabel{lr_grad_improv}{{2.5}{24}{Gradient Learning Improvements}{subsection.2.5}{}}
\newlabel{lr_grad_improv@cref}{{[subsection][5][2]2.5}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Gradient Adjustments and Regularization}{24}{subsubsection.2.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Dropout}{24}{subsubsection.2.5.2}}
\citation{Smith}
\citation{Smith}
\citation{Loshchilov}
\citation{Loshchilov}
\citation{Ioannidis}
\citation{BailyPBO}
\citation{McLean}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Learning Rate Schedules}{25}{subsubsection.2.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Backtesting and Model Validation}{25}{subsection.2.6}}
\newlabel{lr_backtesting}{{2.6}{25}{Backtesting and Model Validation}{subsection.2.6}{}}
\newlabel{lr_backtesting@cref}{{[subsection][6][2]2.6}{25}}
\citation{Schorfheide}
\citation{Prado}
\citation{Schorfheide}
\citation{Weiss}
\citation{Hawkins}
\citation{BailyPBO}
\citation{Hansen}
\citation{Aparicio}
\citation{BailyPBO}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Testing Methodologies}{26}{subsubsection.2.6.1}}
\newlabel{lr_cscv}{{2.6.1}{26}{Testing Methodologies}{subsubsection.2.6.1}{}}
\newlabel{lr_cscv@cref}{{[subsubsection][1][2,6]2.6.1}{26}}
\citation{Lo}
\citation{BaileyBTL}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Test Data Length}{27}{subsubsection.2.6.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ shorten section?}{27}{section*.7}}
\pgfsyspdfmark {pgfid26}{7555917}{36130890}
\pgfsyspdfmark {pgfid29}{36009438}{36164450}
\pgfsyspdfmark {pgfid30}{37631454}{35919264}
\newlabel{SRAnnual}{{1}{27}{Test Data Length}{equation.2.1}{}}
\newlabel{SRAnnual@cref}{{[equation][1][]1}{27}}
\newlabel{SRConvergence}{{2}{27}{Test Data Length}{equation.2.2}{}}
\newlabel{SRConvergence@cref}{{[equation][2][]2}{27}}
\newlabel{MinBTL}{{3}{27}{Test Data Length}{equation.2.3}{}}
\newlabel{MinBTL@cref}{{[equation][3][]3}{27}}
\citation{BaileyBTL}
\citation{BaileySharpe}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Sharpe Ratio}{28}{subsubsection.2.6.3}}
\newlabel{SR}{{4}{28}{Sharpe Ratio}{equation.2.4}{}}
\newlabel{SR@cref}{{[equation][4][]4}{28}}
\newlabel{tratio}{{5}{28}{Sharpe Ratio}{equation.2.5}{}}
\newlabel{tratio@cref}{{[equation][5][]5}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation: Data Processing and Generation }{29}{section.3}}
\newlabel{Data}{{3}{29}{Implementation: Data Processing and Generation }{section.3}{}}
\newlabel{Data@cref}{{[section][3][]3}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data Processing}{29}{subsection.3.1}}
\newlabel{data_processing}{{3.1}{29}{Data Processing}{subsection.3.1}{}}
\newlabel{data_processing@cref}{{[subsection][1][3]3.1}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Log Difference Transformation and Aggregation}{29}{subsubsection.3.1.1}}
\newlabel{ldata_og_difference}{{3.1.1}{29}{Log Difference Transformation and Aggregation}{subsubsection.3.1.1}{}}
\newlabel{ldata_og_difference@cref}{{[subsubsection][1][3,1]3.1.1}{29}}
\newlabel{eq_logdiff}{{6}{29}{Log Difference Transformation and Aggregation}{equation.3.6}{}}
\newlabel{eq_logdiff@cref}{{[equation][6][]6}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Data Scaling}{29}{subsubsection.3.1.2}}
\newlabel{data_scaling}{{3.1.2}{29}{Data Scaling}{subsubsection.3.1.2}{}}
\newlabel{data_scaling@cref}{{[subsubsection][2][3,1]3.1.2}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Reverse Data Scaling}{30}{subsubsection.3.1.3}}
\newlabel{data_reverse_scaling}{{3.1.3}{30}{Reverse Data Scaling}{subsubsection.3.1.3}{}}
\newlabel{data_reverse_scaling@cref}{{[subsubsection][3][3,1]3.1.3}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Price Reconstruction}{30}{subsubsection.3.1.4}}
\newlabel{data_price_recon}{{3.1.4}{30}{Price Reconstruction}{subsubsection.3.1.4}{}}
\newlabel{data_price_recon@cref}{{[subsubsection][4][3,1]3.1.4}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Synthetic Data Generation}{31}{subsection.3.2}}
\newlabel{data_synthetic}{{3.2}{31}{Synthetic Data Generation}{subsection.3.2}{}}
\newlabel{data_synthetic@cref}{{[subsection][2][3]3.2}{31}}
\newlabel{algo_brownianmotion}{{1}{31}{Synthetic Data Generation}{algocfline.1}{}}
\newlabel{algo_brownianmotion@cref}{{[line][1][]1}{31}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Geometric Brownian Motion Simulation\relax }}{31}{algocf.1}}
\citation{Schmidhuber}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation: Models and Algorithms}{32}{section.4}}
\newlabel{Implementation}{{4}{32}{Implementation: Models and Algorithms}{section.4}{}}
\newlabel{Implementation@cref}{{[section][4][]4}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Process Overview}{32}{subsection.4.1}}
\newlabel{ProcessOverview}{{4.1}{32}{Process Overview}{subsection.4.1}{}}
\newlabel{ProcessOverview@cref}{{[subsection][1][4]4.1}{32}}
\newlabel{imp_overview}{{4.1}{32}{Process Overview}{subsection.4.1}{}}
\newlabel{imp_overview@cref}{{[subsection][1][4]4.1}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Feedforward Neural Networks}{32}{subsection.4.2}}
\newlabel{imp_ffn}{{4.2}{32}{Feedforward Neural Networks}{subsection.4.2}{}}
\newlabel{imp_ffn@cref}{{[subsection][2][4]4.2}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example diagram of a Feedforwrd Neural Network with 1 hidden layer\relax }}{33}{figure.caption.8}}
\newlabel{figure-neural_network_diagram}{{2}{33}{An example diagram of a Feedforwrd Neural Network with 1 hidden layer\relax }{figure.caption.8}{}}
\newlabel{figure-neural_network_diagram@cref}{{[figure][2][]2}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Notation and Network Representation}{33}{subsubsection.4.2.1}}
\newlabel{imp_ffn_functions}{{4.2.1}{33}{Notation and Network Representation}{subsubsection.4.2.1}{}}
\newlabel{imp_ffn_functions@cref}{{[subsubsection][1][4,2]4.2.1}{33}}
\newlabel{eq_weighted_input}{{16}{33}{Notation and Network Representation}{equation.4.16}{}}
\newlabel{eq_weighted_input@cref}{{[equation][16][]16}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Activation Functions}{33}{subsubsection.4.2.2}}
\newlabel{imp_activation_functions}{{4.2.2}{33}{Activation Functions}{subsubsection.4.2.2}{}}
\newlabel{imp_activation_functions@cref}{{[subsubsection][2][4,2]4.2.2}{33}}
\@writefile{toc}{\contentsline {subparagraph}{Sigmoid}{33}{subsubsection.4.2.2}}
\newlabel{func_sigmoid}{{17}{33}{Sigmoid}{equation.4.17}{}}
\newlabel{func_sigmoid@cref}{{[equation][17][]17}{33}}
\newlabel{func_sigmoidprime}{{18}{33}{Sigmoid}{equation.4.18}{}}
\newlabel{func_sigmoidprime@cref}{{[equation][18][]18}{33}}
\@writefile{toc}{\contentsline {subparagraph}{ReLU}{34}{equation.4.18}}
\newlabel{func_relu}{{19}{34}{ReLU}{equation.4.19}{}}
\newlabel{func_relu@cref}{{[equation][19][]19}{34}}
\newlabel{func_relu_prime}{{4.2.2}{34}{ReLU}{equation.4.20}{}}
\newlabel{func_relu_prime@cref}{{[subsubsection][2][4,2]4.2.2}{34}}
\@writefile{toc}{\contentsline {subparagraph}{Leaky ReLU}{34}{equation.4.20}}
\newlabel{func_leaky_relu}{{4.2.2}{34}{Leaky ReLU}{equation.4.21}{}}
\newlabel{func_leaky_relu@cref}{{[subsubsection][2][4,2]4.2.2}{34}}
\newlabel{func_leaky_relu_prime}{{4.2.2}{34}{Leaky ReLU}{equation.4.22}{}}
\newlabel{func_leaky_relu_prime@cref}{{[subsubsection][2][4,2]4.2.2}{34}}
\@writefile{toc}{\contentsline {subparagraph}{Linear Activation}{34}{equation.4.22}}
\newlabel{func_linear}{{23}{34}{Linear Activation}{equation.4.23}{}}
\newlabel{func_linear@cref}{{[equation][23][]23}{34}}
\newlabel{func_linear_prime}{{24}{34}{Linear Activation}{equation.4.24}{}}
\newlabel{func_linear_prime@cref}{{[equation][24][]24}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Backpropagation}{35}{subsubsection.4.2.3}}
\newlabel{imp_backprop}{{4.2.3}{35}{Backpropagation}{subsubsection.4.2.3}{}}
\newlabel{imp_backprop@cref}{{[subsubsection][3][4,2]4.2.3}{35}}
\newlabel{func_MSE}{{25}{35}{Backpropagation}{equation.4.25}{}}
\newlabel{func_MSE@cref}{{[equation][25][]25}{35}}
\newlabel{eq_lambda1}{{26}{35}{Backpropagation}{equation.4.26}{}}
\newlabel{eq_lambda1@cref}{{[equation][26][]26}{35}}
\newlabel{eq_lambda2}{{27}{35}{Backpropagation}{equation.4.27}{}}
\newlabel{eq_lambda2@cref}{{[equation][27][]27}{35}}
\newlabel{eq_lambda3}{{28}{35}{Backpropagation}{equation.4.28}{}}
\newlabel{eq_lambda3@cref}{{[equation][28][]28}{35}}
\newlabel{eq_lambda4}{{29}{35}{Backpropagation}{equation.4.29}{}}
\newlabel{eq_lambda4@cref}{{[equation][29][]29}{35}}
\newlabel{eq_bp_weightupdate}{{30}{36}{Backpropagation}{equation.4.30}{}}
\newlabel{eq_bp_weightupdate@cref}{{[equation][30][]30}{36}}
\newlabel{algo_backprop}{{2}{36}{Backpropagation}{algocfline.2}{}}
\newlabel{algo_backprop@cref}{{[line][2][]2}{36}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Backpropagation\relax }}{36}{algocf.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Gradient Descent Algorithms}{36}{subsubsection.4.2.4}}
\newlabel{imp_sgd}{{4.2.4}{36}{Gradient Descent Algorithms}{subsubsection.4.2.4}{}}
\newlabel{imp_sgd@cref}{{[subsubsection][4][4,2]4.2.4}{36}}
\newlabel{eq_backprop_weightupdate_sgd}{{31}{37}{Gradient Descent Algorithms}{equation.4.31}{}}
\newlabel{eq_backprop_weightupdate_sgd@cref}{{[equation][31][]31}{37}}
\@writefile{toc}{\contentsline {subparagraph}{Online Gradient Descent}{37}{equation.4.31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Regularization}{37}{subsubsection.4.2.5}}
\newlabel{imp_regularization}{{4.2.5}{37}{Regularization}{subsubsection.4.2.5}{}}
\newlabel{imp_regularization@cref}{{[subsubsection][5][4,2]4.2.5}{37}}
\newlabel{func_l2reg}{{32}{37}{Regularization}{equation.4.32}{}}
\newlabel{func_l2reg@cref}{{[equation][32][]32}{37}}
\newlabel{func_l2_weight_update}{{33}{37}{Regularization}{equation.4.33}{}}
\newlabel{func_l2_weight_update@cref}{{[equation][33][]33}{37}}
\newlabel{func_sgd_l2}{{34}{37}{Regularization}{equation.4.34}{}}
\newlabel{func_sgd_l2@cref}{{[equation][34][]34}{37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.6}Learning Rate Schedule}{37}{subsubsection.4.2.6}}
\newlabel{imp_learning_rate_schedule}{{4.2.6}{37}{Learning Rate Schedule}{subsubsection.4.2.6}{}}
\newlabel{imp_learning_rate_schedule@cref}{{[subsubsection][6][4,2]4.2.6}{37}}
\citation{Smith}
\newlabel{func_learning_rate_sched}{{35}{38}{Learning Rate Schedule}{equation.4.35}{}}
\newlabel{func_learning_rate_sched@cref}{{[equation][35][]35}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning rates calculated over 1000 epochs with $\eta _{\qopname  \relax m{min}} = 0.1$ to $\eta _{\qopname  \relax m{max}} = 1.0$ and $i=100$\relax }}{38}{figure.caption.9}}
\newlabel{figure-SGDRLearningRates}{{3}{38}{Learning rates calculated over 1000 epochs with $\eta _{\min } = 0.1$ to $\eta _{\max } = 1.0$ and $i=100$\relax }{figure.caption.9}{}}
\newlabel{figure-SGDRLearningRates@cref}{{[figure][3][]3}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.7}Dropout}{38}{subsubsection.4.2.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Restricted Boltzmann Machines}{38}{subsection.4.3}}
\newlabel{imp_rbm}{{4.3}{38}{Restricted Boltzmann Machines}{subsection.4.3}{}}
\newlabel{imp_rbm@cref}{{[subsection][3][4]4.3}{38}}
\citation{Hinton5}
\citation{Hinton5}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example diagram of a Restricted Boltzmann Machine network\relax }}{39}{figure.caption.10}}
\newlabel{figure-rbm_network_diagram}{{4}{39}{An example diagram of a Restricted Boltzmann Machine network\relax }{figure.caption.10}{}}
\newlabel{figure-rbm_network_diagram@cref}{{[figure][4][]4}{39}}
\newlabel{func_rbmenergy}{{36}{39}{Restricted Boltzmann Machines}{equation.4.36}{}}
\newlabel{func_rbmenergy@cref}{{[equation][36][]36}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Contrastive Divergence}{40}{subsubsection.4.3.1}}
\newlabel{imp_CD}{{4.3.1}{40}{Contrastive Divergence}{subsubsection.4.3.1}{}}
\newlabel{imp_CD@cref}{{[subsubsection][1][4,3]4.3.1}{40}}
\newlabel{algo_cd1}{{3}{40}{Contrastive Divergence}{equation.4.41}{}}
\newlabel{algo_cd1@cref}{{[line][3][]3}{40}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces CD-1\relax }}{40}{algocf.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}CD-1 and SGD}{40}{subsubsection.4.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Stacked Autoencoders}{40}{subsection.4.4}}
\newlabel{imp_SAE}{{4.4}{40}{Stacked Autoencoders}{subsection.4.4}{}}
\newlabel{imp_SAE@cref}{{[subsection][4][4]4.4}{40}}
\citation{Hinton2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Sigmoid based Greedy Layerwise SAE Training}{41}{subsubsection.4.4.1}}
\newlabel{imp_sigmoidsae}{{4.4.1}{41}{Sigmoid based Greedy Layerwise SAE Training}{subsubsection.4.4.1}{}}
\newlabel{imp_sigmoidsae@cref}{{[subsubsection][1][4,4]4.4.1}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}ReLU based SAE Training}{41}{subsubsection.4.4.2}}
\newlabel{imp_relusae}{{4.4.2}{41}{ReLU based SAE Training}{subsubsection.4.4.2}{}}
\newlabel{imp_relusae@cref}{{[subsubsection][2][4,4]4.4.2}{41}}
\citation{He}
\citation{Glorot}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Denoising Autoencoders}{42}{subsubsection.4.4.3}}
\@writefile{toc}{\contentsline {subparagraph}{Additive Gaussian Noise}{42}{subsubsection.4.4.3}}
\@writefile{toc}{\contentsline {subparagraph}{Masking Noise}{42}{equation.4.42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Variance Based Weight Initializations}{42}{subsection.4.5}}
\newlabel{imp_weights}{{4.5}{42}{Variance Based Weight Initializations}{subsection.4.5}{}}
\newlabel{imp_weights@cref}{{[subsection][5][4]4.5}{42}}
\newlabel{func_uniform_init}{{43}{42}{Variance Based Weight Initializations}{equation.4.43}{}}
\newlabel{func_uniform_init@cref}{{[equation][43][]43}{42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Initialization Rationale}{42}{subsubsection.4.5.1}}
\citation{Glorot}
\citation{He}
\newlabel{eq_init_var}{{47}{43}{Initialization Rationale}{equation.4.47}{}}
\newlabel{eq_init_var@cref}{{[equation][47][]47}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Initializations}{43}{subsubsection.4.5.2}}
\@writefile{toc}{\contentsline {subparagraph}{Xavier}{43}{subsubsection.4.5.2}}
\@writefile{toc}{\contentsline {subparagraph}{He}{43}{equation.4.48}}
\@writefile{toc}{\contentsline {subparagraph}{He-Adjusted}{43}{equation.4.49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}CSCV \& PBO}{43}{subsection.4.6}}
\newlabel{imp_cscv}{{4.6}{43}{CSCV \& PBO}{subsection.4.6}{}}
\newlabel{imp_cscv@cref}{{[subsection][6][4]4.6}{43}}
\citation{BailyPBO}
\newlabel{eq:PBO1}{{51}{44}{CSCV \& PBO}{equation.4.51}{}}
\newlabel{eq:PBO1@cref}{{[equation][51][]51}{44}}
\newlabel{eq:PBO2}{{52}{44}{CSCV \& PBO}{equation.4.52}{}}
\newlabel{eq:PBO2@cref}{{[equation][52][]52}{44}}
\newlabel{algo_cscv}{{4}{45}{CSCV \& PBO}{equation.4.53}{}}
\newlabel{algo_cscv@cref}{{[line][4][]4}{45}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces CSCV\relax }}{45}{algocf.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Money Management Strategy and Returns}{46}{subsection.4.7}}
\newlabel{imp_mms}{{4.7}{46}{Money Management Strategy and Returns}{subsection.4.7}{}}
\newlabel{imp_mms@cref}{{[subsection][7][4]4.7}{46}}
\@writefile{toc}{\contentsline {subparagraph}{Input Variables}{46}{subsection.4.7}}
\@writefile{toc}{\contentsline {subparagraph}{Calculated Stock Variables}{46}{subsection.4.7}}
\@writefile{toc}{\contentsline {subparagraph}{Calculated Strategy Variables}{47}{equation.4.64}}
\citation{Wilcox}
\@writefile{toc}{\contentsline {section}{\numberline {5}Implementation: Process}{48}{section.5}}
\newlabel{imp_proc}{{5}{48}{Implementation: Process}{section.5}{}}
\newlabel{imp_proc@cref}{{[section][5][]5}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data Preparation}{48}{subsection.5.1}}
\newlabel{proc_dataprep}{{5.1}{48}{Data Preparation}{subsection.5.1}{}}
\newlabel{proc_dataprep@cref}{{[subsection][1][5]5.1}{48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Data Window Aggregations}{48}{subsubsection.5.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Point Predictions}{48}{subsubsection.5.1.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add result reference}{48}{section*.11}}
\pgfsyspdfmark {pgfid31}{8349364}{8610957}
\pgfsyspdfmark {pgfid34}{36009438}{8644517}
\pgfsyspdfmark {pgfid35}{37631454}{8399331}
\citation{BailyPBO}
\citation{BaileyBTL}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add result references}{49}{section*.12}}
\pgfsyspdfmark {pgfid36}{26819891}{37729588}
\pgfsyspdfmark {pgfid39}{36009438}{37763148}
\pgfsyspdfmark {pgfid40}{37631454}{37517962}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Scaling}{49}{subsubsection.5.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Data Segregation}{49}{subsection.5.2}}
\newlabel{proc_dataseg}{{5.2}{49}{Data Segregation}{subsection.5.2}{}}
\newlabel{proc_dataseg@cref}{{[subsection][2][5]5.2}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}SAE Training}{49}{subsection.5.3}}
\newlabel{proc_sae}{{5.3}{49}{SAE Training}{subsection.5.3}{}}
\newlabel{proc_sae@cref}{{[subsection][3][5]5.3}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Prediction Network Training}{50}{subsection.5.4}}
\newlabel{proc_predictionnetwork}{{5.4}{50}{Prediction Network Training}{subsection.5.4}{}}
\newlabel{proc_predictionnetwork@cref}{{[subsection][4][5]5.4}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Price Reconstruction}{50}{subsection.5.5}}
\newlabel{proc_precerecon}{{5.5}{50}{Price Reconstruction}{subsection.5.5}{}}
\newlabel{proc_precerecon@cref}{{[subsection][5][5]5.5}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Money Management Strategy}{50}{subsection.5.6}}
\newlabel{proc_mms}{{5.6}{50}{Money Management Strategy}{subsection.5.6}{}}
\newlabel{proc_mms@cref}{{[subsection][6][5]5.6}{50}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ Can we learn to beat best stock paper ? Paper}{51}{section*.13}}
\pgfsyspdfmark {pgfid41}{7555917}{36943156}
\pgfsyspdfmark {pgfid44}{36009438}{36976716}
\pgfsyspdfmark {pgfid45}{37631454}{36731530}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}CSCV \& PBO}{51}{subsection.5.7}}
\newlabel{proc_cscv}{{5.7}{51}{CSCV \& PBO}{subsection.5.7}{}}
\newlabel{proc_cscv@cref}{{[subsection][7][5]5.7}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Process Diagram}{51}{subsection.5.8}}
\newlabel{proc_diagram}{{5.8}{51}{Process Diagram}{subsection.5.8}{}}
\newlabel{proc_diagram@cref}{{[subsection][8][5]5.8}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Overall Process Flow\relax }}{52}{figure.caption.14}}
\newlabel{figure-proc_diagram}{{5}{52}{Overall Process Flow\relax }{figure.caption.14}{}}
\newlabel{figure-proc_diagram@cref}{{[figure][5][]5}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Datasets Used}{53}{section.6}}
\newlabel{Datasets}{{6}{53}{Datasets Used}{section.6}{}}
\newlabel{Datasets@cref}{{[section][6][]6}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Synthetic Datasets}{53}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Synthetic6}{53}{subsubsection.6.1.1}}
\newlabel{dataset_synthetic6}{{6.1.1}{53}{Synthetic6}{subsubsection.6.1.1}{}}
\newlabel{dataset_synthetic6@cref}{{[subsubsection][1][6,1]6.1.1}{53}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Synthetic 6 Dataset Configuration\relax }}{53}{table.caption.15}}
\newlabel{tab_synth6}{{1}{53}{Synthetic 6 Dataset Configuration\relax }{table.caption.15}{}}
\newlabel{tab_synth6@cref}{{[table][1][]1}{53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Synthetic10}{53}{subsubsection.6.1.2}}
\newlabel{dataset_synthetic10}{{6.1.2}{53}{Synthetic10}{subsubsection.6.1.2}{}}
\newlabel{dataset_synthetic10@cref}{{[subsubsection][2][6,1]6.1.2}{53}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Synthetic 10 Dataset Configuration\relax }}{54}{table.caption.16}}
\newlabel{tab_synth10}{{2}{54}{Synthetic 10 Dataset Configuration\relax }{table.caption.16}{}}
\newlabel{tab_synth10@cref}{{[table][2][]2}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Actual Datasets}{54}{subsection.6.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add ref}{54}{section*.17}}
\pgfsyspdfmark {pgfid46}{31755965}{23132196}
\pgfsyspdfmark {pgfid49}{36009438}{23165756}
\pgfsyspdfmark {pgfid50}{37631454}{22920570}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Actual10}{54}{subsubsection.6.2.1}}
\newlabel{dataset_actual10}{{6.2.1}{54}{Actual10}{subsubsection.6.2.1}{}}
\newlabel{dataset_actual10@cref}{{[subsubsection][1][6,2]6.2.1}{54}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Sytnetic 10 Dataset Configuration\relax }}{55}{table.caption.18}}
\newlabel{tab_actual10}{{3}{55}{Sytnetic 10 Dataset Configuration\relax }{table.caption.18}{}}
\newlabel{tab_actual10@cref}{{[table][3][]3}{55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}AGL}{55}{subsubsection.6.2.2}}
\newlabel{dataset_agl}{{6.2.2}{55}{AGL}{subsubsection.6.2.2}{}}
\newlabel{dataset_agl@cref}{{[subsubsection][2][6,2]6.2.2}{55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}AGL\&ACL}{55}{subsubsection.6.2.3}}
\newlabel{dataset_aglacl}{{6.2.3}{55}{AGL\&ACL}{subsubsection.6.2.3}{}}
\newlabel{dataset_aglacl@cref}{{[subsubsection][3][6,2]6.2.3}{55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4}Scaling10}{55}{subsubsection.6.2.4}}
\newlabel{dataset_scaling10}{{6.2.4}{55}{Scaling10}{subsubsection.6.2.4}{}}
\newlabel{dataset_scaling10@cref}{{[subsubsection][4][6,2]6.2.4}{55}}
\citation{Peters}
\@writefile{toc}{\contentsline {section}{\numberline {7}Results}{56}{section.7}}
\newlabel{Results}{{7}{56}{Results}{section.7}{}}
\newlabel{Results@cref}{{[section][7][]7}{56}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ }{56}{section*.19}}
\pgfsyspdfmark {pgfid51}{30844910}{45214588}
\pgfsyspdfmark {pgfid54}{36009438}{45248148}
\pgfsyspdfmark {pgfid55}{37631454}{45002962}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Linearity, Complexity and Structure of Data}{56}{subsection.7.1}}
\newlabel{results_linearity}{{7.1}{56}{Linearity, Complexity and Structure of Data}{subsection.7.1}{}}
\newlabel{results_linearity@cref}{{[subsection][1][7]7.1}{56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}GBM Generated Data}{56}{subsubsection.7.1.1}}
\newlabel{results_gbm_data}{{7.1.1}{56}{GBM Generated Data}{subsubsection.7.1.1}{}}
\newlabel{results_gbm_data@cref}{{[subsubsection][1][7,1]7.1.1}{56}}
\citation{Hinton2}
\citation{Glorot2}
\citation{Hinton2}
\citation{Glorot2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Effects of Activation Functions and Scaling}{57}{subsubsection.7.1.2}}
\newlabel{results_activations_scaling}{{7.1.2}{57}{Effects of Activation Functions and Scaling}{subsubsection.7.1.2}{}}
\newlabel{results_activations_scaling@cref}{{[subsubsection][2][7,1]7.1.2}{57}}
\newlabel{figure-actual_mse_scaling}{{6a}{57}{\textbf {Scaling Technique} \newline \relax }{figure.caption.20}{}}
\newlabel{figure-actual_mse_scaling@cref}{{[subfigure][1][6]6a}{57}}
\newlabel{sub@figure-actual_mse_scaling}{{a}{57}{\textbf {Scaling Technique} \newline \relax }{figure.caption.20}{}}
\newlabel{sub@figure-actual_mse_scaling@cref}{{[subfigure][1][6]6a}{57}}
\newlabel{figure-actual_mse_output}{{6b}{57}{\textbf {Output Activation} \newline \relax }{figure.caption.20}{}}
\newlabel{figure-actual_mse_output@cref}{{[subfigure][2][6]6b}{57}}
\newlabel{sub@figure-actual_mse_output}{{b}{57}{\textbf {Output Activation} \newline \relax }{figure.caption.20}{}}
\newlabel{sub@figure-actual_mse_output@cref}{{[subfigure][2][6]6b}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Dataset: Scaling10 dataset (\ref  {dataset_scaling10}), Configuration \newline  Figure (a) here shows SAE MSE performance according to the scaling techniques as described in \ref  {data_scaling}. As discussed more fully in \ref  {proc_dataprep}, the use of standardization for scaling the data doesn't allow for effective outlier treatments, resulting in the significantly worse performance seen here. \newline  Figure (b) shows the effects of the output activation for configurations using Normalized scaling. The ReLU activations also seemed to result in very poor performance, most likely due to the loss of error signal in the output layer where it is most critical. \relax }}{57}{figure.caption.20}}
\newlabel{figure-mse_scaling}{{6}{57}{Dataset: Scaling10 dataset (\ref {dataset_scaling10}), Configuration \newline Figure (a) here shows SAE MSE performance according to the scaling techniques as described in \ref {data_scaling}. As discussed more fully in \ref {proc_dataprep}, the use of standardization for scaling the data doesn't allow for effective outlier treatments, resulting in the significantly worse performance seen here. \newline Figure (b) shows the effects of the output activation for configurations using Normalized scaling. The ReLU activations also seemed to result in very poor performance, most likely due to the loss of error signal in the output layer where it is most critical. \relax }{figure.caption.20}{}}
\newlabel{figure-mse_scaling@cref}{{[figure][6][]6}{57}}
\newlabel{figure-actual_mse_encoding_activations}{{7a}{58}{\textbf {Encoding Activations} \newline \relax }{figure.caption.21}{}}
\newlabel{figure-actual_mse_encoding_activations@cref}{{[subfigure][1][7]7a}{58}}
\newlabel{sub@figure-actual_mse_encoding_activations}{{a}{58}{\textbf {Encoding Activations} \newline \relax }{figure.caption.21}{}}
\newlabel{sub@figure-actual_mse_encoding_activations@cref}{{[subfigure][1][7]7a}{58}}
\newlabel{figure-actual_mse_hidden_activations}{{7b}{58}{\textbf {Hidden Activations} \newline \relax }{figure.caption.21}{}}
\newlabel{figure-actual_mse_hidden_activations@cref}{{[subfigure][2][7]7b}{58}}
\newlabel{sub@figure-actual_mse_hidden_activations}{{b}{58}{\textbf {Hidden Activations} \newline \relax }{figure.caption.21}{}}
\newlabel{sub@figure-actual_mse_hidden_activations@cref}{{[subfigure][2][7]7b}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Dataset: Scaling10 dataset (\ref  {dataset_scaling10}), Configuration \newline  Figure (a) shows the best (minimum) MSE scores for different encoding activations at the different encoding layer sizes (with a network input of 30). There is a consisent out performance of Linear activations in the encoding layer, which usual to expected from literature \cite  {Hinton2}, and in line with reducing loss of error signal at critical points. \newline  Figure (b) shows the best (minimum) MSE scores for different hidden layer activations at the different encoding layer sizes (with a network input of 30). At larger encoding layers, there is a competitive performance from all linear networks, where they are less pressured to take advantage of non-linear effects. As the size of the encoding layer is reduced, the advantages of non-linear activations become more apparent, with outperformance by ReLU and Simgoid activations. Sigmoid activations are known to suffer from learning slow down as a result of saturation and so have increased sensitivity to vanishing gradients \cite  {Glorot2}. SAE networks are deep by nature, and so it is not surprising that the Sigmoid activations are resulting in worse performance over the same training period when compared to ReLU activations. \newline  \relax }}{58}{figure.caption.21}}
\newlabel{figure-mse_encoding_activations}{{7}{58}{Dataset: Scaling10 dataset (\ref {dataset_scaling10}), Configuration \newline Figure (a) shows the best (minimum) MSE scores for different encoding activations at the different encoding layer sizes (with a network input of 30). There is a consisent out performance of Linear activations in the encoding layer, which usual to expected from literature \cite {Hinton2}, and in line with reducing loss of error signal at critical points. \newline Figure (b) shows the best (minimum) MSE scores for different hidden layer activations at the different encoding layer sizes (with a network input of 30). At larger encoding layers, there is a competitive performance from all linear networks, where they are less pressured to take advantage of non-linear effects. As the size of the encoding layer is reduced, the advantages of non-linear activations become more apparent, with outperformance by ReLU and Simgoid activations. Sigmoid activations are known to suffer from learning slow down as a result of saturation and so have increased sensitivity to vanishing gradients \cite {Glorot2}. SAE networks are deep by nature, and so it is not surprising that the Sigmoid activations are resulting in worse performance over the same training period when compared to ReLU activations. \newline \relax }{figure.caption.21}{}}
\newlabel{figure-mse_encoding_activations@cref}{{[figure][7][]7}{58}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.3}Predictive FFN Activations and Scaling}{58}{subsubsection.7.1.3}}
\newlabel{figure-synth_pl_scaling}{{8a}{59}{\textbf {Encoding Activations} \newline \relax }{figure.caption.22}{}}
\newlabel{figure-synth_pl_scaling@cref}{{[subfigure][1][8]8a}{59}}
\newlabel{sub@figure-synth_pl_scaling}{{a}{59}{\textbf {Encoding Activations} \newline \relax }{figure.caption.22}{}}
\newlabel{sub@figure-synth_pl_scaling@cref}{{[subfigure][1][8]8a}{59}}
\newlabel{figure-synth_pl_hidden}{{8b}{59}{\textbf {Hidden Activations} \newline \relax }{figure.caption.22}{}}
\newlabel{figure-synth_pl_hidden@cref}{{[subfigure][2][8]8b}{59}}
\newlabel{sub@figure-synth_pl_hidden}{{b}{59}{\textbf {Hidden Activations} \newline \relax }{figure.caption.22}{}}
\newlabel{sub@figure-synth_pl_hidden@cref}{{[subfigure][2][8]8b}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Dataset Synthetic6 (\ref  {dataset_synthetic6})), Configuration \newline  Figure (a) shows the impact of the limited scaling technique (as described in \ref  {data_scaling}) in comparison to the non-limited version. The implementation of this is not a choice when interested in simulating a real world implementation, but it is still of interest to note the impact of this issue. Additionally, the use of a Linear output layer has been shown to assist in reducing the impact of this effect. \newline  Figure (b) offers a comparison of Linear and ReLU hidden layer activations on P\&L, with the linear activations resulting in notably better P\&L when compared to ReLU activations. Unlike the SAE networks, this persists even when the network size is decreased - this difference highlights the effects of GBM data and that it can be represented linearly, as per \ref  {results_gbm_data}, whereas we would see actual stock data to benefit from a non-linear representation. Linear configurations for actual data were not run, as real stock data is not expected to follow patterns that are linear through time, thus offering little value.\relax }}{59}{figure.caption.22}}
\newlabel{figure-pl_activations_scaling}{{8}{59}{Dataset Synthetic6 (\ref {dataset_synthetic6})), Configuration \newline Figure (a) shows the impact of the limited scaling technique (as described in \ref {data_scaling}) in comparison to the non-limited version. The implementation of this is not a choice when interested in simulating a real world implementation, but it is still of interest to note the impact of this issue. Additionally, the use of a Linear output layer has been shown to assist in reducing the impact of this effect. \newline Figure (b) offers a comparison of Linear and ReLU hidden layer activations on P\&L, with the linear activations resulting in notably better P\&L when compared to ReLU activations. Unlike the SAE networks, this persists even when the network size is decreased - this difference highlights the effects of GBM data and that it can be represented linearly, as per \ref {results_gbm_data}, whereas we would see actual stock data to benefit from a non-linear representation. Linear configurations for actual data were not run, as real stock data is not expected to follow patterns that are linear through time, thus offering little value.\relax }{figure.caption.22}{}}
\newlabel{figure-pl_activations_scaling@cref}{{[figure][8][]8}{59}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.4}Leaky ReLU vs ReLU}{59}{subsubsection.7.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Dataset Synthetic6 (\ref  {dataset_synthetic6}) ; Configuration \newline  The plot shows the FFN P\&L, grouped by activation, showing some improvements from the Leaky ReLU activation. The effect is most noticeable in reducing the lower bounds of performance (±10.8\%), though has a clear benefit in the upper bounds as well (3.4\%).\relax }}{60}{figure.caption.23}}
\newlabel{figure-synthetic_pl_leakyrelu}{{9}{60}{Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration \newline The plot shows the FFN P\&L, grouped by activation, showing some improvements from the Leaky ReLU activation. The effect is most noticeable in reducing the lower bounds of performance (±10.8\%), though has a clear benefit in the upper bounds as well (3.4\%).\relax }{figure.caption.23}{}}
\newlabel{figure-synthetic_pl_leakyrelu@cref}{{[figure][9][]9}{60}}
\citation{Hinton2}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Weight Initialization Techniques}{61}{subsection.7.2}}
\newlabel{results_init}{{7.2}{61}{Weight Initialization Techniques}{subsection.7.2}{}}
\newlabel{results_init@cref}{{[subsection][2][7]7.2}{61}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}RBM Pretraining for Sigmoid Networks}{61}{subsubsection.7.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Dataset AGL\&ACL (\ref  {dataset_aglacl}); Configuration7 (\ref  {config7}) \newline  \newline  The boxplots here show the summary of configuration performances, by minimum MSE achieved, grouped according to the number of pre-training epochs which the network had. There is a clear favour to having no pre-training in this scenario as there is a clear decrease in performance as the number of epochs increase (the low value outliers that can be seen for the 1 epoch configuration were with learning rates which were low enough to approximate no epochs).\relax }}{61}{figure.caption.24}}
\newlabel{figure-results-pretraining-effect}{{10}{61}{Dataset AGL\&ACL (\ref {dataset_aglacl}); Configuration7 (\ref {config7}) \newline \newline The boxplots here show the summary of configuration performances, by minimum MSE achieved, grouped according to the number of pre-training epochs which the network had. There is a clear favour to having no pre-training in this scenario as there is a clear decrease in performance as the number of epochs increase (the low value outliers that can be seen for the 1 epoch configuration were with learning rates which were low enough to approximate no epochs).\relax }{figure.caption.24}{}}
\newlabel{figure-results-pretraining-effect@cref}{{[figure][10][]10}{61}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {7.2.1.1}Sigmoid Activation Functions}{61}{paragraph.7.2.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Variance Based Weight Initialization Techniques}{62}{subsubsection.7.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}); Configuration 5 (\ref  {config5}) \newline  The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the similar performance of He-Adj and Xavier on synthetic datasets.\relax }}{63}{figure.caption.25}}
\newlabel{figure-results_it4_sae_init}{{11}{63}{Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configuration 5 (\ref {config5}) \newline The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the similar performance of He-Adj and Xavier on synthetic datasets.\relax }{figure.caption.25}{}}
\newlabel{figure-results_it4_sae_init@cref}{{[figure][11][]11}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Dataset Actual10 \newline  The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the outperformance of He-Adj to Xavier on actual datasets.\relax }}{63}{figure.caption.26}}
\newlabel{figure-results_init_actual10_all}{{12}{63}{Dataset Actual10 \newline The box plots show the MSE for a series of SAE networks trained, showing the outperformance of He-Adj compared to He in networks with changing layer sizes, and the outperformance of He-Adj to Xavier on actual datasets.\relax }{figure.caption.26}{}}
\newlabel{figure-results_init_actual10_all@cref}{{[figure][12][]12}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Dataset: AGL (\ref  {dataset_agl}); Configuration 3 (\ref  {config3}) \newline  The configurations run for AGL show notably better performance using the He-Adj based initialization in comparison to both He and Xavier. The emphasis seen here can be attributed to increased pressure on suitable weight initizliations as 1 and 2 node encoding layers were used.\relax }}{64}{figure.caption.27}}
\newlabel{figure-results_init_agl_all}{{13}{64}{Dataset: AGL (\ref {dataset_agl}); Configuration 3 (\ref {config3}) \newline The configurations run for AGL show notably better performance using the He-Adj based initialization in comparison to both He and Xavier. The emphasis seen here can be attributed to increased pressure on suitable weight initizliations as 1 and 2 node encoding layers were used.\relax }{figure.caption.27}{}}
\newlabel{figure-results_init_agl_all@cref}{{[figure][13][]13}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}); Configurations 5 and 6 (\ref  {config5}, \ref  {config6}) \newline  The figure here shows the P\&L performance for He-Adj and He being higher than Xavier, as to be expected with more balanced network layers and ReLU activations. The better performance in the lower end of the He-Adj and configurations when compared to He can be attributed to the inclusion of inconsistently sized layers in the configuration set, where He-Adj and would have performed better.\relax }}{64}{figure.caption.28}}
\newlabel{figure-init4_ffn_init}{{14}{64}{Dataset: Synthetic10 (\ref {dataset_synthetic10}); Configurations 5 and 6 (\ref {config5}, \ref {config6}) \newline The figure here shows the P\&L performance for He-Adj and He being higher than Xavier, as to be expected with more balanced network layers and ReLU activations. The better performance in the lower end of the He-Adj and configurations when compared to He can be attributed to the inclusion of inconsistently sized layers in the configuration set, where He-Adj and would have performed better.\relax }{figure.caption.28}{}}
\newlabel{figure-init4_ffn_init@cref}{{[figure][14][]14}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Dataset: AGL (\ref  {dataset_agl}); Configuration 4 (\ref  {config4}) \newline  The configurations run for AGL show notably better performance using the He-Adj and and He based initialization in comparison Xavier when using actual data, as per the differences explained above.\relax }}{65}{figure.caption.29}}
\newlabel{figure-init5_ffn_init}{{15}{65}{Dataset: AGL (\ref {dataset_agl}); Configuration 4 (\ref {config4}) \newline The configurations run for AGL show notably better performance using the He-Adj and and He based initialization in comparison Xavier when using actual data, as per the differences explained above.\relax }{figure.caption.29}{}}
\newlabel{figure-init5_ffn_init@cref}{{[figure][15][]15}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Feature Selection}{66}{subsection.7.3}}
\newlabel{results_reduction}{{7.3}{66}{Feature Selection}{subsection.7.3}{}}
\newlabel{results_reduction@cref}{{[subsection][3][7]7.3}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Dataset: ; Configuration \newline  Actual P\&L grouped according to feature selection size (where 0 is no encoding), showing increased performance the closer feature selection is to the number of assets (in this case, 10).\relax }}{66}{figure.caption.30}}
\newlabel{figure-results_encoding_actual}{{16}{66}{Dataset: ; Configuration \newline Actual P\&L grouped according to feature selection size (where 0 is no encoding), showing increased performance the closer feature selection is to the number of assets (in this case, 10).\relax }{figure.caption.30}{}}
\newlabel{figure-results_encoding_actual@cref}{{[figure][16][]16}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}) ; Configurations 5 \& 6 (\ref  {config5}, \ref  {config6}) \newline  Synthetic P\&L grouped according to feature selection size.\relax }}{67}{figure.caption.31}}
\newlabel{figure-results_encoding_synthetic}{{17}{67}{Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations 5 \& 6 (\ref {config5}, \ref {config6}) \newline Synthetic P\&L grouped according to feature selection size.\relax }{figure.caption.31}{}}
\newlabel{figure-results_encoding_synthetic@cref}{{[figure][17][]17}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Network Structure and Training}{69}{subsection.7.4}}
\newlabel{results_network}{{7.4}{69}{Network Structure and Training}{subsection.7.4}{}}
\newlabel{results_network@cref}{{[subsection][4][7]7.4}{69}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Effects of Network Size}{69}{subsubsection.7.4.1}}
\newlabel{figure-actual_mse_lines}{{18a}{69}{\textbf {SAE MSE Scores} \newline \relax }{figure.caption.32}{}}
\newlabel{figure-actual_mse_lines@cref}{{[subfigure][1][18]18a}{69}}
\newlabel{sub@figure-actual_mse_lines}{{a}{69}{\textbf {SAE MSE Scores} \newline \relax }{figure.caption.32}{}}
\newlabel{sub@figure-actual_mse_lines@cref}{{[subfigure][1][18]18a}{69}}
\newlabel{figure-actual_pl_lines}{{18b}{69}{\textbf {Predictive P\&L Scores} \newline \relax }{figure.caption.32}{}}
\newlabel{figure-actual_pl_lines@cref}{{[subfigure][2][18]18b}{69}}
\newlabel{sub@figure-actual_pl_lines}{{b}{69}{\textbf {Predictive P\&L Scores} \newline \relax }{figure.caption.32}{}}
\newlabel{sub@figure-actual_pl_lines@cref}{{[subfigure][2][18]18b}{69}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Dataset: Actual10 dataset (\ref  {dataset_actual10}), Configuration \newline  Figure (a) shows the median minimum MSE by SAE networks achieved with the indicated number of layers and layer nodes. It's worth noting that the number of layers here indicate the final SAE structure of $N$ hidden layers, rather than the training structure of $2N + 1$ hidden layers. Interestingly, the networks with 120 node layers seem to struggled with training, performance decreases as layers increased. This is to be expected if the learning parameters (e.g. SGD learning rate) are not ideal for the structure, the effects of which are amplified as network sizes increase. This is resolved in the larger 240 node networks which show significant improvement as more layers are added (as one would generally expect, subject to effective SGD). \newline  Figure (b) shows the median OOS P\&L achieved by predictive networks with the indicated number of layers and layer nodes. The behaviour is as expected here, where network performance generally increases with size. The exception is the four layer network of 60 nodes, where the training parameters were no longer effective in passing signal back (larger layer sizes are less likely to suffer from this, particuarly with ReLU activations). \relax }}{69}{figure.caption.32}}
\newlabel{figure-network_size}{{18}{69}{Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration \newline Figure (a) shows the median minimum MSE by SAE networks achieved with the indicated number of layers and layer nodes. It's worth noting that the number of layers here indicate the final SAE structure of $N$ hidden layers, rather than the training structure of $2N + 1$ hidden layers. Interestingly, the networks with 120 node layers seem to struggled with training, performance decreases as layers increased. This is to be expected if the learning parameters (e.g. SGD learning rate) are not ideal for the structure, the effects of which are amplified as network sizes increase. This is resolved in the larger 240 node networks which show significant improvement as more layers are added (as one would generally expect, subject to effective SGD). \newline Figure (b) shows the median OOS P\&L achieved by predictive networks with the indicated number of layers and layer nodes. The behaviour is as expected here, where network performance generally increases with size. The exception is the four layer network of 60 nodes, where the training parameters were no longer effective in passing signal back (larger layer sizes are less likely to suffer from this, particuarly with ReLU activations). \relax }{figure.caption.32}{}}
\newlabel{figure-network_size@cref}{{[figure][18][]18}{69}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {7.4.1.1}SAE Network Structures}{69}{paragraph.7.4.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Effects of Learning Rates and Schedules}{70}{subsubsection.7.4.2}}
\newlabel{results_lr}{{7.4.2}{70}{Effects of Learning Rates and Schedules}{subsubsection.7.4.2}{}}
\newlabel{results_lr@cref}{{[subsubsection][2][7,4]7.4.2}{70}}
\newlabel{figure-synth_mse_minmax_lr}{{19a}{70}{\textbf {Synthetic Data} \newline \relax }{figure.caption.33}{}}
\newlabel{figure-synth_mse_minmax_lr@cref}{{[subfigure][1][19]19a}{70}}
\newlabel{sub@figure-synth_mse_minmax_lr}{{a}{70}{\textbf {Synthetic Data} \newline \relax }{figure.caption.33}{}}
\newlabel{sub@figure-synth_mse_minmax_lr@cref}{{[subfigure][1][19]19a}{70}}
\newlabel{figure-actual_mse_minmax_lr}{{19b}{70}{\textbf {Actual Data} \newline \relax }{figure.caption.33}{}}
\newlabel{figure-actual_mse_minmax_lr@cref}{{[subfigure][2][19]19b}{70}}
\newlabel{sub@figure-actual_mse_minmax_lr}{{b}{70}{\textbf {Actual Data} \newline \relax }{figure.caption.33}{}}
\newlabel{sub@figure-actual_mse_minmax_lr@cref}{{[subfigure][2][19]19b}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Dataset: Synthetic10 dataset (\ref  {dataset_synthetic10}), Actual10 dataset (\ref  {dataset_actual10}), Configuration \newline  It is interesting to note that the effects here are quite different for Synthetic and Actual data, with networks for Synthetic data favouring learning rate ranges with a higher upper bound, and Actual data networks trending in favour of lower upper bound ranges. The synthetic data is structurally less complex, as discussed further in \ref  {results_gbm_data} and \ref  {results_data_hist}, and so the larger learning rates will allow for quicker optimisation. Conversely, SAE networks for actual data benefit from a more careful exploration of the solution space.\relax }}{70}{figure.caption.33}}
\newlabel{figure-mse_lr}{{19}{70}{Dataset: Synthetic10 dataset (\ref {dataset_synthetic10}), Actual10 dataset (\ref {dataset_actual10}), Configuration \newline It is interesting to note that the effects here are quite different for Synthetic and Actual data, with networks for Synthetic data favouring learning rate ranges with a higher upper bound, and Actual data networks trending in favour of lower upper bound ranges. The synthetic data is structurally less complex, as discussed further in \ref {results_gbm_data} and \ref {results_data_hist}, and so the larger learning rates will allow for quicker optimisation. Conversely, SAE networks for actual data benefit from a more careful exploration of the solution space.\relax }{figure.caption.33}{}}
\newlabel{figure-mse_lr@cref}{{[figure][19][]19}{70}}
\newlabel{figure-synth_pl_minmax_lr}{{20a}{71}{\textbf {Synthetic Data} \newline \relax }{figure.caption.34}{}}
\newlabel{figure-synth_pl_minmax_lr@cref}{{[subfigure][1][20]20a}{71}}
\newlabel{sub@figure-synth_pl_minmax_lr}{{a}{71}{\textbf {Synthetic Data} \newline \relax }{figure.caption.34}{}}
\newlabel{sub@figure-synth_pl_minmax_lr@cref}{{[subfigure][1][20]20a}{71}}
\newlabel{figure-actual_pl_minmax_lr}{{20b}{71}{\textbf {Actual Data} \newline \relax }{figure.caption.34}{}}
\newlabel{figure-actual_pl_minmax_lr@cref}{{[subfigure][2][20]20b}{71}}
\newlabel{sub@figure-actual_pl_minmax_lr}{{b}{71}{\textbf {Actual Data} \newline \relax }{figure.caption.34}{}}
\newlabel{sub@figure-actual_pl_minmax_lr@cref}{{[subfigure][2][20]20b}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Dataset: Synthetic10 dataset (\ref  {dataset_synthetic10}), Actual10 dataset (\ref  {dataset_actual10}), Configuration \newline  The learning rates for Actual data showin in Figure (b) here were chosen accordingly to the best rates seen in the SAE training. There's no substantial difference shown by changing the lower bound by a factor. The results for Synthetic data were interesting in that the networks had higher P\&L when trainined with ssmaller learning rates, suggesting that the predictive task still warrants a more careful solution space exploration despite the less complex nature of the data.\relax }}{71}{figure.caption.34}}
\newlabel{figure-pl_lr}{{20}{71}{Dataset: Synthetic10 dataset (\ref {dataset_synthetic10}), Actual10 dataset (\ref {dataset_actual10}), Configuration \newline The learning rates for Actual data showin in Figure (b) here were chosen accordingly to the best rates seen in the SAE training. There's no substantial difference shown by changing the lower bound by a factor. The results for Synthetic data were interesting in that the networks had higher P\&L when trainined with ssmaller learning rates, suggesting that the predictive task still warrants a more careful solution space exploration despite the less complex nature of the data.\relax }{figure.caption.34}{}}
\newlabel{figure-pl_lr@cref}{{[figure][20][]20}{71}}
\newlabel{figure-actual_mse_lr_epochs}{{21a}{71}{\textbf {MSE for SAE Networks} \newline \relax }{figure.caption.35}{}}
\newlabel{figure-actual_mse_lr_epochs@cref}{{[subfigure][1][21]21a}{71}}
\newlabel{sub@figure-actual_mse_lr_epochs}{{a}{71}{\textbf {MSE for SAE Networks} \newline \relax }{figure.caption.35}{}}
\newlabel{sub@figure-actual_mse_lr_epochs@cref}{{[subfigure][1][21]21a}{71}}
\newlabel{figure-actual_pl_lr_epochs}{{21b}{71}{\textbf {P\&L for Predictive Networks} \newline \relax }{figure.caption.35}{}}
\newlabel{figure-actual_pl_lr_epochs@cref}{{[subfigure][2][21]21b}{71}}
\newlabel{sub@figure-actual_pl_lr_epochs}{{b}{71}{\textbf {P\&L for Predictive Networks} \newline \relax }{figure.caption.35}{}}
\newlabel{sub@figure-actual_pl_lr_epochs@cref}{{[subfigure][2][21]21b}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Dataset: Actual10 dataset (\ref  {dataset_actual10}), Configurations \newline  Figure(a) shows the MSE scores for SAE networks, with the shorter epoch cycle offering the best (and the worst) results. Synthetic data for the SAE MSE scores behaved similarly, as seen in the appendix (section \ref  {results_network_appendix}). \newline  In Figure (b), for the predictive networks, P\&L was highest in the 100 epoch cycle as well, where -1 indicates a constant learning rate rather than the cyclical pattern, and 10 epoch learning rates were used for configurations which only ran for 10 epochs. The configurations without learning epochs (-1) were in the second round of testing and had a more effective set of parameters elsewhere, hence the large difference in the lower bounds relative to the other configurations. Ultimately, the implementation of the learning rate cycle has offered a small but real improvement relative to the constant learning rate for P\&L.\relax }}{71}{figure.caption.35}}
\newlabel{figure-epochs_lr}{{21}{71}{Dataset: Actual10 dataset (\ref {dataset_actual10}), Configurations \newline Figure(a) shows the MSE scores for SAE networks, with the shorter epoch cycle offering the best (and the worst) results. Synthetic data for the SAE MSE scores behaved similarly, as seen in the appendix (section \ref {results_network_appendix}). \newline In Figure (b), for the predictive networks, P\&L was highest in the 100 epoch cycle as well, where -1 indicates a constant learning rate rather than the cyclical pattern, and 10 epoch learning rates were used for configurations which only ran for 10 epochs. The configurations without learning epochs (-1) were in the second round of testing and had a more effective set of parameters elsewhere, hence the large difference in the lower bounds relative to the other configurations. Ultimately, the implementation of the learning rate cycle has offered a small but real improvement relative to the constant learning rate for P\&L.\relax }{figure.caption.35}{}}
\newlabel{figure-epochs_lr@cref}{{[figure][21][]21}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Dataset: Actual10 dataset (\ref  {dataset_actual10}), Configurations \newline  The P\&L performance for OGD learning rates is expectedly non-linear, with performance steadily increasing up until 0.05 where the network is adapting at the correct rate, and degrading thereafter as the network will overcorrect to noise and short term trends picked up in the online learning period. Networks trained on synthetic data showed similar trends (though were not trained past a turning point), and can be seen in the Appendix in Section \ref  {results_network_appendix}.\relax }}{72}{figure.caption.36}}
\newlabel{figure-actual_ogd_lr}{{22}{72}{Dataset: Actual10 dataset (\ref {dataset_actual10}), Configurations \newline The P\&L performance for OGD learning rates is expectedly non-linear, with performance steadily increasing up until 0.05 where the network is adapting at the correct rate, and degrading thereafter as the network will overcorrect to noise and short term trends picked up in the online learning period. Networks trained on synthetic data showed similar trends (though were not trained past a turning point), and can be seen in the Appendix in Section \ref {results_network_appendix}.\relax }{figure.caption.36}{}}
\newlabel{figure-actual_ogd_lr@cref}{{[figure][22][]22}{72}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Effects of Regularization}{72}{subsubsection.7.4.3}}
\newlabel{figure-actual_mse_reg}{{23a}{72}{\textbf {MSE for SAE Networks} \newline \relax }{figure.caption.37}{}}
\newlabel{figure-actual_mse_reg@cref}{{[subfigure][1][23]23a}{72}}
\newlabel{sub@figure-actual_mse_reg}{{a}{72}{\textbf {MSE for SAE Networks} \newline \relax }{figure.caption.37}{}}
\newlabel{sub@figure-actual_mse_reg@cref}{{[subfigure][1][23]23a}{72}}
\newlabel{figure-actual_pl_reg}{{23b}{72}{\textbf {P\&L for Predictive Networks} \newline \relax }{figure.caption.37}{}}
\newlabel{figure-actual_pl_reg@cref}{{[subfigure][2][23]23b}{72}}
\newlabel{sub@figure-actual_pl_reg}{{b}{72}{\textbf {P\&L for Predictive Networks} \newline \relax }{figure.caption.37}{}}
\newlabel{sub@figure-actual_pl_reg@cref}{{[subfigure][2][23]23b}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Dataset: Actual10 dataset (\ref  {dataset_actual10}), Configurations \newline  \relax }}{72}{figure.caption.37}}
\newlabel{figure-results-reg}{{23}{72}{Dataset: Actual10 dataset (\ref {dataset_actual10}), Configurations \newline \relax }{figure.caption.37}{}}
\newlabel{figure-results-reg@cref}{{[figure][23][]23}{72}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.4}Effects of Denoising}{73}{subsubsection.7.4.4}}
\newlabel{figure-actual_mse_gaussian}{{24a}{73}{\textbf {Gaussian Denoising} \newline \relax }{figure.caption.38}{}}
\newlabel{figure-actual_mse_gaussian@cref}{{[subfigure][1][24]24a}{73}}
\newlabel{sub@figure-actual_mse_gaussian}{{a}{73}{\textbf {Gaussian Denoising} \newline \relax }{figure.caption.38}{}}
\newlabel{sub@figure-actual_mse_gaussian@cref}{{[subfigure][1][24]24a}{73}}
\newlabel{figure-actual_mse_masking}{{24b}{73}{\textbf {Masking Denoising} \newline \relax }{figure.caption.38}{}}
\newlabel{figure-actual_mse_masking@cref}{{[subfigure][2][24]24b}{73}}
\newlabel{sub@figure-actual_mse_masking}{{b}{73}{\textbf {Masking Denoising} \newline \relax }{figure.caption.38}{}}
\newlabel{sub@figure-actual_mse_masking@cref}{{[subfigure][2][24]24b}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Dataset: , Configurations \newline  \relax }}{73}{figure.caption.38}}
\newlabel{figure-results_mse_denoising}{{24}{73}{Dataset: , Configurations \newline \relax }{figure.caption.38}{}}
\newlabel{figure-results_mse_denoising@cref}{{[figure][24][]24}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Dataset: Actual10 dataset (\ref  {dataset_actual10}), Configurations \newline  \relax }}{73}{figure.caption.39}}
\newlabel{figure-actual_pl_masking}{{25}{73}{Dataset: Actual10 dataset (\ref {dataset_actual10}), Configurations \newline \relax }{figure.caption.39}{}}
\newlabel{figure-actual_pl_masking@cref}{{[figure][25][]25}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}The Effects of Data Aggregation and Value of Historical Data}{74}{subsection.7.5}}
\newlabel{results_hist}{{7.5}{74}{The Effects of Data Aggregation and Value of Historical Data}{subsection.7.5}{}}
\newlabel{results_hist@cref}{{[subsection][5][7]7.5}{74}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}Data Aggregation and SAE MSE Scores}{74}{subsubsection.7.5.1}}
\newlabel{results_data_mse}{{7.5.1}{74}{Data Aggregation and SAE MSE Scores}{subsubsection.7.5.1}{}}
\newlabel{results_data_mse@cref}{{[subsubsection][1][7,5]7.5.1}{74}}
\newlabel{figure-test_aggregate_dist}{{26a}{74}{\textbf {Log Difference Distributions} \newline \relax }{figure.caption.40}{}}
\newlabel{figure-test_aggregate_dist@cref}{{[subfigure][1][26]26a}{74}}
\newlabel{sub@figure-test_aggregate_dist}{{a}{74}{\textbf {Log Difference Distributions} \newline \relax }{figure.caption.40}{}}
\newlabel{sub@figure-test_aggregate_dist@cref}{{[subfigure][1][26]26a}{74}}
\newlabel{figure-test_aggregation_mse}{{26b}{74}{\textbf {SAE MSE Scores} \newline \relax }{figure.caption.40}{}}
\newlabel{figure-test_aggregation_mse@cref}{{[subfigure][2][26]26b}{74}}
\newlabel{sub@figure-test_aggregation_mse}{{b}{74}{\textbf {SAE MSE Scores} \newline \relax }{figure.caption.40}{}}
\newlabel{sub@figure-test_aggregation_mse@cref}{{[subfigure][2][26]26b}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Dataset: Synthetic10 dataset (\ref  {dataset_synthetic10}), Configuration \newline  Figure (a) shows the distribution of values to be replicated by the SAE for synthetic data. Geometric Brownian Motion generates discrete changes that follow a log-normal distribution, the log difference and scaling of which (as per the data processing described in \ref  {data_processing}) result in normally distributed price changes. We see small differences in the distributions according to the data aggregation used, as one would expect, with slightly lower variances occuring for smaller data windows ($\sigma _{[1,5,20]} = 0.145$, $\sigma _{[5,20,60]} = 0.152$, $\sigma _{[10,20,60]} = 0.154$). There were some differences in SAE performance as indicated in Figure (b), though not to large extents and considering the very similar distribution of values, there isn't any expectation of seeing fundamental differences in the networks ability to compress and replicate them. \relax }}{74}{figure.caption.40}}
\newlabel{figure-data_sae_synthetic}{{26}{74}{Dataset: Synthetic10 dataset (\ref {dataset_synthetic10}), Configuration \newline Figure (a) shows the distribution of values to be replicated by the SAE for synthetic data. Geometric Brownian Motion generates discrete changes that follow a log-normal distribution, the log difference and scaling of which (as per the data processing described in \ref {data_processing}) result in normally distributed price changes. We see small differences in the distributions according to the data aggregation used, as one would expect, with slightly lower variances occuring for smaller data windows ($\sigma _{[1,5,20]} = 0.145$, $\sigma _{[5,20,60]} = 0.152$, $\sigma _{[10,20,60]} = 0.154$). There were some differences in SAE performance as indicated in Figure (b), though not to large extents and considering the very similar distribution of values, there isn't any expectation of seeing fundamental differences in the networks ability to compress and replicate them. \relax }{figure.caption.40}{}}
\newlabel{figure-data_sae_synthetic@cref}{{[figure][26][]26}{74}}
\newlabel{figure-actual_aggregate_dist}{{27a}{75}{\textbf {Log Difference Distribution} \newline \relax }{figure.caption.41}{}}
\newlabel{figure-actual_aggregate_dist@cref}{{[subfigure][1][27]27a}{75}}
\newlabel{sub@figure-actual_aggregate_dist}{{a}{75}{\textbf {Log Difference Distribution} \newline \relax }{figure.caption.41}{}}
\newlabel{sub@figure-actual_aggregate_dist@cref}{{[subfigure][1][27]27a}{75}}
\newlabel{figure-actual_aggregation_mse}{{27b}{75}{\textbf {SAE MSE Scores} \newline \relax }{figure.caption.41}{}}
\newlabel{figure-actual_aggregation_mse@cref}{{[subfigure][2][27]27b}{75}}
\newlabel{sub@figure-actual_aggregation_mse}{{b}{75}{\textbf {SAE MSE Scores} \newline \relax }{figure.caption.41}{}}
\newlabel{sub@figure-actual_aggregation_mse@cref}{{[subfigure][2][27]27b}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Dataset: Actual10 dataset (\ref  {dataset_actual10}), Configuration \newline  Figure (a) shows the distribution of values to be replicated by the SAE for actual data. The distributions are noticeably different from synthetic data, as there is no prior on the price change values, and so variances differ far more across the configurations ($\sigma _{[1,5,20]} = 0.118$, $\sigma _{[5,20,60]} = 0.146$, $\sigma _{[10,20,60]} = 0.150$). Smaller data windows are less likely to capture larger fundamental price changes, resulting in the lower variances. This in turn, makes for easier replication for the SAE networks due to less variety in the sample set, which is seen in Figure (b) with the noticeably better performance in the [1, 5, 20] configuration. \relax }}{75}{figure.caption.41}}
\newlabel{fig:data_sae_actual}{{27}{75}{Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration \newline Figure (a) shows the distribution of values to be replicated by the SAE for actual data. The distributions are noticeably different from synthetic data, as there is no prior on the price change values, and so variances differ far more across the configurations ($\sigma _{[1,5,20]} = 0.118$, $\sigma _{[5,20,60]} = 0.146$, $\sigma _{[10,20,60]} = 0.150$). Smaller data windows are less likely to capture larger fundamental price changes, resulting in the lower variances. This in turn, makes for easier replication for the SAE networks due to less variety in the sample set, which is seen in Figure (b) with the noticeably better performance in the [1, 5, 20] configuration. \relax }{figure.caption.41}{}}
\newlabel{fig:data_sae_actual@cref}{{[figure][27][]27}{75}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}Data Aggregation and Predictive P\&L Scores}{75}{subsubsection.7.5.2}}
\newlabel{results_data_pl}{{7.5.2}{75}{Data Aggregation and Predictive P\&L Scores}{subsubsection.7.5.2}{}}
\newlabel{results_data_pl@cref}{{[subsubsection][2][7,5]7.5.2}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces  Dataset: Synthetic10 dataset (\ref  {dataset_synthetic10}), Configuration \newline  The boxplots here show the P\&L from the MMS according group by the data window configurations. There's a clear trend of P\&L increasing as the length of the windows increase. Shorter term GBM data would be more likely to represent fluctuations, whereas the longer term windows will be more representative of the constant mean in the stocks, leading to easier predictive performance and higher P\&L.\relax }}{75}{figure.caption.42}}
\newlabel{figure-test_aggregation_pl}{{28}{75}{Dataset: Synthetic10 dataset (\ref {dataset_synthetic10}), Configuration \newline The boxplots here show the P\&L from the MMS according group by the data window configurations. There's a clear trend of P\&L increasing as the length of the windows increase. Shorter term GBM data would be more likely to represent fluctuations, whereas the longer term windows will be more representative of the constant mean in the stocks, leading to easier predictive performance and higher P\&L.\relax }{figure.caption.42}{}}
\newlabel{figure-test_aggregation_pl@cref}{{[figure][28][]28}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces  Dataset: Actual10 dataset (\ref  {dataset_actual10}), Configuration \newline  The P\&L by data window configurations for actual data show a more complex view. At a general level, predictive performance is increasing as the span of the windows increase. However, the highest P\&L occurs for the shortest window configuration ([1, 5, 20]). The suggestion here is that both long term trends and short term fluctuations are useful for making short term predictions in price changes, but that a middle of the road configuration offers the worst of both dynamics.\relax }}{76}{figure.caption.43}}
\newlabel{figure-actual_aggregation_pl}{{29}{76}{Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration \newline The P\&L by data window configurations for actual data show a more complex view. At a general level, predictive performance is increasing as the span of the windows increase. However, the highest P\&L occurs for the shortest window configuration ([1, 5, 20]). The suggestion here is that both long term trends and short term fluctuations are useful for making short term predictions in price changes, but that a middle of the road configuration offers the worst of both dynamics.\relax }{figure.caption.43}{}}
\newlabel{figure-actual_aggregation_pl@cref}{{[figure][29][]29}{76}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.3}Effects of IS Training and Historical Data}{76}{subsubsection.7.5.3}}
\newlabel{results_data_hist}{{7.5.3}{76}{Effects of IS Training and Historical Data}{subsubsection.7.5.3}{}}
\newlabel{results_data_hist@cref}{{[subsubsection][3][7,5]7.5.3}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces  Dataset: Actual10 dataset (\ref  {dataset_actual10}), Configuration \newline  The box plots above show P\&L grouped by the number of epochs in the SGD IS training phase (i.e. the number of times the IS data was trained on). In this set of configurations, 100 Epochs offers the best overall performance, and further training to 500 or 1000 epochs degrades performance due to the network overfitting on the IS data. The results here are noteworthy as they suggest the benefit of historical data is limited - having a network become better at predicting returns 10 years ago is not leading to increased P\&L for more current data. The small difference between 10 and 100 Epochs further emphasises this point. \relax }}{77}{figure.caption.44}}
\newlabel{figure-results_pl_max_epochs}{{30}{77}{Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration \newline The box plots above show P\&L grouped by the number of epochs in the SGD IS training phase (i.e. the number of times the IS data was trained on). In this set of configurations, 100 Epochs offers the best overall performance, and further training to 500 or 1000 epochs degrades performance due to the network overfitting on the IS data. The results here are noteworthy as they suggest the benefit of historical data is limited - having a network become better at predicting returns 10 years ago is not leading to increased P\&L for more current data. The small difference between 10 and 100 Epochs further emphasises this point. \relax }{figure.caption.44}{}}
\newlabel{figure-results_pl_max_epochs@cref}{{[figure][30][]30}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces  Dataset: Actual10 dataset (\ref  {dataset_actual10}), Configuration (some samples with 0 P\&L were excluded for more effective visualization) \newline  To further explore the effect of IS training on historical data, configurations were run with a percentage of the usual trianing data excluded, with the P\&L results grouped above. It is very interesting to note that the exclusion of up to 80\% of the IS training data resulted in only a 2.2\% drop in median P\&L for those networks. The training in these instances were not adjusted to increase the number of epochs according to the size of IS data, and so the configurations with more data excluded were also in essence trained less. These results, combined with those in Figure \ref  {figure-results_pl_max_epochs} show the limited use in training on historical data, and hence the far greater value in current cross sectional information.\relax }}{77}{figure.caption.45}}
\newlabel{figure-results_it3_validationset}{{31}{77}{Dataset: Actual10 dataset (\ref {dataset_actual10}), Configuration (some samples with 0 P\&L were excluded for more effective visualization) \newline To further explore the effect of IS training on historical data, configurations were run with a percentage of the usual trianing data excluded, with the P\&L results grouped above. It is very interesting to note that the exclusion of up to 80\% of the IS training data resulted in only a 2.2\% drop in median P\&L for those networks. The training in these instances were not adjusted to increase the number of epochs according to the size of IS data, and so the configurations with more data excluded were also in essence trained less. These results, combined with those in Figure \ref {figure-results_pl_max_epochs} show the limited use in training on historical data, and hence the far greater value in current cross sectional information.\relax }{figure.caption.45}{}}
\newlabel{figure-results_it3_validationset@cref}{{[figure][31][]31}{77}}
\citation{BailyPBO}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Probability of Backtest Overfitting}{78}{subsection.7.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}Concerns Regarding the PBO Calculation}{78}{subsubsection.7.6.1}}
\newlabel{results_pboconcerns}{{7.6.1}{78}{Concerns Regarding the PBO Calculation}{subsubsection.7.6.1}{}}
\newlabel{results_pboconcerns@cref}{{[subsubsection][1][7,6]7.6.1}{78}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add ref here to low max epochs results}{78}{section*.46}}
\pgfsyspdfmark {pgfid56}{31191626}{20234057}
\pgfsyspdfmark {pgfid59}{36009438}{20267617}
\pgfsyspdfmark {pgfid60}{37631454}{20022431}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ add ref here}{78}{section*.47}}
\pgfsyspdfmark {pgfid61}{13901806}{10010441}
\pgfsyspdfmark {pgfid64}{36009438}{10044001}
\pgfsyspdfmark {pgfid65}{37631454}{9798815}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces  \newline  The curve here shows the significant impact that the choice in the split parameters has for the PBO score calculated, with a monotonically decreasing score as S increases (these scores were for the same set of results). \relax }}{79}{figure.caption.48}}
\newlabel{figure-PBO_by_Split}{{32}{79}{\newline The curve here shows the significant impact that the choice in the split parameters has for the PBO score calculated, with a monotonically decreasing score as S increases (these scores were for the same set of results). \relax }{figure.caption.48}{}}
\newlabel{figure-PBO_by_Split@cref}{{[figure][32][]32}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces  \newline  The number of combinations that have to be procssed by the CSCV method according to the value of the split parameter ($S$) chosen\relax }}{80}{figure.caption.49}}
\newlabel{figure-s_combinations}{{33}{80}{\newline The number of combinations that have to be procssed by the CSCV method according to the value of the split parameter ($S$) chosen\relax }{figure.caption.49}{}}
\newlabel{figure-s_combinations@cref}{{[figure][33][]33}{80}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.2}PBO Results}{80}{subsubsection.7.6.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ correct vals if needed}{80}{section*.50}}
\pgfsyspdfmark {pgfid66}{8538958}{26069056}
\pgfsyspdfmark {pgfid69}{36009438}{26102616}
\pgfsyspdfmark {pgfid70}{37631454}{25857430}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  The CSCV logit distribution for all configurations run, with a calculated PBO of 1.6\%.\relax }}{80}{figure.caption.51}}
\newlabel{figure-results_logits_all}{{34}{80}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The CSCV logit distribution for all configurations run, with a calculated PBO of 1.6\%.\relax }{figure.caption.51}{}}
\newlabel{figure-results_logits_all@cref}{{[figure][34][]34}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  The CSCV logit distribution for a narrower of configurations run, with a calculated PBO of 6.3\%.\relax }}{81}{figure.caption.52}}
\newlabel{figure-results_logits_subset}{{35}{81}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The CSCV logit distribution for a narrower of configurations run, with a calculated PBO of 6.3\%.\relax }{figure.caption.52}{}}
\newlabel{figure-results_logits_subset@cref}{{[figure][35][]35}{81}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.3}Framework Success}{81}{subsubsection.7.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}MMS Results}{82}{subsection.7.7}}
\newlabel{results_mms}{{7.7}{82}{MMS Results}{subsection.7.7}{}}
\newlabel{results_mms@cref}{{[subsection][7][7]7.7}{82}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.7.1}Summary of Experiment Results}{82}{subsubsection.7.7.1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ }{82}{section*.53}}
\pgfsyspdfmark {pgfid71}{7555917}{42640463}
\pgfsyspdfmark {pgfid74}{36009438}{42674023}
\pgfsyspdfmark {pgfid75}{37631454}{42428837}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations ) \newline  The PDF of all OOS P\&L values, with the benchmark P\&L indicated in orange.\relax }}{82}{figure.caption.54}}
\newlabel{figure-results_pl_pdf}{{36}{82}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations ) \newline The PDF of all OOS P\&L values, with the benchmark P\&L indicated in orange.\relax }{figure.caption.54}{}}
\newlabel{figure-results_pl_pdf@cref}{{[figure][36][]36}{82}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.7.2}Best Network Results}{82}{subsubsection.7.7.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Best Network vesus Benchmark\relax }}{82}{table.caption.55}}
\newlabel{tab_bestvsbenchmark}{{4}{82}{Best Network vesus Benchmark\relax }{table.caption.55}{}}
\newlabel{tab_bestvsbenchmark@cref}{{[table][4][]4}{82}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Best Network vesus Benchmark\relax }}{83}{table.caption.56}}
\newlabel{tab_best_confusion}{{5}{83}{Best Network vesus Benchmark\relax }{table.caption.56}{}}
\newlabel{tab_best_confusion@cref}{{[table][5][]5}{83}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8}Results Summary}{83}{subsection.7.8}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusions}{84}{section.8}}
\newlabel{Conclusion}{{8}{84}{Conclusions}{section.8}{}}
\newlabel{Conclusion@cref}{{[section][8][]8}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Appendix}{85}{section.9}}
\newlabel{Appendix}{{9}{85}{Appendix}{section.9}{}}
\newlabel{Appendix@cref}{{[section][9][]9}{85}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Additional Results}{85}{subsection.9.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}Additional Results for Section \ref  {results_activations_scaling} - Activation Functions and Scaling }{85}{subsubsection.9.1.1}}
\newlabel{results_activations_appendix}{{9.1.1}{85}{Additional Results for Section \ref {results_activations_scaling} - Activation Functions and Scaling }{subsubsection.9.1.1}{}}
\newlabel{results_activations_appendix@cref}{{[subsubsection][1][9,1]9.1.1}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Dataset Synthetic6 (\ref  {dataset_synthetic6}) ; Configuration \newline  The plot above shows the SAE MSE, grouped by encoding size and activations, showing a marginal increase in performance for Leaky ReLU activations. \relax }}{85}{figure.caption.57}}
\newlabel{figure-synthetic_mse_leakyrelu}{{37}{85}{Dataset Synthetic6 (\ref {dataset_synthetic6}) ; Configuration \newline The plot above shows the SAE MSE, grouped by encoding size and activations, showing a marginal increase in performance for Leaky ReLU activations. \relax }{figure.caption.57}{}}
\newlabel{figure-synthetic_mse_leakyrelu@cref}{{[figure][37][]37}{85}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2}Additional Results for Section \ref  {results_init} - Weight Initialization Techniques }{85}{subsubsection.9.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces The plot above shows the prediction accuracy achieved on the MNIST dataset according to the number of pre-training epochs using the RBM greedy layerwise methodology as per section \ref  {imp_rbm}. There's a clear indication that pre-training allows the network to achieve much higher accuracy much quicker.\relax }}{85}{figure.caption.58}}
\newlabel{figure-rbm_pretraining}{{38}{85}{The plot above shows the prediction accuracy achieved on the MNIST dataset according to the number of pre-training epochs using the RBM greedy layerwise methodology as per section \ref {imp_rbm}. There's a clear indication that pre-training allows the network to achieve much higher accuracy much quicker.\relax }{figure.caption.58}{}}
\newlabel{figure-rbm_pretraining@cref}{{[figure][38][]38}{85}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.3}Additional Results for Section \ref  {results_reduction} - Feature Selection }{86}{subsubsection.9.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Dataset: AGL (\ref  {dataset_agl}) ; Configurations 3 \& 4 (\ref  {config3}, \ref  {config4}) \newline  This figure shows the effects of the difference encoding layer sizes on the P\&L when the relevant SAE is used for the predictive FFN, as for the AGL dataset (with an input of 3 features). Performance for both sizes are similar, though increasing the number of assets may result in a different effect being seen.\relax }}{86}{figure.caption.59}}
\newlabel{figure-results_encoding_agl}{{39}{86}{Dataset: AGL (\ref {dataset_agl}) ; Configurations 3 \& 4 (\ref {config3}, \ref {config4}) \newline This figure shows the effects of the difference encoding layer sizes on the P\&L when the relevant SAE is used for the predictive FFN, as for the AGL dataset (with an input of 3 features). Performance for both sizes are similar, though increasing the number of assets may result in a different effect being seen.\relax }{figure.caption.59}{}}
\newlabel{figure-results_encoding_agl@cref}{{[figure][39][]39}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.4}Additional Results for Section \ref  {results_network} - Network Structure and Training }{86}{subsubsection.9.1.4}}
\newlabel{results_network_appendix}{{9.1.4}{86}{Additional Results for Section \ref {results_network} - Network Structure and Training }{subsubsection.9.1.4}{}}
\newlabel{results_network_appendix@cref}{{[subsubsection][4][9,1]9.1.4}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations \newline  This figure shows the MSE achieved by SAE networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. The network sizes here indicate the final $N$ layers for the SAE, rather than the $2N + 1$ that are present in training. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training. As noted in \ref  {results_network}, the more typical structure of SAEs with descending layer sizes did not show better performance.\relax }}{86}{figure.caption.60}}
\newlabel{figure-actual_sae_mse_box}{{40}{86}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations \newline This figure shows the MSE achieved by SAE networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. The network sizes here indicate the final $N$ layers for the SAE, rather than the $2N + 1$ that are present in training. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training. As noted in \ref {results_network}, the more typical structure of SAEs with descending layer sizes did not show better performance.\relax }{figure.caption.60}{}}
\newlabel{figure-actual_sae_mse_box@cref}{{[figure][40][]40}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}) ; Configurations \newline  This figure shows the MSE achieved by networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. The network sizes here indicate the final $N$ layers for the SAE, rather than the $2N + 1$ that are present in training. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training. As noted in \ref  {results_network}, the more typical structure of SAEs with descending layer sizes did not show better performance.\relax }}{87}{figure.caption.61}}
\newlabel{figure-synth_mse_box}{{41}{87}{Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline This figure shows the MSE achieved by networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. The network sizes here indicate the final $N$ layers for the SAE, rather than the $2N + 1$ that are present in training. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training. As noted in \ref {results_network}, the more typical structure of SAEs with descending layer sizes did not show better performance.\relax }{figure.caption.61}{}}
\newlabel{figure-synth_mse_box@cref}{{[figure][41][]41}{87}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces Dataset: Actual10 (\ref  {dataset_actual10}) ; Configurations \newline  This figure shows the P\&L achieved by networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. Networks that suffered from exploding ReLU's (and hence have approximately 0 P\&L) have been excluded for the more effective visualization. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training.\relax }}{87}{figure.caption.62}}
\newlabel{figure-results_actual_pl_box}{{42}{87}{Dataset: Actual10 (\ref {dataset_actual10}) ; Configurations \newline This figure shows the P\&L achieved by networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. Networks that suffered from exploding ReLU's (and hence have approximately 0 P\&L) have been excluded for the more effective visualization. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training.\relax }{figure.caption.62}{}}
\newlabel{figure-results_actual_pl_box@cref}{{[figure][42][]42}{87}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}) ; Configurations \newline  This figure shows the P\&L achieved by networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. Networks that suffered from exploding ReLU's (and hence have approximately 0 P\&L) have been excluded for the more effective visualization. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training.\relax }}{88}{figure.caption.63}}
\newlabel{figure-results_synth_pl_box}{{43}{88}{Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline This figure shows the P\&L achieved by networks with the indicated sizes, showing a general increase in performance as both layers and layer sizes increase. Networks that suffered from exploding ReLU's (and hence have approximately 0 P\&L) have been excluded for the more effective visualization. Training parameters such as number of SGD epochs were not adjusted for network size, and so some smaller networks achieved higher performance as they had more accomodating training. The general trends should be focused on as the indicator for more tailored training.\relax }{figure.caption.63}{}}
\newlabel{figure-results_synth_pl_box@cref}{{[figure][43][]43}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}) ; Configurations \newline  The SAE networks didn't show significant differences according to the epoch cycles. As discussed in sections \ref  {results_gbm_data} and \ref  {results_data_hist}, the structure of synthetic data for SAE replication is structurally less complex, and so learning optimizations are less likely to produce large differences.\relax }}{88}{figure.caption.64}}
\newlabel{figure-synth_mse_lr_epochs}{{44}{88}{Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline The SAE networks didn't show significant differences according to the epoch cycles. As discussed in sections \ref {results_gbm_data} and \ref {results_data_hist}, the structure of synthetic data for SAE replication is structurally less complex, and so learning optimizations are less likely to produce large differences.\relax }{figure.caption.64}{}}
\newlabel{figure-synth_mse_lr_epochs@cref}{{[figure][44][]44}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}) ; Configurations \newline  The networks trained on Synthetic data showed a sharp P\&L increase as the OGD learning rate increased. It wasn't tested if they would experience the same turning point and begin to degrade as seen in networks for Actual data in \ref  {results_lr}\relax }}{89}{figure.caption.65}}
\newlabel{figure-synth_ogd_lr}{{45}{89}{Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline The networks trained on Synthetic data showed a sharp P\&L increase as the OGD learning rate increased. It wasn't tested if they would experience the same turning point and begin to degrade as seen in networks for Actual data in \ref {results_lr}\relax }{figure.caption.65}{}}
\newlabel{figure-synth_ogd_lr@cref}{{[figure][45][]45}{89}}
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces Dataset: Synthetic10 (\ref  {dataset_synthetic10}) ; Configurations \newline  \relax }}{89}{figure.caption.66}}
\newlabel{figure-synth_pl_reg}{{46}{89}{Dataset: Synthetic10 (\ref {dataset_synthetic10}) ; Configurations \newline \relax }{figure.caption.66}{}}
\newlabel{figure-synth_pl_reg@cref}{{[figure][46][]46}{89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Configuration Sets Used}{90}{subsection.9.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Configuration1 - SAE}{90}{subsubsection.9.2.1}}
\newlabel{config1}{{9.2.1}{90}{Configuration1 - SAE}{subsubsection.9.2.1}{}}
\newlabel{config1@cref}{{[subsubsection][1][9,2]9.2.1}{90}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Config2 - Predictive FFN}{90}{subsubsection.9.2.2}}
\newlabel{config2}{{9.2.2}{90}{Config2 - Predictive FFN}{subsubsection.9.2.2}{}}
\newlabel{config2@cref}{{[subsubsection][2][9,2]9.2.2}{90}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.3}Configuration3 - SAE}{90}{subsubsection.9.2.3}}
\newlabel{config3}{{9.2.3}{90}{Configuration3 - SAE}{subsubsection.9.2.3}{}}
\newlabel{config3@cref}{{[subsubsection][3][9,2]9.2.3}{90}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.4}Configuration4 - Predictive FFN}{91}{subsubsection.9.2.4}}
\newlabel{config4}{{9.2.4}{91}{Configuration4 - Predictive FFN}{subsubsection.9.2.4}{}}
\newlabel{config4@cref}{{[subsubsection][4][9,2]9.2.4}{91}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.5}Configuration5 - SAE}{91}{subsubsection.9.2.5}}
\newlabel{config5}{{9.2.5}{91}{Configuration5 - SAE}{subsubsection.9.2.5}{}}
\newlabel{config5@cref}{{[subsubsection][5][9,2]9.2.5}{91}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.6}Configuration6 - Predictive FFN}{91}{subsubsection.9.2.6}}
\newlabel{config6}{{9.2.6}{91}{Configuration6 - Predictive FFN}{subsubsection.9.2.6}{}}
\newlabel{config6@cref}{{[subsubsection][6][9,2]9.2.6}{91}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.7}Configuration7 - SAE}{92}{subsubsection.9.2.7}}
\newlabel{config7}{{9.2.7}{92}{Configuration7 - SAE}{subsubsection.9.2.7}{}}
\newlabel{config7@cref}{{[subsubsection][7][9,2]9.2.7}{92}}
\bibcite{Albers}{1}
\bibcite{Aparicio}{2}
\bibcite{Arthur}{3}
\bibcite{BailyPBO}{4}
\bibcite{BaileyBTL}{5}
\bibcite{BaileySharpe}{6}
\bibcite{Bakiri}{7}
\bibcite{Bao}{8}
\bibcite{Bartlett}{9}
\bibcite{Bengio1}{10}
\bibcite{Bengio2}{11}
\@writefile{toc}{\contentsline {section}{\numberline {10}References}{93}{section.10}}
\bibcite{Bengio3}{12}
\bibcite{Bottou}{13}
\bibcite{Bottou2}{14}
\bibcite{Ciresan}{15}
\bibcite{Chu}{16}
\bibcite{Chung}{17}
\bibcite{Crutchfield}{18}
\bibcite{Dauphin}{19}
\bibcite{Devarakonda}{20}
\bibcite{Donoho}{21}
\bibcite{Duchi}{22}
\bibcite{Erhan}{23}
\bibcite{Fama}{24}
\bibcite{Fan1}{25}
\bibcite{Fan2}{26}
\bibcite{Ge}{27}
\bibcite{Goodfellow}{28}
\bibcite{Glorot}{29}
\bibcite{Glorot2}{30}
\bibcite{Griffioen}{31}
\bibcite{Hansen}{32}
\bibcite{Harvey}{33}
\bibcite{Hawkins}{34}
\bibcite{He}{35}
\bibcite{Hinton1}{36}
\bibcite{Hinton2}{37}
\bibcite{Hinton3}{38}
\bibcite{Hinton4}{39}
\bibcite{Hinton5}{40}
\bibcite{HLZ}{41}
\bibcite{Hochreiter}{42}
\bibcite{Hornik}{43}
\bibcite{Hsu}{44}
\bibcite{Ivakhnenko}{45}
\bibcite{ImageNet}{46}
\bibcite{Ioannidis}{47}
\bibcite{Johnson}{48}
\bibcite{Kahn}{49}
\bibcite{Knerr}{50}
\bibcite{Langford}{51}
\bibcite{Langkvist}{52}
\bibcite{LeCun}{53}
\bibcite{LeCun2}{54}
\bibcite{LeCun3}{55}
\bibcite{LeCun4}{56}
\bibcite{LeRoux}{57}
\bibcite{Liu}{58}
\bibcite{Lo}{59}
\bibcite{Loshchilov}{60}
\bibcite{Lv}{61}
\bibcite{Mahajan}{62}
\bibcite{McLean}{63}
\bibcite{Minksy}{64}
\bibcite{Murphy}{65}
\bibcite{Packard}{66}
\bibcite{Pascanu}{67}
\bibcite{Peters}{68}
\bibcite{Prado}{69}
\bibcite{Povey}{70}
\bibcite{Ranzato1}{71}
\bibcite{Rumelhart}{72}
\bibcite{Schaefer}{73}
\bibcite{Schmidhuber}{74}
\bibcite{Schorfheide}{75}
\bibcite{Schwager}{76}
\bibcite{Sermanet}{77}
\bibcite{Shalev}{78}
\bibcite{Siegelmann}{79}
\bibcite{Smith}{80}
\bibcite{Skabar}{81}
\bibcite{Takeuchi}{82}
\bibcite{Takens}{83}
\bibcite{Troiano}{84}
\bibcite{Tseng}{85}
\bibcite{Vincent}{86}
\bibcite{Wan}{87}
\bibcite{Wang}{88}
\bibcite{Wang2}{89}
\bibcite{WaveNet}{90}
\bibcite{Wilcox}{91}
\bibcite{Weiss}{92}
\bibcite{Werbos}{93}
\bibcite{Werbos2}{94}
\bibcite{Wu}{95}
\bibcite{Yin}{96}
\bibcite{Zeiler}{97}
\bibcite{Zinkevich}{98}
\bibcite{Zhao}{99}
\bibcite{Zhang}{100}
\bibcite{Zhou}{101}
