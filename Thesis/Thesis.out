\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{LiteratureReview}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Technical Analysis}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Neural Networks}{section.2}% 4
\BOOKMARK [3][-]{subsubsection.2.2.1}{Training and Backpropagation}{subsection.2.2}% 5
\BOOKMARK [3][-]{subsubsection.2.2.2}{Activation Functions}{subsection.2.2}% 6
\BOOKMARK [3][-]{subsubsection.2.2.3}{Deep Learning}{subsection.2.2}% 7
\BOOKMARK [3][-]{subsubsection.2.2.4}{Backpropagation Improvements}{subsection.2.2}% 8
\BOOKMARK [2][-]{subsection.2.3}{Stacked Autoencoders}{section.2}% 9
\BOOKMARK [3][-]{subsubsection.2.3.1}{High Dimensional Data Reduction}{subsection.2.3}% 10
\BOOKMARK [3][-]{subsubsection.2.3.2}{Deep Belief Networks}{subsection.2.3}% 11
\BOOKMARK [3][-]{subsubsection.2.3.3}{Stacked Denoising Autoencoders}{subsection.2.3}% 12
\BOOKMARK [3][-]{subsubsection.2.3.4}{Pre-training}{subsection.2.3}% 13
\BOOKMARK [3][-]{subsubsection.2.3.5}{Time Series Applications}{subsection.2.3}% 14
\BOOKMARK [3][-]{subsubsection.2.3.6}{Financial Applications}{subsection.2.3}% 15
\BOOKMARK [2][-]{subsection.2.4}{Data Segmentation}{section.2}% 16
\BOOKMARK [2][-]{subsection.2.5}{Online Learning Algorithms and Gradient Descent}{section.2}% 17
\BOOKMARK [2][-]{subsection.2.6}{Backtesting and Model Validation}{section.2}% 18
\BOOKMARK [3][-]{subsubsection.2.6.1}{Testing Methodologies}{subsection.2.6}% 19
\BOOKMARK [3][-]{subsubsection.2.6.2}{Test Data Length}{subsection.2.6}% 20
\BOOKMARK [3][-]{subsubsection.2.6.3}{Sharpe Ratio}{subsection.2.6}% 21
\BOOKMARK [1][-]{section.3}{Implementation}{}% 22
\BOOKMARK [2][-]{subsection.3.1}{Process Overview}{section.3}% 23
\BOOKMARK [2][-]{subsection.3.2}{Feedforward Neural Networks}{section.3}% 24
\BOOKMARK [3][-]{subsubsection.3.2.1}{Notation and Network Representation}{subsection.3.2}% 25
\BOOKMARK [3][-]{subsubsection.3.2.2}{Activation Functions}{subsection.3.2}% 26
\BOOKMARK [3][-]{subsubsection.3.2.3}{Backpropagation}{subsection.3.2}% 27
\BOOKMARK [3][-]{subsubsection.3.2.4}{Gradient Descent Algorithms}{subsection.3.2}% 28
\BOOKMARK [3][-]{subsubsection.3.2.5}{Gradient Descent Improvements}{subsection.3.2}% 29
\BOOKMARK [2][-]{subsection.3.3}{Restricted Boltzmann Machines}{section.3}% 30
\BOOKMARK [3][-]{subsubsection.3.3.1}{Contrastive Divergence}{subsection.3.3}% 31
\BOOKMARK [3][-]{subsubsection.3.3.2}{CD-1 and SGD}{subsection.3.3}% 32
\BOOKMARK [2][-]{subsection.3.4}{Stacked Autoencoders}{section.3}% 33
\BOOKMARK [3][-]{subsubsection.3.4.1}{Greedy Layerwise SAE Training}{subsection.3.4}% 34
\BOOKMARK [3][-]{subsubsection.3.4.2}{Denoising Autoencoders}{subsection.3.4}% 35
\BOOKMARK [2][-]{subsection.3.5}{Deep Network Weight Initializations}{section.3}% 36
\BOOKMARK [3][-]{subsubsection.3.5.1}{Deep Belief Network Training}{subsection.3.5}% 37
\BOOKMARK [1][-]{section.4}{Data}{}% 38
\BOOKMARK [1][-]{section.5}{Results}{}% 39
\BOOKMARK [1][-]{section.6}{Conclusion}{}% 40
\BOOKMARK [1][-]{section.7}{References}{}% 41
